Research Area,Professor Name,Profile URL,Biography,Research Interests,Education,Publication Date,Publication Title,Publication Link,Citation,Publication Summary
Algorithms and Theory,Huy Lê Nguyen,https://www.khoury.northeastern.edu/people/huy-le-nguyen/,"Huy Lê Nguyen is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Algorithmic techniques for massive data sets; Optimization; Machine learning,"PhD in Computer Science, Princeton University; MEng in Computer Science, Massachusetts Institute of Technology; BS in Computer Science and Mathematics, Massachusetts Institute of Technology","October 31st, 2022",Improved Learning-augmented Algorithms for k-means and k-medians Clustering,https://openreview.net/pdf?id=dCSFiAl_VO3," Thy Dinh Nguyen, Anamay Chaturvedi, Huy L. Nguyen. (2023). Improved Learning-augmented Algorithms for k-means and k-medians Clustering ICLR. https://openreview.net/pdf?id=dCSFiAl_VO3","We consider the problem of clustering in the learning-augmented setting. We are given a data set in d-dimensional Euclidean space, and a label for each data point given by a predictor indicating what subsets of points should be clustered together. This setting captures situations where we have access to some auxiliary informa- tion about the data set relevant for our clustering objective, for instance the labels output by a neural network. Following prior work, we assume that there are at most an α ∈(0, c) for some c < 1 fraction of false positives and false negatives in each predicted cluster, in the absence of which the labels would attain the optimal clustering cost OPT. For a dataset of size m, we propose a deterministic k-means algorithm that produces centers with an improved bound on the clustering cost compared to the previous randomized state-of-the-art algorithm while preserv- ing the O(dm log m) runtime. Furthermore, our algorithm works even when the predictions are not very accurate,conclusion not found"
Algorithms and Theory,Rajmohan Rajaraman,https://www.khoury.northeastern.edu/people/rajmohan-rajaraman/,"Rajmohan Rajaraman is a professor and the associate dean of faculty affairs in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Approximation algorithms for intractable optimization problems; Network design; Distributed computing in dynamic environments; Algorithmic game theory,"PhD in Computer Science, University of Texas at Austin; BS in Computer Science, IIT Kanpur — India","July 11th, 2016",Robust and Probabilistic Failure-Aware Placement,http://www.ccs.neu.edu/home/rraj/Pubs/FailureAwarePlacementSPAA2016.pdf," Rajmohan Rajaraman , M. Korupolu ACM Symposium on Parallelism in Algorithms and Architectures, July 2016","Motivated by the growing complexity and heterogeneity of modern data centers, and the prevalence of commodity com- ponent failures, this paper studies the failure-aware place- ment problem of placing tasks of a parallel job on machines in the data center with the goal of increasing availability. We consider two models of failures: adversarial and prob- abilistic. In the adversarial model, each node has a weight (higher weight implying higher reliability) and the adver- sary can remove any subset of nodes of total weight at most a given bound W and our goal is to ﬁnd a placement that incurs the least disruption against such an adversary. In the probabilistic model, each node has a probability of failure and we need to ﬁnd a placement that maximizes the proba- bility that at least K out of N tasks survive at any time. For adversarial failures, we ﬁrst show that (i) the prob- lems are in Σ2, the second level of the polynomial hierarchy, (ii) a basic variant, that we call RobustFAP, is co-NP- hard, and (iii) an all-or-nothing version of RobustFAP is Σ2-complete. We then give a PTAS for RobustFAP, a key ingredient of which is a solution that we design for a frac- tional version of RobustFAP. We then study fractional Ro- bustFAP over hierarchies, denoted HierRobustFAP, and introduce a notion of hierarchical max-min fairness and a novel Generalized Spreading algorithm which is simultane- ously optimal for all W. These generalize the classical notion of max-min fairness to work with nodes of diﬀering capaci- ties, diﬀering reliability weights and hierarchical structures. Using randomized rounding, we extend this to give an algo- rithm for integral HierRobustFAP. For the probabilistic version, we ﬁrst give an algorithm that achieves an additive ε approximation in the failure probability for the single level version, called ProbFAP, while giving up a (1+ε) multiplicative factor in the number of failures. We then extend the result to the hierarchical ver- sion, HierProbFAP, achieving an ε additive approximation ∗Supported in part by NSF grants CNS-1217981, CCF- 1422715, and CCF-1535929, and a Google Research Award. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. SPAA ’16, July 11 - 13, 2016, Paciﬁc Grove, CA, USA c⃝2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4210-0/16/07...$15.00 DOI: http://dx.dos and open problems In this paper, we formulate and initiate the study of failure- aware placement for hierarchical data centers under adver- sarial and probabilistic failures. For both classes, we give hardness results and approximation algorithms for multiple variants: based on generalized spreading for HierRobust- FAP and Poisson approximation for HierProbFAP. One natural question is to improve the approximation guarantees and design more practical implementations for some of the variants. In particular, the approximation fac- tor in the bicriteria PTAS for HierProbFAP grows linearly in the number of levels. While this is reasonable for current datacenters (which typically have three to four levels of hi- erarchy), it would be interesting to see if the approximation factor can be made sublinear or even independent of number of levels. Similarly, our PTASes incur high running times (e.g., for RobustFAP, a standard implementation would take time O(n(3+ln(1/ε))/ε) time), and it is important to de- sign practical implementations of PTASes that trade oﬀap- proximation guarantees for simplicity. On a related note, our study here has focused on identical tasks of the same size. It will be interesting to extend this to an on-line sequence of jobs of possibly diﬀerent sizes. A further extension would be to consider dynamic task migra- tions to improve availability as jobs ﬁnish and leave."
Algorithms and Theory,Rajmohan Rajaraman,https://www.khoury.northeastern.edu/people/rajmohan-rajaraman/,"Rajmohan Rajaraman is a professor and the associate dean of faculty affairs in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Approximation algorithms for intractable optimization problems; Network design; Distributed computing in dynamic environments; Algorithmic game theory,"PhD in Computer Science, University of Texas at Austin; BS in Computer Science, IIT Kanpur — India","May 8th, 2016",Robust Secret Sharing with Essentially Optimal Share Size,http://www.ccs.neu.edu/home/rraj/Pubs/SecretSharingEC2016.pdf," Rajmohan Rajaraman, A. Bishop, V. Pastro, and D. Wichs, Eurocrypt, May 2016","In a t-out-of-n robust secret sharing scheme, a secret message is shared among n parties who can reconstruct the message by combining their shares. An adversary can adaptively corrupt up to t of the parties, get their shares, and modify them arbitrarily. The scheme should sat- isfy privacy, meaning that the adversary cannot learn anything about the shared message, and robustness, meaning that the adversary cannot cause the reconstruction procedure to output an incorrect message. Such schemes are only possible in the case of an honest majority, and here we focus on unconditional security in the maximal corruption setting where n = 2t +We constructed an eﬃcient robust secret sharing scheme for the maximal cor- ruption setting with n = 2t + 1 parties with nearly optimal share size of m + ek bits, where m is the length of the message and 2−k is the failure probability of the reconstruction procedure with adversarial shares. One open question would be to optimize the poly-logarithmic terms in our construction. Indeed, it should be relatively easy to improve on these terms which we did not analyze carefully, but it seems challenging and interesting to attempt to go all the way down to m + O(k) or perhaps even just m + k bits. We leave this as a challenge for future work."
Algorithms and Theory,Emanuele Viola,https://www.khoury.northeastern.edu/people/emanuele-viola/,"Emanuele Viola is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Theoretical computer science,"PhD in Computer Science, Harvard University; BS in Computer Science, La Sapienza University — Italy","May 15th, 2012","Substitution-permutation networks, pseudo random functions, and natural proofs",https://eprint.iacr.org/2011/226.pdf, Eric Miles and Emanuele Viola,"This paper takes a new step towards closing the troubling gap between pseudoran- dom functions (PRF) and their popular, bounded-input-length counterparts. This gap is both quantitative, because these counterparts are more eﬃcient than PRF in various ways, and methodological, because these counterparts usually ﬁt in the substitution- permutation network paradigm (SPN) which has not been used to construct PRF. We give several candidate PRF Fi that are inspired by the SPN paradigm. This paradigm involves a “substitution function” (S-box). Our main candidates are: F1 : {0, 1}n →{0, 1}n is an SPN whose S-box is a random function on b bits given as part of the seed. We prove unconditionally that F1 resists attacks that run in time ≤2ǫb. Setting b = ω(lg n) we obtain an ineﬃcient PRF, which however seems to be the ﬁrst such construction using the SPN paradigm. F2 : {0, 1}n →{0, 1}n is an SPN where the S-box is (patched) ﬁeld inversion, a common choice in practical constructions. F2 is computable with Boolean circuits of size n · logO(1) n, and in particular with seed length n · logO(1) n. We prove that this candidate has exponential security 2Ω(n) against linear and diﬀerential cryptanalysis. F3 : {0, 1}n →{0, 1} is a non-standard variant on the SPN paradigm, where “states” grow in length. F3 is computable with size n1+ǫ, for any ǫ > 0, in the restricted circuit class TC0 of unbounded fan-in majority circuits of constant-depth. We prove that F3 is almost 3-wise independent. F4 : {0, 1}n →{0, 1} uses an extreme setting of the SPN parameters (one round, one S-box, no diﬀusion matrix). The S-box is again (patched) ﬁeld inversion. We prove that this candidate fools all parity tests that look at ≤20.9n outputs. Assuming the security of our candidates, our work also narrows the gap between the “Natural Proofs barrier” [Razborov & Rudich; JCSS ’97] and existing lower bounds, in three models: unbounded-depth circuits, TC0 circuits, and Turing machines. In particular, the eﬃciency of the circuits computing F3 is related to a result by Allender and Koucky [JACM ’10] who show that a lower bound for such circuits would imply a lower bound for TC0. ∗Supported by NSF grant CCF-0845003. Email: {enmiles,viola}@ccs.neu.edu 1and future work Two obvious directions for future work are to extend the analysis of F1 to handle inverse queries (necessarily choosing the S-box as a random permutation), and to extend Theorem 1.8 to prove almost d-wise independence of F3 for d > 3. A more foundational question left unanswered is to understand how the degree of each output bit of an SPN (as a polynomial in the input bits) is aﬀected by the degree of the S-box and by the “mixing” properties of the linear transformation. Exploring other choices of the S-box besides inversion may lead to more eﬃcient construc- tions, and utilizing other properties of the linear transformation besides maximal-branch- number may allow stronger proofs of security. This could potentially give a (plausibly secure) SPN computable by circuits of size O(n). Recall from §1 that a PRF computable with size O(n) and with security 2n would bring the Natural Proofs barrier to the current frontier of lower bounds against unbounded-depth circuits. Abstracting from the SPN structure, one may arrive to the following paradigm for con- structing PRF: alternate the application of (1) an error-correcting code and (2) a bundle-wise 25 application of any local map that has high degree over GF(2) and resists attacks correspond- ing to linear and diﬀerential cryptanalysis. This viewpoint may lead to a PRF candidate computable in ACC0, since for (1) one just needs parity gates, while, say, taking parities of suitable mod 3 maps one should get a map that satisﬁes (2). However a good choice for this latter map is not clear to us at this moment. We believe a good candidate PRF should be the simplest candidate that resists known attacks. As noted in [DR02], some of the choices in the design of AES are not motivated by any known attack, but are there as a safeguard (for example, one can reduce the number of rounds and still no attack is known). While this is comprehensible when having to choose a standard that is diﬃcult to change or when deploying a system that is to be widely used, one can argue that a better way for the research community to proceed is to put forth the simplest candidate PRF, possibly break it, and iterate until hopefully converging to a secure PRF. We view this paper as a step in this direction."
Algorithms and Theory,Daniel Wichs,https://www.khoury.northeastern.edu/people/daniel-wichs/,"Daniel Wichs is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Modern cryptography; Information security,"PhD in Computer Science, New York University; MS in Computer Science, Stanford University; BS in Mathematics, Stanford University","October 15th, 2017",Obfuscating Compute-and-Compare Programs under LWE,http://ieee-focs.org/FOCS-2017-Papers/3464a600.pdf, Daniel Wichs and Giorgos Zirdelis,Failed to download
Artificial Intelligence,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","February 20th, 2023",Improving Deep Policy Gradients with Value Function Search,https://openreview.net/pdf?id=6qZC7pfenQm," Enrico Marchesini, Christopher Amato. (2023). Improving Deep Policy Gradients with Value Function Search ICLR. https://openreview.net/pdf?id=6qZC7pfenQm","Deep Policy Gradient (PG) algorithms employ value networks to drive the learn- ing of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approxima- tion and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the su- pervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains. 1VFS introduces a two-scale perturbation operator voted to diversify a population of value networks to (i) explore local variations of current critics’ predictions and (ii) allow to explore diversified value functions to escape from local optima. The practical results of such components have been investigated with additional experiments that also motivate the improvement in sample efficiency and performance of VFS-based algorithms in a range of standard continuous control benchmarks. Our findings suggest that improving fundamental Deep PG primitives translates into higher-performing policies and better sample efficiency. 9 Published as a conference paper at ICLR 2023 7"
Artificial Intelligence,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","July 5th, 2018",Decision-Making Under Uncertainty in Multi-Agent and Multi-Robot Systems: Planning and Learning,http://www.ccs.neu.edu/home/camato/publications/ijcai18.pdf," Christopher Amato. In the Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18), July 2018","Multi-agent planning and learning methods are be- coming increasingly important in today’s intercon- nected world. Methods for real-world domains, such as robotics, must consider uncertainty and limited communication in order to generate high- quality, robust solutions. This paper discusses our work on developing principled models to represent these problems and planning and learning methods that can scale to realistic multi-agent and multi- robot tasks. 1The methods that have been discussed have shown a lot of promise, but there are other methods that are also promising (e.g., [Dibangoye et al., 2016; Claes et al., 2017; Nguyen et al., 2017]) and many open questions yet to solve. It is worth noting that the methods in this paper focused on the ‘full’ Dec-POMDP problem, where there was uncertainty about outcomes, sensing and communication and agents are depen- dent on all others, but some problems do not have all of these characteristics. In these cases, ideas from Dec-POMDP mod- els and methods could still be used, but structure may be able to be exploited to allow higher-quality, more efﬁcient solution methods. Some such structure has been explored [Oliehoek and Amato, 2016], but efﬁciently exploiting structure in other cases could lead to scalable methods that also consider un- certainty (e.g., probabilistic solutions to common multi-robot problems). Traditionally, scalability of Dec-POMDPs solution meth- ods has been an issue, but more recent methods (such as those discussed in this paper) can scale to large domains. One approach that is used to overcome this lack of scal- ability is deep reinforcement learning. Developing deep RL approaches for Dec-POMDP-based models is becom- ing a very active ﬁeld (e.g., [Foerster et al., 2016; 2017; Mordatch and Abbeel, 2017; Omidshaﬁei et al., 2017c; Rashid et al., 2018]) and the resulting methods can handle large state and observation spaces (and potentially large ac- tion spaces). These methods (including ours) require a lot of data and typically use a simulator, so they may ﬁt better in situations akin to the ofﬂine sample-based planning scenarios described earlier. Therefore, there has been a great deal of success in scal- ing planning and ofﬂine learning methods to large domains, but efﬁcient online learning remains a challenge. As men- tioned above, online learning in a Dec-POMDP must be de- centralized (since fast and free communication is not avail- able to centralize decision-making), leading to nonstationar- ity, which must (continue to) be tackled. Besides more effec- tively conquering nonstationarity in learning, other open re- search topics include developing planning and learning meth- ods with additional efﬁciency and scalability (in terms of the number of agents as well as action and observation spaces) and how to properly represent and encode history informa- tion for planning and learning. Overall, there has been great progress in solving large, realistic Dec-POMDPs. It will be exciting to see what further developments and applications will arise in the future."
Artificial Intelligence,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","May 5th, 2018",Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems,http://www.ccs.neu.edu/home/camato/publications/ICRA2018.pdf," Nghia Hoang, Yuchen Xiao, Kavinayan Sivakumar, Christopher Amato and Jonathan P. How. In the Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA-18), May 2018.","— A key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self- interested or even adversarial parties who actively try to prevent the agents from achieving their goals. The practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with ﬁxed, procedurally characterized strategies. In contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies. This problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly. Our approach ﬁrst optimizes a set of stratagems that represent these best responses. These optimized stratagems are then inte- grated into a uniﬁed policy that can detect and respond when the adversaries change their strategies. The near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware.This paper introduces a novel near-optimal adversarial pol- icy switching algorithm for decentralized, non-cooperative multi-agent systems. Unlike the existing works in literature which are mostly limited to simple decision-making sce- narios where a single agent plans its best response against an adversary whose strategy is speciﬁed a priori under reasonable assumptions, we investigate instead a class of multi-agent scenarios where multiple robots need to operate independently in collaboration with their teammates to act effectively against adversaries with changing strategies. To achieve this, we ﬁrst optimize a set of basic stratagems that each is tuned to respond optimally to a pre-identiﬁed basic tactic of the adversaries. The stratagems are then integrated into a uniﬁed policy which performs near-optimally against B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 (a) (b) (c) B3 B2 B1 R1 R2 B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 (d) (e) (f) Fig. 6: Image excerpts from a video demo showing (1) a team of 3 allied (blue) robots (B1,B2 and B3) that implement the optimized stratagem produced by our framework (Section III) to compete against (2) an opposing team of 2 opponent (red) robots (R1 and R2) which implement the hand-coded tactics DL and DR (see Section VI-A), respectively: (a) B1,B2 and B3 decide to invade the opposition territory; (b) B1 and B3 decide to attack the center while B2 decides to take the left ﬂank of the opposition; (c) B2 passes through R1’s defense while B1 takes an interesting position to block R2 so that B3 can pass through its defense; (d) B1 and B2 detect the ﬂag and mount a pincer attack; (e) R2 arrives to defend the ﬂag and B2 retreats to avoid getting tagged; and (f) without noticing B1 from behind, R2 continues its DR patrol, thus losing the ﬂag to B1. any high-level strategies of the adversaries that switches between their basic tactics. The near-optimality of our pro- posed framework can be established in both theoretical and empirical settings with interesting and consistent results. We believe this is a signiﬁcant step towards bridging the gap between theory and practice in multi-agent research."
Artificial Intelligence,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","August 4th, 2017",COG-DICE: An Algorithm for Solving Continuous-Observation Dec-POMDPs,http://www.ccs.neu.edu/home/camato/publications/cogdice_ijcai.pdf," COG-DICE: An Algorithm for Solving Continuous-Observation Dec-POMDPs. Madison Clark-Turner and Christopher Amato. In the Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17), August 2017","The decentralized partially observable Markov de- cision process (Dec-POMDP) is a powerful model for representing multi-agent problems with de- centralized behavior. Unfortunately, current Dec- POMDP solution methods cannot solve problems with continuous observations, which are common in many real-world domains. To that end, we present a framework for representing and gener- ating Dec-POMDP policies that explicitly include continuous observations. We apply our algorithm to a novel tagging problem and an extended version of a common benchmark, where it generates poli- cies that meet or exceed the values of equivalent discretized domains without the need for ﬁnding an adequate discretization. 1This paper presented, for the ﬁrst time, an algorithm that gen- erates joint policies for Dec-POMDPs with continuous ob- servations. We presented both a discrete-observation ver- sion of the algorithm, which is applicable in domains with a large number of discrete observations, and a continuous- observation version. This method is broadly applicable as many real-world domains have large or continuous observa- tion spaces. COG-DICE has been successful in generating joint policies for both a novel and a preexisting problem and has highlighted the negative impacts that inappropriate dis- cretization can have on joint policy structure and value. For future work, we are interested in extending this work to high- dimensional observation spaces by exploring other (nonlin- ear) divisions and optimizing the algorithm parameters by ei- ther integrating these optimizations into the algorithm or pos- sibly building on previous work on Bayesian non-parametrics [Liu et al., 2015]."
Artificial Intelligence,David Bau,https://www.khoury.northeastern.edu/people/david-bau/,"David Bau is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Machine learning; Computer vision; Artificial intelligence; Natural language processing; Human–computer interaction,"PhD in Computer Science, Massachusetts Institute of Technology; MS in Computer Science, Cornell University; AB in Mathematics, Harvard University","October 24th, 2022",Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,https://openreview.net/pdf?id=DeG07_TcZvT," Kenneth Li , Aspen K. Hopkins, David Bau, Fernanda B. Viégas, Hanspeter Pfister, Martin Wattenberg. (2023). Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task ICLR. https://openreview.net/pdf?id=DeG07_TcZvT","Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question in a synthetic setting by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network. By leveraging these intervention techniques, we produce “latent saliency maps” that help explain predictions. 1 1Our experiments provide evidence that Othello-GPT maintains a representation of game board states— that is, the Othello “world”—to produce sequences it was trained on. This representation appears to be nonlinear in an essential way. Further, we find that these representations can be causally linked to how the model makes its predictions. Understanding of the internal representations of a sequence model is interesting in its own right, but may also be helpful in deeper interpretations of the network. We have also described how interventional experiments may be used to create a “latent saliency map”, which gives a picture, in terms of the Othello board, of how the network has made a prediction. Applied to two versions of Othello-GPT that were trained on different data sets, the latent saliency maps highlight the dramatic differences between underlying representations of the Othello-GPT trained on synthetic dataset and its counterpart trained on championship dataset. There are several potential lines of future work. One natural extension would be to perform the same type of investigations with other, more complex games. It would also be interesting to compare the strategies learned by a sequence model trained on game transcripts with those of a model trained with a priori knowledge of Othello. One option is to compare latent saliency maps of Othello–GPT with standard saliency maps of an Othello-playing program which has the actual board state as input. More broadly, it would be interesting to study how our results generalize to models trained on natural language. One stepping stone might be to look at language models whose training data has included game transcripts. Will we see similar representation of board state? Grammar engineering tools (Weston et al., 2015; Hermann et al., 2017; Cˆot´e et al., 2018) could help define a synthetic data generation process that maps world representations onto natural language sentences, providing a similarly controllable setting like Othello while closing the distance to natural languages. For more complex natural language tasks, can we find meaningful world representations? Our hope is that the tools described in this paper—nonlinear probes, layerwise interventions, and latent saliency maps—may prove useful in natural language settings. 9 Published as a conference paper at ICLR 2023"
Artificial Intelligence,David Bau,https://www.khoury.northeastern.edu/people/david-bau/,"David Bau is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Machine learning; Computer vision; Artificial intelligence; Natural language processing; Human–computer interaction,"PhD in Computer Science, Massachusetts Institute of Technology; MS in Computer Science, Cornell University; AB in Mathematics, Harvard University","October 13th, 2022",Mass-Editing Memory in a Transformer,https://openreview.net/pdf?id=MkbcAHIYgyS," Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, David Bau. (2023). Mass-Editing Memory in a Transformer ICLR. https://openreview.net/pdf?id=MkbcAHIYgyS","Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at memit.baulab.info. 1We have developed MEMIT, a method for editing factual memories in large language models by directly manipulating specific layer parameters. Our method scales to much larger sets of edits (100x) than other approaches while maintaining excellent specificity, generalization, and fluency. Our investigation also reveals some challenges: certain relations are more difficult to edit with robust specificity, yet even on challenging cases we find that MEMIT outperforms other methods by a clear margin. The knowledge representation we study is also limited in scope to working with directional (s, r, o) relations: it does not cover spatial or temporal reasoning, mathematical knowledge, linguistic knowledge, procedural knowledge, or even symmetric relations. For example, the association that “Tim Cook is CEO of Apple” must be processed separately from the opposite association that “The CEO of Apple is Tim Cook.” Despite these limitations, it is noteworthy that large-scale model updates can be constructed using an explicit analysis of internal computations. Our results raise a question: might interpretability-based methods become a commonplace alternative to traditional opaque fine-tuning approaches? Our positive experience brings us optimism that further improvements to our understanding of network internals will lead to more transparent and practical ways to edit, control, and audit models. 9 Published as a conference paper at ICLR 2023 7 ETHICAL CONSIDERATIONS Although we test a language model’s ability to serve as a knowledge base, we do not find these models to be a reliable source of knowledge, and we caution readers that a LLM should not be used as an authoritative source of facts. Our memory-editing methods shed light on the internal mechanisms of models and potentially reduce the cost and energy needed to fix errors in a model, but the same methods might also enable a malicious actor to insert false or damaging information into a model that was not originally present in the training data. 8"
Artificial Intelligence,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","December 8th, 2019",Deep Supervised Summarization: Algorithm and Application to Learning Instructions,https://khoury.northeastern.edu/home/eelhami/publications/SupFL_NeurIPS19.pdf," Deep Supervised Summarization: Algorithm and Application to Learning Instructions C. Xu and E. Elhamifar, Neural Information Processing Systems (NeurIPS), 2019.","We address the problem of ﬁnding representative points of datasets by learning from multiple datasets and their ground-truth summaries. We develop a supervised subset selection framework, based on the facility location utility function, which learns to map datasets to their ground-truth representatives. To do so, we propose to learn representations of data so that the input of transformed data to the facility location recovers their ground-truth representatives. Given the NP-hardness of the utility function, we consider its convex relaxation based on sparse representation and investigate conditions under which the solution of the convex optimization recovers ground-truth representatives of each dataset. We design a loss function whose minimization over the parameters of the data representation network leads to satisfying the theoretical conditions, hence guaranteeing recovering ground- truth summaries. Given the non-convexity of the loss function, we develop an efﬁcient learning scheme that alternates between representation learning by mini- mizing our proposed loss given the current assignments of points to ground-truth representatives and updating assignments given the current data representation. By experiments on the problem of learning key-steps (subactivities) of instruc- tional videos, we show that our proposed framework improves the state-of-the-art supervised subset selection algorithms. 1s We addressed the problem of supervised subset selection by generalizing the facility location to learn from ground-truth summaries. We considered an efﬁcient sparse optimization of the uncapacitated facility location and investigated conditions under which it recovers ground-truth representatives and also becomes equivalent to the original NP-hard problem. We designed a loss function and an efﬁcient framework to learn representations of data so that the input of transformed data to the facility location satisﬁes the theoretical conditions, hence, recovers ground-truth summaries. We showed the effectiveness of our method for recovering key-steps of instructional videos. To the best of our knowledge, this is the ﬁrst work on supervised subset selection that derives conditions under which subset selection recovers ground-truth representatives and employs them to design a loss function for deep representation learning. We believe that this work took a major step towards a theoretically motivated supervised subset selection framework."
Artificial Intelligence,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","October 27th, 2019",Unsupervised Procedure Learning via Joint Dynamic Summarization,http://www.ccs.neu.edu/home/eelhami/publications/ICCV19-ProceL-Ehsan.pdf," Unsupervised Procedure Learning via Joint Dynamic Summarization. E. Elhamifar and Z. Naing, International Conference on Computer Vision (ICCV), 2019.","We address the problem of unsupervised procedure learning from unconstrained instructional videos. Our goal is to produce a summary of the procedure key-steps and their ordering needed to perform a given task, as well as localization of the key-steps in videos. We develop a col- laborative sequential subset selection framework, where we build a dynamic model on videos by learning states and transitions between them, where states correspond to dif- ferent subactivities, including background and procedure steps. To extract procedure key-steps, we develop an opti- mization framework that ﬁnds a sequence of a small number of states that well represents all videos and is compatible with the state transition model. Given that our proposed optimization is non-convex and NP-hard, we develop a fast greedy algorithm whose complexity is linear in the length of the videos and the number of states of the dynamic model, hence, scales to large datasets. Under appropriate condi- tions on the transition model, our proposed formulation is approximately submodular, hence, comes with performance guarantees. We also present ProceL, a new multimodal dataset of 47.3 hours of videos and their transcripts from diverse tasks, for procedure learning evaluation. By exten- sive experiments, we show that our framework signiﬁcantly improves the state of the art performance.We developed a joint dynamic summarization method and a fast greedy algorithm for unsupervised procedure learning. Our method handles repeated key-steps, back- ground and missing or additional key-steps in videos. We presented ProceL, a new multimodal dataset for procedure learning. We showed our method signiﬁcantly improves the state of the art performance and showed the effectiveness of summarization tools, in general, for procedure learning."
Artificial Intelligence,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","June 16th, 2019",Facility Location: Approximate Submodularity and Greedy Algorithm,http://www.ccs.neu.edu/home/eelhami/publications/SeqFL_ICML19.pdf," Facility Location: Approximate Submodularity and Greedy Algorithm, E. Elhamifar, International Conference on Machine Learning (ICML), 2019.","We develop and analyze a novel utility function and a fast optimization algorithm for subset se- lection in sequential data that incorporates the dynamic model of data. We propose a cardinality- constrained sequential facility location function that ﬁnds a ﬁxed number of representatives, where the sequence of representatives is compatible with the dynamic model and well encodes the data. As maximizing this new objective function is NP- hard, we develop a fast greedy algorithm based on submodular maximization. Unlike the con- ventional facility location, the computation of the marginal gain in our case cannot be done by oper- ations on each item independently. We exploit the sequential structure of the problem and develop an efﬁcient dynamic programming-based algorithm that computes the marginal gain exactly. We in- vestigate conditions on the dynamic model, under which our utility function is (ε-approximately) submodualr, hence, the greedy algorithm comes with performance guarantees. By experiments on synthetic data and the problem of procedure learning from instructional videos, we show that our framework signiﬁcantly improves the compu- tational time, achieves better objective function values and obtains more coherent summaries.s We proposed a utility function and a fast greedy algorithm for subset selection in sequential datasets, taking advantage of the dynamic model of data. We proved that under appro- priate conditions on transition dynamics, our utility function is ε-approximately submodular, hence, enjoys approximate guarantees via the greedy method. By experiments on syn- thetic and real data, we showed the effectiveness of our method in terms of running time and attained objective val- ues as well as addressing the procedure learning task. Greedy Sequential Facility Location"
Artificial Intelligence,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","December 4th, 2017",Subset Selection and Summarization in Sequential Data,http://www.ccs.neu.edu/home/eelhami/publications/SeqSS-NIPS17-Ehsan.pdf," E. Elhamifar and M. C. De Paolis Kaluza; Neural Information Processing Systems (NIPS), 2017.","Subset selection, which is the task of ﬁnding a small subset of representative items from a large ground set, ﬁnds numerous applications in different areas. Sequential data, including time-series and ordered data, contain important structural relation- ships among items, imposed by underlying dynamic models of data, that should play a vital role in the selection of representatives. However, nearly all existing subset selection techniques ignore underlying dynamics of data and treat items independently, leading to incompatible sets of representatives. In this paper, we develop a new framework for sequential subset selection that ﬁnds a set of represen- tatives compatible with the dynamic models of data. To do so, we equip items with transition dynamic models and pose the problem as an integer binary optimization over assignments of sequential items to representatives, that leads to high encoding, diversity and transition potentials. Our formulation generalizes the well-known facility location objective to deal with sequential data, incorporating transition dynamics among facilities. As the proposed formulation is non-convex, we derive a max-sum message passing algorithm to solve the problem efﬁciently. Experiments on synthetic and real data, including instructional video summarization, show that our sequential subset selection framework not only achieves better encoding and diversity than the state of the art, but also successfully incorporates dynamics of data, leading to compatible representatives. 1s and Future Work We developed a new framework for sequential subset selection that takes advantage of the underlying dynamic models of data, promoting to select a set of representatives that are compatible according to the dynamic models of data. By experiments on synthetic and real data, we showed the effectiveness of our method for summarization of sequential data. Our ongoing research include development of fast greedy algorithms for our sequential subset selection formulation, investigation of the theoretical guarantees of our method, as well as development of more effective summarization-based feature extraction techniques and working with larger datasets for the task of instructional data summarization. 9"
Artificial Intelligence,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","July 21st, 2017",Online Summarization via Submodular and Convex Optimization,http://www.ccs.neu.edu/home/eelhami/publications/onlineSS_CVPR17-Ehsan.pdf," E. Elhamifar and M. C. De Paolis Kaluza  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.","We consider the problem of subset selection in the online setting, where data arrive incrementally. Instead of stor- ing and running subset selection on the entire dataset, we propose an incremental subset selection framework that, at each time instant, uses the previously selected set of repre- sentatives and the new batch of data in order to update the set of representatives. We cast the problem as an integer bi- nary optimization minimizing the encoding cost of the data via representatives regularized by the number of selected items. As the proposed optimization is, in general, NP-hard and non-convex, we study a greedy approach based on un- constrained submodular optimization and also propose an efﬁcient convex relaxation. We show that, under appropri- ate conditions, the solution of our proposed convex algo- rithm achieves the global optimal solution of the non-convex problem. Our results also address the conventional problem of subset selection in the ofﬂine setting, as a special case. By extensive experiments on the problem of video summa- rization, we demonstrate that our proposed online subset selection algorithms perform well on real data, capturing diverse representative events in videos, while they obtain objective function values close to the ofﬂine setting.s We studied the problem of subset selection in the online setting, where data arrive incrementally. We proposed an incremental subset selection framework that, at each time instant, uses the previously selected set of representatives and the new batch of data in order to update the set of representatives. We cast the problem as an integer binary optimization minimizing the encoding cost of the data via representatives regularized by the number of selected items. We studied a randomized greedy approach based on uncon- strained submodular optimization and proposed a convex algorithm with theoretical performance guarantees. By ex- periments on real videos, we demonstrated the effectiveness of our methods for online video summarization."
Artificial Intelligence,Bruce Maxwell,https://www.khoury.northeastern.edu/people/bruce-maxwell/,"Bruce Maxwell is a teaching professor and assistant director of computing programs at Northeastern University’s Seattle campus. He earned both his bachelor’s degrees in political science and engineering from Swarthmore College, his master of philosophy in computer speech and natural language processing from the University of Cambridge, and his PhD in robotics from Carnegie Mellon University. His areas of teaching include computer vision, computer graphics, robotics, machine learning, and more.",Computer vision; Machine learning; Robotics; Data science,"PhD in Robotics, Carnegie Mellon University; MPhil in Computer Speech and Natural Language Processing, University of Cambridge; BS in Engineering, Swarthmore College; BA in Political Science, Swarthmore College","May 14th, 2021",Real-time Physics-based Removal of Shadows and Shading from Road Surfaces,https://cs.colby.edu/maxwell/papers/pdfs/Maxwell-WAD-2019.pdf," B. A. Maxwell, C. A. Smith, M. Qraitem, R. Messing, S. Whitt, N. Thien, R. M. Friedhoff, ""Real-time Physics-based Removal of Shadows and Shading from Road Surfaces"", Workshop on Autonomous Driving [WAD], affiliated with CVPR, 2019.",Failed to download
Artificial Intelligence,Bruce Maxwell,https://www.khoury.northeastern.edu/people/bruce-maxwell/,"Bruce Maxwell is a teaching professor and assistant director of computing programs at Northeastern University’s Seattle campus. He earned both his bachelor’s degrees in political science and engineering from Swarthmore College, his master of philosophy in computer speech and natural language processing from the University of Cambridge, and his PhD in robotics from Carnegie Mellon University. His areas of teaching include computer vision, computer graphics, robotics, machine learning, and more.",Computer vision; Machine learning; Robotics; Data science,"PhD in Robotics, Carnegie Mellon University; MPhil in Computer Speech and Natural Language Processing, University of Cambridge; BS in Engineering, Swarthmore College; BA in Political Science, Swarthmore College","June 23rd, 2008",A Bi-Illuminant Dichromatic Reflection Model for Understanding Images,https://cs.colby.edu/maxwell/papers/pdfs/Maxwell-CVPR-2008.pdf," B. A. Maxwell, R. M. Friedhoff, and C. A. Smith, ""A Bi-Illuminant Dichromatic Reflection Model for Understanding Images"", in IEEE Conf. on Computer Vision and Pattern Recognition, 2008.",Failed to download
Artificial Intelligence,Byron Wallace,https://www.khoury.northeastern.edu/people/byron-wallace/,"Byron Wallace is an associate dean of graduate programs, director for the undergraduate data science program, and the Sy and Laurie Sternberg Interdisciplinary Associate Professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Data mining; Machine learning; Natural language processing,"PhD in Computer Science, Tufts University; BS in Computer Science, University of Massachusetts at Amherst","April 25th, 2017",Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization,https://arxiv.org/pdf/1702.02535v3.pdf," Ye Zhang, Matthew Lease, Byron C. Wallace","A fundamental advantage of neural mod- els for NLP is their ability to learn rep- resentations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., Word- Net or domain speciﬁc ontologies such as the Uniﬁed Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compres- sion. In contrast, we treat weight shar- ing as a ﬂexible mechanism for incorpo- rating prior knowledge into neural models. We show that this approach consistently yields improved performance on classiﬁ- cation tasks compared to baseline strate- gies that do not exploit weight sharing. 1We have proposed a novel method for incorporat- ing prior semantic knowledge into neural models via stochastic weight sharing. We have showed it generally improves text classiﬁcation performance vs. model variants which do not exploit external resources and vs. an approach based on retroﬁtting prior to training. In future work, we will inves- tigate generalizing our approach beyond classiﬁ- cation, and to inform weight sharing using other varieties and sources of linguistic knowledge."
Artificial Intelligence,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","February 27th, 2023",Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,https://openreview.net/pdf?id=_2bDpAtr7PI," David Klee, Ondrej Biza, Robert Platt, Robin Walters. (2023). Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction ICLR. https://openreview.net/pdf?id=_2bDpAtr7PI","Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in SO(3). How- ever, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages SO(3) equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at https://dmklee.github.io/image2sphere. 1In this work, we present the first method to leverage SO(3)-equivariance for predicting distributions over 3D rotations from single images. Our method is better suited than regression methods at handling unknown object symmetries, generates more expressive distributions than methods using parametric families of multi-modal distributions while requiring fewer samples than an implicit modeling approach. We demonstrate state-of-the-art performance on the challenging PASCAL3D+ dataset composed of real images. One limitation of our work is that we use a high maximum frequency, L, in the spherical convolution operations to have higher resolution predictions. Because the number of operations in a spherical convolution is quadratic in L, it may be impractical for applications where more spherical convolutions are required. 9 Published as a conference paper at ICLR 2023"
Artificial Intelligence,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","February 1st, 2023","Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow",https://openreview.net/pdf?id=9ZpciCOunFb," Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, Nima Dehmamy. (2023). Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow ICLR. https://openreview.net/pdf?id=9ZpciCOunFb","Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neu- ral networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness un- der certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating con- served quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability. 1conclusion not found"
Artificial Intelligence,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","November 16th, 2022",The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,https://openreview.net/pdf?id=P4MUGRM4Acu," Dian Wang , Jung Yeon Park, Neel Sortur, Lawson L. S. Wong, Robin Walters, Robert Platt. (2023). The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry ICLR. https://openreview.net/pdf?id=P4MUGRM4Acu","Extensive work has demonstrated that equivariant neural networks can signifi- cantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the do- main symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the in- put. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surpris- ingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, impos- ing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. 1conclusion not found"
Artificial Intelligence,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","June 8th, 2022",Integrating Symmetry into Differentiable Planning with Steerable Convolutions,https://openreview.net/pdf?id=n7CPzMPKQl," Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L. S. Wong. (2023). Integrating Symmetry into Differentiable Planning with Steerable Convolutions ICLR. https://openreview.net/pdf?id=n7CPzMPKQl","In this paper, we study a principled approach on incorporating group symme- try into end-to-end differentiable planning algorithms and explore the benefits of symmetry in planning. To achieve this, we draw inspiration from equivariant con- volution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant op- erator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navi- gation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2- DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN. 1, SymVIN and SymGPPN generalize better to different map sizes, com- pared to all non-equivariant baselines. Remark. In summary, our results show that the Sym- Plan models demonstrate end-to-end planning and learn- ing ability, potentially enabling further applications to other tasks as a differentiable component for planning. Additional results and ablation studies are in Appendix H. 7 DISCUSSION In this work, we study the symmetry in the 2D path-planning problem, and build a framework using the theory of steerable CNNs to prove that value iteration in path planning is actually a form of steer- able CNN (on 2D grids). Motivated by our theory, we proposed two symmetric planning algorithms that provided significant empirical improvements in several path-planning domains. Although our focus in this paper has been on Z2, our framework can potentially generalize to path planning on higher-dimensional or even continuous Euclidean spaces (Weiler et al., 2018; Brandstetter et al., 2021), by using equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We hope that our SymPlan frame- work, along with the design of practical symmetric planning algorithms, can provide a new pathway for integrating symmetry into differentiable planning. 9 Published as a conference paper at ICLR 2023 8 ACKNOWLEDGEMENT This work was supported by NSF Grants #2107256 and #2134178. R. Walters is supported by The Roux Institute and the Harold Alfond Foundation. We also thank the audience from previous poster and talk presentations for helpful discussions and anonymous reviewers for useful feedback. 9 REPRODUCIBILITY STATEMENT We provide additional details in the appendix. We also plan to open source the codebase. We briefly outline the appendix below. 1. Additional Discussion 2. Background: Technical background and concepts on steerable CNNs and group CNNs 3. Method: we provide full details on how to reproduce it 4. Theory/Framework: we provide the complete version of the theory statements 5. Proofs: this includes all proofs 6. Experiment / Environment / Implementation details: useful details for reproducibility 7. Additional results 10 Published as a conference paper at ICLR 2023"
Artificial Intelligence,Lawson Wong,https://www.khoury.northeastern.edu/people/lawson-wong/,"Lawson L.S. Wong is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"PhD, Massachusetts Institute of Technology","November 16th, 2022",The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,https://openreview.net/pdf?id=P4MUGRM4Acu," Dian Wang , Jung Yeon Park, Neel Sortur, Lawson L. S. Wong, Robin Walters, Robert Platt. (2023). The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry ICLR. https://openreview.net/pdf?id=P4MUGRM4Acu","Extensive work has demonstrated that equivariant neural networks can signifi- cantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the do- main symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the in- put. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surpris- ingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, impos- ing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. 1conclusion not found"
Artificial Intelligence,Lawson Wong,https://www.khoury.northeastern.edu/people/lawson-wong/,"Lawson L.S. Wong is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"PhD, Massachusetts Institute of Technology","October 24th, 2022",Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation,https://openreview.net/pdf?id=PYbe4MoHf32," Linfeng Zhao, Huazhe Xu, Lawson L. S. Wong. (2023). Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation ICLR. https://openreview.net/pdf?id=PYbe4MoHf32","Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which cou- ples forward computation and backpropagation and needs to balance forward plan- ner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to de- couple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scal- ability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace. 1already mentioned in the above section: Beyond an intermediate iteration number (around 30-50), IDPs are more favorable because of scalability and computational cost. We present other analyses here. We start from ConvGPPN and ID-ConvGPPN, which perform the best in ADPs and IDPs class, re- spectively. They also have the most number of parameters and use greatest time because of the gates in ConvGRU units. As shown in Figure 2, this also caused two issues of ConvGPPN: scalability to larger maps/iterations (out of memory for 27 × 27 80 iterations and 49 × 49 50 and 80 iterations), and also convergence stability (e.g. 27 × 27 50 iterations). For SymVIN and ID-SymVIN, they replace Conv2D with SteerableConv, with computational cost slightly higher than VIN and much lower than ConvGPPN. Thus, they can successfully run on all tasks and iteration numbers. However, we find that explicit SymVIN may diverge due to bad ini- tialization, and this is more severe if the network is deeper (more iterations), as in Figure 2’s 50 and 7 Published as a conference paper at ICLR 2023 30 50 80 #Layers Klayer 0 100 200 Forward Time Task = 15 × 15 30 50 80 #Layers Klayer Task = 27 × 27 30 50 80 #Layers Klayer Task = 49 × 49 30 50 80 Forward Kfwd 0 100 200 Forward Time Task = 15 × 15 30 50 80 Forward Kfwd Task = 27 × 27 30 50 80 Forward Kfwd Task = 49 × 49 30 50 80 #Layers Klayer 0 50 100 Backward Time Task = 15 × 15 30 50 80 #Layers Klayer Task = 27 × 27 30 50 80 #Layers Klayer Task = 49 × 49 VIN SymVIN ConvGPPN 30 50 80 Forward Kfwd 0 50 100 Backward Time Task = 15 × 15 30 50 80 Forward Kfwd Task = 27 × 27 30 50 80 Forward Kfwd Task = 49 × 49 ID-VIN ID-SymVIN ID-ConvGPPN Figure 6: The runtime (in seconds) on 2D navigation tasks with size 15 × 15, 27 × 27, and 49 × 49, averaged over 5 seeds. The ADPs are on the left six figures and the IDPs on the right. Missing dots are due to out of memory caused by algorithmic differentiation. The upper row is for forward pass runtime, and the lower row is for backward runtime. The horizontal axes mean differently: (1) ADPs: the number of layers Klayer, also number of iterations, and (2) IDPs: the forward pass iterations Kfwd. 0 10 20 30 40 50 60 Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Successful Rate 0 10 20 30 40 50 60 Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Successful Rate VIN ID-VIN SymVIN ID-SymVIN ConvGPPN ID-ConvGPPN Figure 7: (Left) Training curves on 2D maze navigation 15 × 15 maps, with 80 layers for ADPs and 80 iterations for IDPs. (Right) Training curves on 49 × 49 maps, with 30 layers for ADPs (due to scalability issue) and 80 iterations for IDPs. 80 iterations. Nevertheless, ID-SymVIN alleviates this issue, since implicit differentiable planning decouples forward and backward pass, so the gradient computation is not affected by forward pass. Furthermore, VIN and ID-VIN are surprisingly less affected by the number of iterations and problem scale. We find their forward passes tends to converge faster to reasonable equilibria, thus further increasing iteration does not help, nor break convergence as long as memory is sufficient. Forward and backward runtime. We visualize the runtime of IDPs and ADPs in Figure 6. For IDPs, we use the forward-iteration solver for the forward pass and Anderson solver for the backward pass. Note that in the bottom left, we intentionally plot backward runtime vs. forward pass iterations. This emphasizes that IDPs decouple forward and backward passes because the backward runtime does not rely on forward pass iterations. Instead, for ADPs, value iteration is done by network layers, thus the backward pass is coupled: the runtime increases with depth and some runs failed due to limited memory (11GB, see missing dots). Therefore, this set of figures shows better scalability of IDPs (no missing dots – out of memory – and constant backward time). In terms of absolute time, the forward runtime of IDPs when using the forward solver is comparable with successful ADPs. 5.3 TRAINING PERFORMANCE Setup. Beyond evaluating generalization to novel maps, we compare their training efficiency with learning curves. Each learning curve is aggregated over 5 seeds, which are from the models in the above section. The learning curves are for all planners on 15 × 15 maps (Figure 7 left) and 49 × 49 maps (Figure 7 right, 30 layers for ADPs – due to scalability issue – and 80 iterations for IDPs). Results. On 15×15 maps, we show Klayer = 80 layers for ADPs and Kfwd = 80 iterations for IDPs. ID-ConvGPPN performs the best and is much more stable than its ADP counterpart ConvGPPN. ID- SymVIN learns reliably, while SymVIN fails to converge due to instability from 80 layers. ID-VIN and VIN are comparable throughout training. On 49 × 49 maps, we visualize Klayer = 30 layers 8 Published as a conference paper at ICLR 2023 for ADPs (due to their limited scalability) and Kfwd = 80 iterations for IDPs. ConvGPPN cannot run at all even for only 30 layers, while ID-ConvGPPN still reaches a near-perfect success rate. ID- SymVIN learns slightly better than SymVIN and reaches higher asymptotic performance. ID-VIN has a similar trend to VIN, but performs worse overall due to the complexity of the task. 5.4 PERFORMANCE ON MORE TASKS Table 1: Averaged test success rate (%) over 5 seeds for using 10K/2K/2K dataset on the rest of 3 types of tasks. We highlight entry with italic for runs with at least one diverged trial (any success rate < 20%). Type Methods 18 × 18 Mani. 36 × 36 Mani. Workspace Mani. Visual Nav. Explicit VIN 89.65±7.97 74.75±8.18 80.98±3.84 66.11±8.91 SymVIN 55.15±49.54 65.72±47.11 82.17±24.72 96.04±4.24 ConvGPPN 79.71±20.71 70.55±36.13 70.23±19.44 81.76±31.04 Implicit (ours) ID-VIN 80.53±6.98 56.27±20.92 77.17±7.24 62.53±15.93 ID-SymVIN 99.63±0.08 98.53±1.42 87.60±24.11 86.41±30.34 ID-ConvGPPN 97.28±0.74 93.60±1.68 92.60±1.83 98.91±0.34 Setup. We run all planners on the other three challenging tasks. For visual navigation, we randomly generate 10K/2K/2K maps using the same strategy as 2D navigation and then render four egocentric panoramic views for each location from produced 3D environments with Gym- MiniWorld (Chevalier-Boisvert, 2018). For configuration-space manipulation and workspace manipulation, we randomly generate 10K/2K/2K tasks with 0 to 5 obstacles in workspace. In configuration-space manipulation, we manually convert each task into a 18 × 18 or 36 × 36 map (20◦or 10◦per bin). The workspace task additionally needs a mapper network to convert the 96×96 workspace (image of obstacles) to an 18 × 18 2-DOF configuration space (2D occupancy grid). We provide additional details in the Section D. Results. In Table 1, due to space limitations, we average over Klayer = 30, 50, 80 for ADPs and Kfwd = 30, 50, 80 for IDPs. For each task, we present the mean and standard deviation over 5 seeds times three hyperparameters and provide the separated results to Section E. We italicize entries for runs with at least one diverged trial (any success rate < 20%). Generally, IDPs perform much more stably. On 18×18 or 36×36 configuration-space manipulation, ID-SymVIN and ID-ConvGPPN reach almost perfect results, while ID-VIN has diverged runs on 36×36 (marked in italic). SymVIN and ConvGPPN are more unstable, while VIN even outperforms them and is also better than ID-VIN. On 18×18 workspace manipulation, because of the difficulty of jointly learning maps and potentially planning on inaccurate maps, most numbers are worse than in configuration-space. ID-ConvGPPN still performs the best, and other methods are comparable. For 15×15 visual navigation, it needs to learn a mapper from panoramic images and is more challenging. ID-ConvGPPN is still the best. ID-SymVIN exhibits some failed runs and gets underperformed by SymVIN in these seeds, and ID-VIN is comparable with VIN. Across all tasks, the results confirm the superiority of scalability and convergence stability of IDPs and demonstrate the ability of jointly training mappers (with algorithmic differentiation for this layer) even when using implicit differentiation for planners. 6 CONCLUSION This work studies how VIN-based differentiable planners can be improved from an implicit-function perspective: using implicit differentiation to solve the equilibrium imposed by the Bellman equation. We develop a practical pipeline for implicit differentiable planning and propose implicit versions of VIN, SymVIN, and ConvGPPN, which is comparable to or outperforms their explicit counterparts. We find that implicit differentiable planners (IDPs) can scale up to longer planning-horizon tasks and larger iterations. In summary, IDPs are favorable for these cases to ADPs for several reasons: (1) better performance mainly due to stability, (2) can scale up while some ADPs fail due to memory limit, (3) less computation time. On the contrary, if using too few iterations, the equilibrium may be poorly solved, and ADPs should be used instead. While we focus on value iteration, the idea of implicit differentiation is general and applicable beyond path planning, such as in continuous control where Neural ODEs can be deployed to solving ODEs or PDEs. 9 Published as a conference paper at ICLR 2023 7 ACKNOWLEDGEMENT This work was supported by NSF Grant #2107256. We also thank Clement Gehring and Lingzhi Kong for helpful discussions and anonymous reviewers for useful feedback. 8 REPRODUCIBILITY STATEMENT We provide an appendix with extended details of implementation in Section D, experiment details in Section D, and additional results in Section E. We plan to open-source our code next. 10 Published as a conference paper at ICLR 2023"
Artificial Intelligence,Lawson Wong,https://www.khoury.northeastern.edu/people/lawson-wong/,"Lawson L.S. Wong is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"PhD, Massachusetts Institute of Technology","June 8th, 2022",Integrating Symmetry into Differentiable Planning with Steerable Convolutions,https://openreview.net/pdf?id=n7CPzMPKQl," Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L. S. Wong. (2023). Integrating Symmetry into Differentiable Planning with Steerable Convolutions ICLR. https://openreview.net/pdf?id=n7CPzMPKQl","In this paper, we study a principled approach on incorporating group symme- try into end-to-end differentiable planning algorithms and explore the benefits of symmetry in planning. To achieve this, we draw inspiration from equivariant con- volution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant op- erator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navi- gation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2- DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN. 1, SymVIN and SymGPPN generalize better to different map sizes, com- pared to all non-equivariant baselines. Remark. In summary, our results show that the Sym- Plan models demonstrate end-to-end planning and learn- ing ability, potentially enabling further applications to other tasks as a differentiable component for planning. Additional results and ablation studies are in Appendix H. 7 DISCUSSION In this work, we study the symmetry in the 2D path-planning problem, and build a framework using the theory of steerable CNNs to prove that value iteration in path planning is actually a form of steer- able CNN (on 2D grids). Motivated by our theory, we proposed two symmetric planning algorithms that provided significant empirical improvements in several path-planning domains. Although our focus in this paper has been on Z2, our framework can potentially generalize to path planning on higher-dimensional or even continuous Euclidean spaces (Weiler et al., 2018; Brandstetter et al., 2021), by using equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We hope that our SymPlan frame- work, along with the design of practical symmetric planning algorithms, can provide a new pathway for integrating symmetry into differentiable planning. 9 Published as a conference paper at ICLR 2023 8 ACKNOWLEDGEMENT This work was supported by NSF Grants #2107256 and #2134178. R. Walters is supported by The Roux Institute and the Harold Alfond Foundation. We also thank the audience from previous poster and talk presentations for helpful discussions and anonymous reviewers for useful feedback. 9 REPRODUCIBILITY STATEMENT We provide additional details in the appendix. We also plan to open source the codebase. We briefly outline the appendix below. 1. Additional Discussion 2. Background: Technical background and concepts on steerable CNNs and group CNNs 3. Method: we provide full details on how to reproduce it 4. Theory/Framework: we provide the complete version of the theory statements 5. Proofs: this includes all proofs 6. Experiment / Environment / Implementation details: useful details for reproducibility 7. Additional results 10 Published as a conference paper at ICLR 2023"
Computational Biology,Benjamin Gyori,https://www.khoury.northeastern.edu/people/benjamin-gyori/,"Benjamin M. Gyori is an associate professor in the Khoury College of Computer Sciences and the College of Engineering at Northeastern University, based in Boston.",,"PhD in Computational Biology, National University of Singapore; BSc in Computer Engineering, Budapest University of Technology and Economics — Hungary","November 7th, 2023","A simple standard for ontological mappings 2023: updates on data model, collaborations and tooling",https://ceur-ws.org/Vol-3591/om2023_STpaper3.pdf," Nicolas Matentzoglu, Ian Braun, Anita R. Caron, Damien Goutte-Gattat, Benjamin M. Gyori, Nomi L. Harris, Emily Hartley, Harshad B. Hegde, Sven Hertling, Charles Tapley Hoyt, Hyeongsik Kim , Huanyu Li , James A. McLaughlin, Cássia Trojahn, Nicole A. Vasilevsky, Christopher J. Mungall. (2023). A simple standard for ontological mappings 2023: updates on data model, collaborations and tooling OM@ISWC, 73-78. https://ceur-ws.org/Vol-3591/om2023_STpaper3.pdf","The Simple Standard for Ontological Mappings (SSSOM) was first published in December 2021 (v. 0.9). After a number of revisions prompted by community feedback, we have published version 0.15.0 in July 2023. Here we report on the progress made since August 2022, in particular changes to tooling, data model and summary of ongoing standardisation efforts. Keywords standards, mappings, ontologies, ontology mapping, FAIR data1s Compared to the last update a year ago, changes to the SSSOM standard are getting fewer, which means that the metadata model is finally stabilising. Our intention is to launch version 1.0 by the year’s end. Efforts have shifted notably to community engagement and collaboration, and expansion of SSSOM related tooling. The main aspiration of the data and terminology mapping community should be to share well defined semantic mappings in completely open Mapping Commons, and support domain experts to curate better, semantically meaningful mappings."
Computer Science Education,Carla E. Brodley,https://www.khoury.northeastern.edu/people/carla-brodley/,Carla E. Brodley is the dean of inclusive computing and the founding executive director of the Center for Inclusive Computing (CIC) at Northeastern University. She is the dean emeritus of the Khoury College of Computer Sciences.,Computing education research; Broadening participation in computing; University administration,"PhD in Computer Science, University of Massachusetts at Amherst; MS in Computer Science, University of Massachusetts at Amherst; BA in Mathematics and Computer Science, McGill University — Canada","April 28th, 2020",The MSCS New Pathways Consortium – a National Invitation,https://carla.khoury.northeastern.edu/files/2020/04/mscs-new-pathways-consortium-4-27.pdf," Carla Brodley, Jan Cuny, RESPECT 2020","— High tech is the U.S. economy’s fastest growing sector. Yet, the current tech talent pipeline falls far short of meeting demand. What’s more, the demographics of the tech workforce remain stubbornly out of sync with the overall population. This is in large part because our education system still struggles to attract diverse people into computational disciplines. Women represent more than 50% of Bachelor’s degree recipients, but only 19% of computer science (CS) graduates. Similarly, underrepresented minorities represent 25% of Bachelor’s degree recipients, but just 10% of CS graduates. The diversity of thought, race, background, and gender in CS is essential to building a robust, high quality, and ethical tech sector. One place where innovation might bridge the gap – a place historically overlooked by higher education – is the Master’s degree. Since 2013, Northeastern University’s Khoury College of Computer Sciences has been testing, refining, and growing the Align program, a Master's of Science in computer science (MSCS) for people who studied something other than CS as undergraduates. The goal is to create a new pathway or onramp to CS for all students, paying particular attention to the recruitment and success of women and underrepresented minorities. In mid- 2019, we launched the MSCS New Pathways Consortium – an effort to collaborate with colleges and universities across the country to scale this approach. Here, we invite others to join the Consortium and, together, make the MSCS the new MBA, a professional degree that people can access regardless of prior experience and knowledge of computing. Keywords—Computer science education; diversity; non- majorsconclusion not found"
Computer Science Education,Carla E. Brodley,https://www.khoury.northeastern.edu/people/carla-brodley/,Carla E. Brodley is the dean of inclusive computing and the founding executive director of the Center for Inclusive Computing (CIC) at Northeastern University. She is the dean emeritus of the Khoury College of Computer Sciences.,Computing education research; Broadening participation in computing; University administration,"PhD in Computer Science, University of Massachusetts at Amherst; MS in Computer Science, University of Massachusetts at Amherst; BA in Mathematics and Computer Science, McGill University — Canada","April 1st, 2020",An MS in CS for non-CS Majors: Moving to Increase Diversity of Thought and Demographics in CS,https://carla.khoury.northeastern.edu/files/2020/04/alignFinal.pdf," Carla Brodley, Megan Barry, Aidan Connell, Catherine Gill, Ian Gorton, Benjamin Hescott, Bryan Lackaye, Cynthia LuBien, Leena Razzaq, Amit Shesh, Tiffani Williams, Andrea Danyluk, SIGCSE '20","We have created, piloted and are growing the Align program, a Mas- ter of Science in Computer Science (MS in CS) for post-secondary graduates who did not major in CS. Our goal is to create a path- way to CS for all students, with particular attention to women and underrepresented minorities. Indeed, women represent 57% and un- derrepresented minorities represent 25% of all bachelor’s recipients in the U.S., but only 19.5% and 12.6% of CS graduates, respectively. If we can fill this opportunity gap, we will satisfy a major economic need and address an issue of social equity and inclusion. In this paper, we present our “Bridge” curriculum, which is a two-semester preparation for students to then join the traditional MS in CS stu- dents in master’s-level classes. We describe co-curricular activities designed to help students succeed in the program. We present our empirical findings around enrollment, demographics, retention and job outcomes. Among our findings is that Align students outper- form our traditional MS in CS students in grade point average. To date we have graduated 137 students and 827 are enrolled. CCS CONCEPTS • Social and professional topics →Computer science educa- tion; Model curricula; Adult education. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGCSE ’20, March 11–14, 2020, Portland, OR, USA © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6793-6/20/03...$15.00 https://doconclusion not found"
Computer Science Education,Felix Muzny,https://www.khoury.northeastern.edu/people/felix-muzny/,"Felix Muzny is the director of teaching assistants, a clinical instructor, and a doctoral student at the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"MS in Computer Science, Stanford University; BS in Computer Science, University of Washington; BA in English, University of Washington","February 1st, 2021",Integrating Ethics into Introductory Programming Classes,https://cmci.colorado.edu/~cafi5706/SIGCSE2021_IntegratingEthics.pdf," Casey Fiesler, Mikhaila Friske, Natalie Garrett, Felix Muzny, Jessie J. Smith, Jason Zietz. 2021. Integrating Ethics into Introductory Programming Classes. In Proceedings of the ACM SIGCSE Technical Symposium on Computer Science Education, Toronto, Canada.","Increasing attention to the role of ethical consideration in com- puting has led to calls for greater integration of this critical topic into technical classes rather than siloed in standalone computing ethics classes. The motivation for such integration is not only to support in-situ learning, but also to emphasize to students that ethical consideration is inherently part of the technical practice of computing. We propose that the logical place to begin emphasiz- ing ethics is on day one of computing education: in introductory programming classes. This paper presents one approach to ethics integration into such classes: assignments that teach basic program- ming concepts (e.g., conditionals or iteration) but are contextualized with real-world ethical dilemmas or concepts. We report on experi- ences with this approach in multiple introductory programming courses, including details about select assignments, insights from instructors and teaching assistants, and results from surveys of a subset of students who took these courses. Based on these experi- ences we provide preliminary plans for future work, along with a roadmap for instructors to emulate our approach and suggestions for overcoming challenges they might face. CCS CONCEPTS • Social and professional topics →Model curricula. KEYWORDS ethics, introductory programming, CS1, social impact, assignments, university, undergraduate, content ACM Reference Format: Casey Fiesler, Mikhaila Friske, Natalie Garrett, Felix Muzny, Jessie J. Smith, and Jason Zietz. 202Nearly 25 years ago an NSF working group suggested integration of an ethics and social impact “tenth strand” throughout computer science core courses [19]. In recent years, large-scale projects like EmbeddedEthiCS [11] and the Responsible Computer Science Chal- lenge [23], as well as individual models such as ethics integration into HCI classes [30] or machine learning [24], have shown more movement towards this goal. As universities grapple with how best to handle the increasing demand for computer scientists with expertise in ethically wrought fields such as artificial intelligence [10], our hope is that touching on ethics early and often, starting with introductory programming, can have a profound impact on the perception of ethics as part of “doing computer science.” 9"
Cybersecurity and Privacy,Engin Kirda,https://www.khoury.northeastern.edu/people/engin-kirda/,"Engin Kirda is a professor in the Khoury College of Computer Sciences and the College of Engineering at Northeastern University, based in Boston.","Systems, software, and network security; Web security; Binary analysis; Malware detection","PhD in Computer Science, Technical University of Vienna — Austria; MS in Computer Science, Technical University of Vienna — Austria; BS in Computer Science, Technical University of Vienna — Austria","August 11th, 2016","UNVEIL: A Large-Scale, Automated Approach to Detecting Ransomware",https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_kharraz.pdf," A. Kharraz, S. Arshad, C. Mulliner, W. Robertson, E. Kirda. ""UNVEIL: A Large-Scale, Automated Approach to Detecting Ransomware"". In USENIX Security Symposium Austin, TX US, Aug 2016.","Although the concept of ransomware is not new (s In this paper we presented UNVEIL, a novel approach to detecting and analyzing ransomware. Our system is the first in the literature to specifically identify typical behavior of ransomware such as malicious encryption of files and locking of user desktops. These are behaviors that are difficult for ransomware to hide or change. The evaluation of UNVEIL shows that our approach was able to correctly detect 13,637 ransomware samples from multiple families in a real-world data feed with zero false positives. In fact, UNVEIL outperformed all ex- isting AV scanners and a modern industrial sandboxing technology in detecting both superficial and technically sophisticated ransomware attacks. Among our findings was also a new ransomware family that no security com- pany had previously detected before we submitted it to VirusTotal. 9"
Cybersecurity and Privacy,Alan Mislove,https://www.khoury.northeastern.edu/people/alan-mislove/,"Alan Mislove is a professor and the senior associate dean for academic affairs in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Network measurement; Networking; Online social networks; Security and privacy,"PhD in Computer Science, Rice University; MS in Computer Science, Rice University; BA in Computer Science, Rice University","November 2nd, 2018",Is the Web Ready for OCSP Must-Staple?,https://mislove.org/publications/OCSP-IMC.pdf," Taejoong Chung, Jay Lok, Balakrishnan Chandrasekaran, David Choffnes, Dave Levin, Bruce M. Maggs, Alan Mislove, John Rula, Nick Sullivan, and Christo Wilson. 2018. Is the Web Ready for OCSP Must-Staple?. In 2018 Internet Measurement Conference (IMC ’18), October 31-November 2, 2018, Boston, MA, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/ 10.1145/3278532.3278543","TLS, the de facto standard protocol for securing communications over the Internet, relies on a hierarchy of certificates that bind names to public keys. Naturally, ensuring that the communicating parties are using only valid certificates is a necessary first step in or- der to benefit from the security of TLS. To this end, most certificates and clients support OCSP, a protocol for querying a certificate’s revocation status and confirming that it is still valid. Unfortunately, however, OCSP has been criticized for its slow performance, unre- liability, soft-failures, and privacy issues. To address these issues, the OCSP Must-Staple certificate extension was introduced, which requires web servers to provide OCSP responses to clients during the TLS handshake, making revocation checks low-cost for clients. Whether all of the players in the web’s PKI are ready to support OCSP Must-Staple, however, remains still an open question. In this paper, we take a broad look at the web’s PKI and deter- mine if all components involved—namely, certificate authorities, web server administrators, and web browsers—are ready to sup- port OCSP Must-Staple. We find that each component does not yet fully support OCSP Must-Staple: OCSP responders are still not fully reliable, and most major web browsers and web server implemen- tations do not fully support OCSP Must-Staple. On the bright side, only a few players need to take action to make it possible for web server administrators to begin relying on certificates with OCSP ∗This work was done while the author was a postdoctoral re- searcher at Northeastern University. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. IMC ’18, October 31-November 2, 2018, Boston, MA, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5619-0/18/10...$15.00 https://doconclusion not found"
Cybersecurity and Privacy,Alan Mislove,https://www.khoury.northeastern.edu/people/alan-mislove/,"Alan Mislove is a professor and the senior associate dean for academic affairs in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Network measurement; Networking; Online social networks; Security and privacy,"PhD in Computer Science, Rice University; MS in Computer Science, Rice University; BA in Computer Science, Rice University","May 24th, 2018",Privacy Risks with Facebook’s PII-based Targeting: Auditing a Data Broker’s Advertising Interface,https://mislove.org/publications/PII-Oakland.pdf," G. Venkatadri et al., ""Privacy Risks with Facebook's PII-Based Targeting: Auditing a Data Broker's Advertising Interface,"" 2018 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, 2018, pp. 89-107.","—Sites like Facebook and Google now serve as de facto data brokers, aggregating data on users for the purpose of implementing powerful advertising platforms. Historically, these services allowed advertisers to select which users see their ads via targeting attributes. Recently, most advertising platforms have begun allowing advertisers to target users directly by uploading the personal information of the users who they wish to advertise to (e.g., their names, email addresses, phone numbers, etc.); these services are often known as custom audiences. Custom audiences effectively represent powerful linking mechanisms, allowing advertisers to leverage any PII (e.g., from customer data, public records, etc.) to target users. In this paper, we focus on Facebook’s custom audience implementation and demonstrate attacks that allow an adversary to exploit the interface to infer users’ PII as well as to infer their activity. Speciﬁcally, we show how the adversary can infer users’ full phone numbers knowing just their email address, determine whether a particular user visited a website, and de-anonymize all the visitors to a website by inferring their phone numbers en masse. These attacks can be conducted without any interaction with the victim(s), cannot be detected by the victim(s), and do not require the adversary to spend money or actually place an ad. We propose a simple and effective ﬁx to the attacks based on reworking the way Facebook de-duplicates uploaded information. Facebook’s security team acknowledged the vulnerability and has put into place a ﬁx that is a variant of the ﬁx we propose. Overall, our results indicate that advertising platforms need to carefully consider the privacy implications of their interfaces.The vast amounts of user data that social networking services have collected is now utilized by their advertising platforms to allow advertisers to target users via their PII. In this paper, we have shown how the inclusion of PII-based targeting opens up new privacy leaks in advertising platforms. By giving advertisers ﬁne-grained control over the set of users targeted, and by providing them with coarse-grained statistics of audience sizes, the platforms open themselves to powerful attacks that can let an adversary learn private information about users. While we have proposed a solution to the attacks we uncovered, our work shows that platforms need to carefully audit their interfaces when introducing PII-based targeting."
Cybersecurity and Privacy,Cristina Nita-Rotaru,https://www.khoury.northeastern.edu/people/cristina-nita-rotaru/,"Cristina Nita-Rotaru is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. She is a founding member of Northeastern's Cybersecurity and Privacy Institute.",Distributed systems and network security; Insider-resilient systems; Analytics for security and fault tolerance; Automated testing and verification,"PhD, Johns Hopkins University; MS, Politechnica University of Bucharest — Romania; BS, Politechnica University of Bucharest —Romania","November 15th, 2017",Taking a Long Look at QUIC: An Approach for Rigorous Evaluation of Rapidly Evolving Transport Protocols,https://www.sjero.net/pubs/2017_IMC_QUIC.pdf," Arash Molavi Kakhki, Samuel Jero, David Choffnes, Alan Mislove, and Cristina Nita-Rotaru In Proceedings of ACM Internet Measurement Conference (IMC'17), London, United Kingdom, Nov 2017.","Google’s QUIC protocol, which implements TCP-like properties at the application layer atop a UDP transport, is now used by the vast majority of Chrome clients accessing Google properties but has no formal state machine specification, limited analysis, and ad-hoc evaluations based on snapshots of the protocol implementation in a small number of environments. Further frustrating attempts to evaluate QUIC is the fact that the protocol is under rapid develop- ment, with extensive rewriting of the protocol occurring over the scale of months, making individual studies of the protocol obsolete before publication. Given this unique scenario, there is a need for alternative tech- niques for understanding and evaluating QUIC when compared with previous transport-layer protocols. First, we develop an ap- proach that allows us to conduct analysis across multiple versions of QUIC to understand how code changes impact protocol effec- tiveness. Next, we instrument the source code to infer QUIC’s state machine from execution traces. With this model, we run QUIC in a large number of environments that include desktop and mobile, wired and wireless environments and use the state machine to understand differences in transport- and application-layer perfor- mance across multiple versions of QUIC and in different environ- ments. QUIC generally outperforms TCP, but we also identified performance issues related to window sizes, re-ordered packets, and multiplexing large number of small objects; further, we identify that QUIC’s performance diminishes on mobile devices and over cellular networks. ACM Reference Format: Arash Molavi Kakhki, Samuel Jero, David Choffnes, Cristina Nita-Rotaru, and Alan Mislove. 2017. Taking a Long Look at QUIC. In Proceedings of IMC ’17, London, United Kingdom, November 1–3, 2017, 14 pages. https://dos. For example, consider the case of Google App Engine (GAE), which supports QUIC and allows us to host our own content for testing. While the latency to GAE frontends was low and constant over time, we found a variable wait time between connection estab- lishment and content being served (Fig. 2, middle bar). We do not know the origins for these variable delays, but we suspect that the constant RTT is due to proxying at the frontend, and the variable delay component is due to GAE’s shared environment without re- source guarantees. The variability was present regardless of time of day, and did not improve when requesting the same content sequentially (thus making it unlikely that the GAE instance was spun down for inactivity). Such variable delay can dominate PLT measurements for small web pages, and cannot reliably be isolated when multiplexing requests. To avoid these issues, we opted instead to run our own QUIC servers. This raises the question of how to configure QUIC pa- rameters to match those used in deployment. We use a two-phase approach. First, we extract all parameters that are exchanged be- tween client and server (e.g., window sizes) and ensure that our QUIC server uses the same ones observed from Google. For parameters not exposed by the QUIC server to the client, we use grey-box testing to infer the likely parameters being used. Specifically, we vary server-side parameters until we obtain perfor- mance that matches QUIC from Google servers. The end result is shown in Fig. 2. The left bar shows that QUIC as configured in the public code release takes twice as long to download a large file when compared to the configuration that most closely matches Google’s QUIC performance (right bar)10. We made two changes to achieve parity with Google’s QUIC servers. First, we increased the maximum allowed congestion win- dow size. At the time of our experiments, this value was 107 by default in Chrome. We increased this value to 430, which matched the maximum allowed congestion window in Chromium’s devel- opment channel. Second, we found and fixed a bug in QUIC that prevented the slow start threshold from being updated using the 10We focus on PLT because it is the metric we use for end-to-end performance comparisons through- out the paper. State Description Init Initial connection establishment Slow Start Slow start phase Congestion Avoidance (CA) Normal congestion avoidance CA-Maxed Max allowed win. size is reached Application Limited Current cong. win. is not being uti- lized, hence window will not be increased Retransmission Timeout Loss detected due to timeout for ACK Recovery Proportional rate reduction fast re- covery Tail Loss Probe [22] Recover tail losses Table 3: QUIC states (Cubic CC) and their meanings. receiver-advertised buffer size. Failure to do so caused poor perfor- mance due to early exit from slow start.11 Prior work did no such calibration. This explains why they ob- served poor QUIC performance in high bandwidth environments or when downloading large web pages [16, 20, 30]. 4.2 Instrumentation While our tests can tell us how QUIC and TCP compare to each other under different circumstances, it is not clear what exactly causes these differences in performance. To shed light on this, we compile QUIC clients and servers from source (using QUIC versions 25 and 34) and instrument them to gain access to the inner workings of the protocol. QUIC implements TCP-like congestion control. To reason about QUIC’s behavior, we instrumented our QUIC server to collect logs that allow us to infer QUIC’s state machine from execution traces, and to track congestion window size and packet loss detection. Table 3 lists QUIC’s congestion control states. We use statistics about state transitions and the frequency of visiting each state to understand the root causes behind good or bad performance for QUIC. For example, we found that the reason QUIC’s performance suffers in the face of packet re-ordering is that re-ordered packets cause QUIC’s loss-detection mechanism to report high numbers of false losses. Note that we evaluate QUIC as a whole, in lieu of isolating the im- pact of protocol components (e.g., congestion avoidance, TLP, etc.). We found that disentangling and changing other (non-modular) parts of QUIC (e.g., to change loss recovery techniques, add HOL blocking, change how packets are ACKed) requires rewriting sub- stantial amount of code, and it is not always clear how to replace them. This is an interesting topic to explore in future work. 5 ANALYSIS In this section, we conduct extensive measurements and analysis to understand and explain QUIC performance. We begin by focus- ing on the protocol-layer behavior, QUIC’s state machine, and its fairness to TCP. We then evaluate QUIC’s application-layer per- formance, using both page load times (PLT) and video streaming as example application metrics. Finally, we examine the evolution 11We confirmed our changes with a member of the QUIC team at Google. He also confirmed our bug report. Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom (a) QUIC’s Cubic CC (b) QUIC’s BBR CC Figure 3: State transition diagram for QUIC’s CC. 0 1 2 3 4 5 0 20 40 60 80 100 Throughput (Mbps) Time (s) QUIC TCP (a) QUIC vs. TCP 0 1 2 3 4 5 0 20 40 60 80 100 Throughput (Mbps) Time (s) QUIC TCP1 TCP2 (b) QUIC vs. two TCP flows Figure 4: Timeline showing unfairness between QUIC and TCP when transferring data over the same 5 Mbps bottle- neck link (RTT=36ms, buffer=30 KB). of QUIC’s performance and evaluate the performance that QUIC “leaves on the table” by encrypting transport-layer headers that prevent transparent proxying commonly used in cellular (and other high-delay) networks. 5.1 State Machine and Fairness In this section, we analyze high-level properties of the QUIC proto- col using our framework. State machine. QUIC has only a draft formal specification and no state machine diagram or formal model; however, the source code is made publicly available. Absent such a model, we took an empirical approach and used traces of QUIC execution to infer the state machine to better understand the dynamics of QUIC and their impact on performance. Specifically, we use Synoptic [15] for automatic generation of QUIC state machine. While static analysis might generate a more complete state machine, a complete model is not necessary for understanding performance changes. Rather, as we show in Sec- tion 5.2, we only need to investigate the states visited and transitions between them at runtime. Scenario Flow Avg. throughput (std. dev.) QUIC vs. TCP QUIC 2.71 (0.46) TCP 1.62 (1.27) QUIC vs. TCPx2 QUIC 2.8 (1.16) TCP 1 0.7 (0.21) TCP 2 0.96 (0.3) QUIC vs. TCPx4 QUIC 2.75 (1.2) TCP 1 0.45 (0.14) TCP 2 0.36 (0.09) TCP 3 0.41 (0.11) TCP 4 0.45 (0.13) Table 4: Average throughput (5 Mbps link, buffer=30 KB, av- eraged over 10 runs) allocated to QUIC and TCP flows when competing with each other. Despite the fact that both pro- tocols use Cubic congestion control, QUIC consumes nearly twice the bottleneck bandwidth than TCP flows combined, resulting in substantial unfairness. Fig. 3a shows the QUIC state machine automatically generated using traces from executing QUIC across all of our experiment configurations. The diagram reveals behaviors that are common to standard TCP implementations, such as connection start (Init, SlowStart), congestion avoidance (CongestionAvoidance), and receiver-limited connections (ApplicationLimited). QUIC also includes states that are non-standard, such as a maximum sending rate (CongestionAvoidanceMaxed), tail loss probes, and propor- tional rate reduction during recovery. Note that capturing the empirical state machine requires in- strumenting QUIC’s source code with log messages that capture transitions between states. In total, this required adding 23 lines of code in 5 files. While the initial instrumentation required approxi- mately 10 hours, applying the instrumentation to subsequent QUIC versions required only about 30 minutes. To further demonstrate how our approach applies to other congestion control implementa- tions, we instrumented QUIC’s experimental BBR implementation and present its state transition diagram in Fig. 3b. This instrumen- tation took approximately 5 hours. Thus, our experience shows that our approach is able to adapt to evolving protocol versions and implementations with low additional effort. IMC ’17, November 1–3, 2017, London, United Kingdom A. Molavi Kakhki et al. 0 20 40 60 80 0 10 20 30 40 50 60 70 80 90 100 Cong. Win. (KB) QUIC TCP (a) QUIC vs. TCP 0 20 40 60 80 20 21 22 23 24 25 Cong. Win. (KB) Time (s) (b) 5-second zoom of above figure Figure 5: Timeline showing congestion window sizes for QUIC and TCP when transferring data over the same 5 Mbps bottleneck link (RTT=36ms, buffer=30 KB). We used inferred state machines for root cause analysis of per- formance issues. In later sections, we demonstrate how they helped us understand QUIC’s poor performance on mobile devices and in the presence of deep packet reordering. Fairness. An essential property of transport-layer protocols is that they do not consume more than their fair share of bottleneck bandwidth resources. Absent this property, an unfair protocol may cause performance degradation for competing flows. We evaluated whether this is the case for the following scenarios, and present aggregate results over 10 runs in Table 4. We expect that QUIC and TCP should be relatively fair to each other because they both use the Cubic congestion control protocol. However, we find this is not the case at all. • QUIC vs. QUIC. We find that two QUIC flows are fair to each other. We also found similar behavior for two TCP flows. • QUIC vs. TCP. QUIC multiplexes requests over a single con- nection, so its designers attempted to set Cubic congestion control parameters so that one QUIC connection emulates N TCP connections (with a default of N = 2 in QUIC 34, and N = 1 in QUIC 37). We found that N had little impact on fair- ness. As Fig. 4a shows, QUIC is unfair to TCP as predicted, and consumes approximately twice the bottleneck bandwidth of TCP even with N = 1. We repeated these tests using different buffer sizes, including those used by Carlucci et al. [17], but did not observe any significant effect on fairness. This directly contradicts their finding that larger buffer sizes allow TCP and QUIC to fairly share available bandwidth. • QUIC vs. multiple TCP connections. When competing with M TCP connections, one QUIC flow should consume 2/(M + 1) of the bottleneck bandwidth. However, as shown in Table 4 and Fig. 4b, QUIC still consumes more than 50% of the bottleneck bandwidth even with 2 and 4 competing TCP flows. Thus, QUIC is not fair to TCP even assuming 2-connection emulation. (a) Varying object size (b) Varying object count Figure 6: QUIC (version 34) vs. TCP with different rate lim- its for (a) different object sizes and (b) with different num- bers of objects. Each heatmap shows the percent difference between QUIC over TCP. Positive numbers—colored red— mean QUIC outperforms TCP and has smaller page-load time. Negative numbers—colored blue—means the opposite. White cells indicate no statistically significant difference. Figure 7: QUIC with and without 0-RTT. Positive numbers— colored red—show the performance gain achieved by 0-RTT. The gain is more significant for small objects, but becomes insignificant as the bandwidth decreases and/or objects be- come larger, where connection establishment is a tiny frac- tion of total PLT. To ensure fairness results were not an artifact of our testbed, we repeated these tests against Google servers. The unfairness results were similar. We further investigate why QUIC is unfair to TCP by instrument- ing the QUIC source code, and using tcpprobe [13] for TCP, to extract the congestion window sizes. Fig. 5a shows the congestion window over time for the two protocols. When competing with TCP, QUIC is able to achieve a larger congestion window. Taking a closer look at the congestion window changes (Fig. 5b), we find that while both protocols use Cubic congestion control scheme, QUIC increases its window more aggressively (both in terms of slope, and in terms of more frequent window size increases). As a result, QUIC is able to grab available bandwidth faster than TCP does, leaving TCP unable to acquire its fair share of the bandwidth. 5.2 Page Load Time This section evaluates QUIC performance compared to TCP for loading web pages (i.e., page load time, or PLT) with different sizes and numbers of objects. Recall from Sec. 3 that we measure PLT using information gathered from Chrome, that we run TCP and QUIC experiments back-to-back, and that we conduct experiments in a variety of emulated network settings. Note that our servers add all necessary HTTP directives to avoid caching content. We also clear the browser cache and close all sockets between experi- ments to prevent “warmed up” connections from impacting results. Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom (a) Varying object size,1% Loss (b) Varying object size, 112 ms RTT (c) Varying object size, 112 ms RTT with 10ms jitter that causes packet reordering (d) Varying #object, 1% Loss (e) Varying #object, 112 ms RTT (f) Varying #object, 112 ms RTT with 10ms jitter that causes packet reordering Figure 8: QUIC v34 vs. TCP at different rate limits, loss, and delay for different object sizes (a, b, and c) and different numbers of objects (d, e, and f). However, we do not clear the state used for QUIC’s 0-RTT con- nection establishment. Furthermore, our PLTs do not include any DNS lookups. This is achieved by extracting resource loading time details from Chrome and excluding the DNS lookups times. In the results that follow, we evaluate whether the observed performance differences are statistically significant or simply due to noise in the environment. We use the Welch’s t-test [14], a two- sample location test which is used to test the hypothesis that two populations have equal means. For each scenario, we calculate the p-value according to the Welch’s t-test. If the p-value is smaller than our threshold (0.01), then we reject the null hypothesis that the mean performance for TCP and QUIC are identical, implying the difference we observe between the two protocols is statistically significant. Otherwise the difference we observe is not significant and is likely due to noise. Desktop environment. We begin with the desktop environ- ment and compare QUIC with TCP performance for different rates, object sizes, and object counts—without adding extra delay or loss (RTT = 36ms and loss = 0%). Fig. 6 shows the results as a heatmap, where the color of each cell corresponds to the percent PLT dif- ference between QUIC and TCP for a given bandwidth (vertical dimension) and object size/number (horizontal direction). Red indi- cates that QUIC is faster (smaller PLT), blue indicates that TCP is faster, and white indicates statistically insignificant differences. Our key findings are that QUIC outperforms TCP in every sce- nario except in the case of large numbers of small objects. QUIC’s performance gain for smaller object sizes is mainly due to QUIC’s 0-RTT connection establishment—substantially reducing delays related to secure connection establishment that corresponds to a substantial portion of total transfer time in these cases. To isolate the impact of 0-RTT, we plotted performance differences between QUIC with and without 0-RTT enabled in Fig. 7. As expected, the benefit is relatively large for small objects and statistically insignif- icant for 10MB objects. To investigate the reason why QUIC performs poorly for large numbers of small objects, we explored different values for QUIC’s Maximum Streams Per Connection (MSPC) parameter to control the level of multiplexing (the default is 100 streams). We found there was no statistically significant impact for doing so, except when setting the MSPC value to a very low number (e.g., 1), which worsens performance substantially. Instead, we focused on QUIC’s congestion control algorithm and identified that in such cases, QUIC’s Hybrid Slow Start [24] causes early exit from Slow Start due to an increase in the minimum observed RTT by the sender, which Hybrid Slow Start uses as an indication that the path is getting congested. This can hurt the PLT significantly when objects are small and the total transfer time is not long enough for the congestion window to increase to its maximum value. Note that the same issue (early exit from Hybrid Slow Start) affects the scenario with a large number of large objects, but QUIC nonetheless outperforms TCP because it has enough time to increase its congestion window and remain at high utilization, thus compensating for exiting Slow Start early.12 Desktop with added delay and loss. We repeat the experi- ments in the previous section, this time adding loss, delay, and jitter. Fig. 8 shows the results, again using heatmaps. Our key observations are that QUIC outperforms TCP under loss (due to better loss recovery and lack of HOL blocking), and in high- delay environments (due to 0-RTT connection setup). However, in the case of high latency, this is not enough to compensate for QUIC’s poor performance for large numbers of small objects. Fig. 9 shows the congestion window over time for the two protocols at 100Mbps and 1% loss. Similar to Fig. 5, under the same network conditions QUIC better recovers from loss events and adjusts its congestion window faster than TCP, resulting in a larger congestion window on average and thus better performance. Under variable delays, QUIC performs significantly worse than TCP. Using our state machine approach, we observed that under variable delay QUIC spends significantly more time in the recovery state compared to relatively stable delay scenarios. To investigate 12We leave investigating the reason behind sudden increase in the minimum observed RTT when multiplexing many objects to future work. IMC ’17, November 1–3, 2017, London, United Kingdom A. Molavi Kakhki et al. 10 30 50 70 Cong. Win. (KB) QUIC 10 30 50 70 1 2 3 4 5 6 7 8 9 10 Cong. Win. (KB) Time (s) TCP Figure 9: Congestion window over time for QUIC and TCP at 100Mbps rate limit and 1% loss. Figure 10: QUIC vs. TCP when downloading a 10MB page (112 ms RTT with 10ms jitter that causes packet reordering). Increasing the NACK threshold for fast retransmit allows QUIC to cope with packet reordering. this, we instrumented QUIC’s loss detection mechanism, and our analysis reveals that variable delays cause QUIC to incorrectly infer packet loss when jitter leads to out-of-order packet delivery. This occurs in our testbed because netem adds jitter by assigning a delay to each packet, then queues each packet based on the adjusted send time, not the packet arrival time—thus causing packet re-ordering. The reason that QUIC cannot cope with packet re-ordering is that it uses a fixed threshold for number of NACKs (default 3) before it determines that a packet is lost and responds with a fast retransmit. Packets reordered deeper than this threshold cause false positive loss detection.13 In contrast, TCP uses the DSACK algorithm [41] to detect packet re-ordering and adapt its NACK threshold accordingly. As we will show later in this section, packet reordering occurs in the cellular networks we tested, so in such cases QUIC will benefit from integrating DSACK. We quantify the impact of using larger DSACK values in Fig. 10, demonstrating that in the presence of packet reordering larger NACK thresholds substantially improve end to end performance compared to smaller NACK thresholds. We shared this result with a QUIC engineer, who subsequently informed us that the QUIC team is experimenting with dynamic threshold and time-based solutions to avoid falsely inferring loss in the presence of reordering. Desktop with variable bandwidth. The previous tests set a static threshold for the available bandwidth. However, in practice 13Note that reordering impact when testing small objects is insignificant because QUIC does not falsely detect losses until a sufficient number of packets are exchanged. 0 40 80 120 0 50 100 150 200 Throughput (Mbps) Time (s) TCP QUIC Figure 11: QUIC vs. TCP when downloading a 210MB object. Bandwidth fluctuates between 50 and 150Mbps (randomly picks a rate in that range every one second). Averaging over 10 runs, QUIC is able to achieve an average throughput of 79Mbps (STD=31) while TCP achieves an average through- put of 46Mbps (STD=12). such values will fluctuate over time, particularly in wireless net- works. To investigate how QUIC and TCP compare in environments with variable bandwidth, we configured our testbed to change the bandwidth randomly within specified ranges and with different frequencies. Fig. 11 shows the throughput over time for three back-to-back TCP and QUIC downloads of a 210MB object when the bandwidth randomly fluctuates between 50 and 150Mbps. As shown in this figure, QUIC is more responsive to bandwidth changes and is able to achieve a higher average throughput compared to TCP. We re- peated this experiment with different bandwidth ranges and change frequencies and observed the same behavior in all cases. Mobile environment. Due to QUIC’s implementation in userspace (as opposed to TCP’s implementation in the OS kernel), resource contention might negatively impact performance indepen- dent of the protocol’s optimizations for transport efficiency. To test whether this is a concern in practice, we evaluated an increasingly common resource-constrained deployment environment: smart- phones. We use the same approach as in the desktop environment, controlling Chrome (with QUIC enabled) over two popular Android phones: the Nexus 6 and the MotoG. These phones are neither top- of-the-line, nor low-end consumer phones, and we expect that they approximate the scenario of a moderately powerful mobile device. Fig. 12 shows heatmaps for the two devices when varying band- width and object size.14 We find that, similar to the desktop envi- ronment, in mobile QUIC outperforms TCP in most cases; however, its advantages diminish across the board. To understand why this is the case, we investigate the QUIC congestion control states visited most in mobile and non-mobile scenarios under the same network conditions. We find that in mo- bile QUIC spends most of its time (58%) in the “Application Limited” state, which contrasts substantially with the desktop scenario (only 7% of the time). The reason for this behavior is that QUIC runs in a userspace process, whereas TCP runs in the kernel. As a result, QUIC is unable to consume received packets as quickly as on a desk- top, leading to suboptimal performance, particularly when there is ample bandwidth available.15 Fig. 13 shows the full state diagram (based on server logs) in both environments for 50Mbps with no added latency or loss. By revealing the changes in time spent in 14We omit 100 Mbps because our phones cannot achieve rates beyond 50 Mbps over WiFi, and we omit results from varying the number of objects because they are similar to the single-object cases. 15A parallel study from Google [26] using aggregate data identifies the same performance issue but does not provide root cause analysis. Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom (a) MotoG, No added loss or latency (b) MotoG, 1% Loss (c) MotoG, 112 ms RTT (d) Nexus6, No added loss or latency (e) Nexus6, 1% Loss (f) Nexus6, 112 ms RTT Figure 12: QUICv34 vs. TCP for varying object sizes on MotoG and Nexus6 smartphones (using WiFi). We find that QUIC’s improvements diminish or disappear entirely when running on mobile devices. (a) MotoG (b) Desktop Figure 13: QUIC state transitions on MotoG vs. Desktop. QUICv34, 50 Mbps, no added loss or delay. Red numbers in- dicate the fraction of time spent in each state, and black numbers indicate the state-transition probability. The fig- ure shows that poor performance for QUIC on mobile de- vices can be attributed to applications not processing pack- ets quickly enough. Note that the zero transition probabili- ties are due to rounding down. each state, such inferred state machines help diagnose problems and develop a better understanding of QUIC dynamics. Tests on commercial cellular networks. We repeated our PLT tests—without any network emulation—over Sprint’s and Veri- zon’s cellular networks, using both 3G and LTE. Table 5 shows the characteristics of these networks at the time of the experiment. To isolate the impact of the network from that of the device they run on, we used our desktop client tethered to a mobile network instead of using a mobile device (because the latter leads to suboptimal performance for QUIC, shown in Fig. 12 and 13). We otherwise keep the same server and client settings as described in Sec. 3.1. Fig. 14 shows the heatmaps for these tests. For LTE, QUIC per- forms similarly to a desktop environment with low bandwidth (Fig. 7). In these cell networks, the benefit of 0-RTT is larger for the 1MB page due to higher latencies in the cellular environment. (a) Varying object size (b) Varying object count Figure 14: QUICv34 vs. TCP over Verizon and Sprint cellular networks. Thrghpt. (Mbps) RTT (STD) (ms) Reordering (%) Loss (%) 3G LTE 3G LTE 3G LTE 3G LTE Verizon 0.17 4.0 109 (20) 62 (14) 9 0.25 0.05 0 Sprint 0.31 2.4 70 (39) 55 (11) 1.38 0.13 0.02 0.02 Table 5: Characteristics of tested cell networks. Throughput and RTT represent averages. In the case of 3G, we see the benefits of QUIC diminish. Com- pared to LTE, the bandwidth in 3G is much lower and the loss is higher—which works to QUIC’s benefit (see Fig. 8a). However, the packet reordering rates are higher compared to LTE, and this works to QUIC’s disadvantage. Note that in 3G scenarios, in many cases QUIC had better performance on average (i.e., lower average PLT); however, the high variance resulted in high p-values, which means we cannot reject the null hypothesis that the two sample sets were drawn form the same (noisy) distribution. 5.3 Video-streaming Performance This section investigates QUIC’s impact on video streaming in the desktop environment. Unlike page load times, which tend to be limited by RTT and multiplexing, video streaming relies on the transport layer to load large objects sufficiently quickly to maintain smooth playback. This exercises a transport-layer’s ability to quickly ramp up and maintain high utilization. We test video-streaming performance using YouTube, which supports both QUIC and TCP. We evaluate the protocols using well known QoE metrics for video such as the time spent waiting for IMC ’17, November 1–3, 2017, London, United Kingdom A. Molavi Kakhki et al. (a) MACW=430 (b) MACW=2000 Figure 15: QUIC (version 37) vs. TCP with different max- imum allowable congestion window (MACW) size. In (a), MACW=430 and QUIC versions 34 and 37 have identical per- formance (see Fig. 6a), In (b), we use the default MACW=2000 for QUIC 37, which results in higher throughput and larger performance gains for large transfers in high bandwidth networks. initial playback, and the number of rebuffering events. For the latter metric, Google reports that, on average users experience 18% fewer re-buffers when watching YouTube videos over QUIC [26]. We developed a tool for automatically streaming a YouTube video and logging quality of experience (QoE) metrics via the API mentioned in Sec. 3.3. The tool opens a one-hour-long YouTube video, selects a specific quality level, lets the video run for 60 sec- onds, and logs the following QoE metrics: time to start the video, video quality, quality changes, re-buffering events, and fraction of video loaded. As we demonstrated in previous work [31], 60 seconds is sufficient to capture QoE differences. We use this tool to stream videos using QUIC and TCP and compare the QoE for the two protocols. Table 6 shows the results for 100 Mbps bandwidth and 1% loss,16 a condition under which QUIC outperforms TCP (Sec. 8). In this environment, at low and medium resolutions we see no significant difference in QoE metrics, but for the highest quality, hd2160, QUIC is able to load a larger fraction of the video in 60 seconds and experience fewer rebuffers per time played, which is consistent with our PLT test results (Sec. 5.2) and with what Google reported. Thus, to refine their observations, we find that QUIC can outperform TCP for video streaming, but this matters only for high resolutions. 5.4 Historical Comparison To understand how QUIC performance has changed over time, we evaluated 10 QUIC versions (25 to 34)17 in our testbed. In order to only capture differences due to QUIC version changes, and not due to different configuration values, we used the same version of Chrome and the same QUIC parameter configuration when testing different QUIC versions. We found that when using the same configuration most QUIC versions in this range yielded nearly identical results, despite sub- stantial changes to the QUIC codebase (including reorganization of code that frustrated our attempts to instrument it). This is corrobo- rated by changelogs [12] that indicate most modifications were to the cryptography logic, QUIC flags, and connection IDs. Based on the relatively stable QUIC performance across recent versions, we expect that our observations about its performance using its current congestion control algorithm are likely to hold in 16We observed similar results for 10 and 50 Mbps under similar loss. 17The stable version of Chrome at the time of analysis (52) did not support QUIC versions earlier than 25. To avoid false inferences from using different Chrome versions and different QUIC versions, we only tested QUIC versions 25 and higher. Client's machine Server Router (running network emulator) Proxy 40ms RTT 40ms RTT 80ms RTT Figure 16: QUIC proxy test setup. The proxy is located mid- way between client and server. the future (except in places where we identified opportunities to improve the protocol). Note that at the time of writing, the recently proposed BBR congestion control algorithm has not been deployed in the “stable” branch and thus we could not evaluate its performance fairly against Cubic in QUIC or TCP. Private communication with a QUIC team member indicated that BBR is “not yet performing as well as Cubic in our deployment tests.” Comparison with QUIC 37. At the time of publication, the latest stable version of Chromium was 60.0.3112.101, which includes QUIC 37 as the latest stable version. To enhance our longitudinal analysis and demonstrate how our approach easily adapts to new versions of QUIC, we instrumented, tested, and compared QUIC 37 with 34 (the one used for experiments through out this paper). We found that the main change in QUIC 37 is that the maximum allowed congestion window (MACW) increased to 2000 (from 430 used in our experiments) in the new versions of Chromium. This al- lows QUIC to achieve much higher throughput compared to version 34, particularly improving performance when compared with TCP for large transfers in high bandwidth networks. Fig. 15 shows the comparison between TCP and QUIC version 37 for various object sizes with MACW of 430 (Fig. 15a) and 2000 (Fig. 15b). When com- paring Fig. 15a and 6a, we find that QUIC versions 34 and 37 have almost identical performance when using the same MACW; this is corroborated by QUIC version changelogs [12]. All our previous findings, e.g., QUIC performance degradation in presence of deep packet reordering, still hold for this new version of QUIC. 5.5 Impact of Proxying We now test the impact of QUIC’s design decisions on in-network performance optimization. Specifically, many high-latency net- works use transparent TCP proxies to reduce end-to-end delays and improve loss recovery [40]. However, due to the fact that QUIC en- crypts not only payloads but also transport headers, such proxying is impossible for in-network devices. We evaluate the extent to which this decision impacts QUIC’s potential performance gains. Specifically, we wrote a QUIC proxy, and co-located it with a TCP proxy so that we could compare the impact of proxying on end-to-end performance (Fig. 16). For these experiments, we consider PLTs as done in previous sections. We present two types of comparison results: QUIC vs. proxied TCP (as this is what one would expect to find in many cellular net- works), and QUIC vs. proxied QUIC (to determine how QUIC would benefit if proxies could transparently terminate QUIC connections). For the former case, we find that QUIC continues to outperform Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom Quality Time to Start video loaded Buffer/Play Time† (%) #rebuffers #rebuffers (secs) in 1 min (%) per playing secs QUIC TCP QUIC TCP QUIC TCP QUIC TCP QUIC TCP tiny 0.5 (0.11) 0.5 (0.21) 33.8 (0.01) 33.8 (0.01) 0.9 (0.17) 0.9 (0.34) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) medium 0.9 (1.04) 0.5 (0.13) 17.9 (0.01) 12.9 (0.92) 1.4 (1.62) 0.9 (0.22) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) hd720 0.7 (0.16) 0.7 (0.18) 8.0 (0.27) 4.3 (0.28) 1.1 (0.27) 1.1 (0.27) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) hd2160 5.9 (2.73) 5.9 (2.51) 0.8 (0.05) 0.4 (0.01) 50.2 (3.01) 73.1 (1.91) 6.7 (0.46) 4.9 (0.3) 0.2 (0.01) 0.3 (0.01) Table 6: Mean (std) of QoE metrics for a YouTube video in different qualities, averaged over 10 runs. 100Mbps, 1% loss. QUIC benefits are clear for high qualities. While the absolute number of rebuffers for QUIC is higher for hd2160, it is able to load and play more of the video in a given time compared to TCP, with fewer (about 30%) rebuffers per playing second. †Buffer/Play Time is the time spent while buffering the video divided by the time playing video. (a) No added loss or latency (b) 1% Loss (c) 100ms added RTT Figure 17: QUIC vs. TCP proxied, where red cells indicate that QUIC performs better. (a) No added loss or latency (b) 1% Loss (c) 100ms added RTT Figure 18: QUIC with and without proxy when downloading objects with different sizes. Positive numbers (red cells) mean QUIC performs better connecting to the server directly. TCP in many scenarios, but its benefits diminish or entirely dis- appear compared to unproxied TCP in low loss/latency cases, and when there is 1% loss. In the case of high delay links, QUIC still outperforms TCP (Fig. 17). Thus, proxies can help TCP to recover many of the benefits of QUIC, but primarily in lossy scenarios, and when the proxy is equidistant from the client and server. In the case of implementing a QUIC proxy (Fig. 18), we find that a proxy hurts performance for small object sizes (likely due to inefficiencies and the inability to establish connections via 0- RTT), but performance is better under loss for large objects. Taken together, our initial attempt at a QUIC proxy provides mixed results, and identifying any other potential benefits will require additional tuning of the proxy code. 6 CONCLUSION In this paper, we address the problem of evaluating an application- layer transport protocol that was built without a formal specifica- tion, is rapidly evolving, and is deployed at scale with nonpublic configuration parameters. To do so, we use a methodology and testbed that allows us to conduct controlled experiments in a vari- ety of network conditions, instrument the protocol to reason about its performance, and ensure that our evaluations use settings that approximate those deployed in the wild. We used this approach to evaluate QUIC, and found cases where it performs well and poorly— both in traditional desktop environments but also in mobile and proxy scenarios not previously tested. With the help of an inferred protocol state machine and information about time spent in each state, we explained the performance results we observed. There are a number of open questions we plan to address in future work. First, we will evaluate performance in additional oper- ational networks, particularly in more mobile ones and data centers. Second, we will investigate techniques to improve QUIC’s fairness to TCP while still maintaining high utilization. Third, we will au- tomate the steps used for analysis in our approach and port it to other application layer protocols. This includes adapting our state- machine inference approach to other protocols, and we encourage developers to annotate state transitions in their code to facilitate such analysis. We believe doing so can lead to a more performant, reliable evolution for such network protocols."
Cybersecurity and Privacy,Cristina Nita-Rotaru,https://www.khoury.northeastern.edu/people/cristina-nita-rotaru/,"Cristina Nita-Rotaru is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. She is a founding member of Northeastern's Cybersecurity and Privacy Institute.",Distributed systems and network security; Insider-resilient systems; Analytics for security and fault tolerance; Automated testing and verification,"PhD, Johns Hopkins University; MS, Politechnica University of Bucharest — Romania; BS, Politechnica University of Bucharest —Romania","August 1st, 2017",Identifier Binding Attacks and Defenses in Software-Defined Networks,https://nds2.ccs.neu.edu/papers/persona_usenix2017.pdf," Samuel Jero, William Koch, Richard Skowyra, Hamed Okhravi, Cristina Nita-Rotaru, David Bigelow. USENIX Security 2017, August 2017",Failed to download
Cybersecurity and Privacy,Cristina Nita-Rotaru,https://www.khoury.northeastern.edu/people/cristina-nita-rotaru/,"Cristina Nita-Rotaru is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. She is a founding member of Northeastern's Cybersecurity and Privacy Institute.",Distributed systems and network security; Insider-resilient systems; Analytics for security and fault tolerance; Automated testing and verification,"PhD, Johns Hopkins University; MS, Politechnica University of Bucharest — Romania; BS, Politechnica University of Bucharest —Romania","June 26th, 2017",SymCerts: Practical Symbolic Execution For Exposing Noncompliance in X.509 Certificate Validation Implementations,https://www.ieee-security.org/TC/SP2017/papers/231.pdf," Sze Yiu Chau, Omar Chowdhury, Endadul Hoque, Huangyi Ge, Aniket Kate, Cristina Nita-Rotaru, Ninghui Li. IEEE Security and Privacy, May 2017","—The X.509 Public-Key Infrastructure has long been used in the SSL/TLS protocol to achieve authentication. A recent trend of Internet-of-Things (IoT) systems employing small foot- print SSL/TLS libraries for secure communication has further propelled its prominence. The security guarantees provided by X.509 hinge on the assumption that the underlying implementa- tion rigorously scrutinizes X.509 certiﬁcate chains, and accepts only the valid ones. Noncompliant implementations of X.509 can potentially lead to attacks and/or interoperability issues. In the literature, black-box fuzzing has been used to ﬁnd ﬂaws in X.509 validation implementations; fuzzing, however, cannot guarantee coverage and thus severe ﬂaws may remain undetected. To thoroughly analyze X.509 implementations in small footprint SSL/TLS libraries, this paper takes the complementary approach of using symbolic execution. We observe that symbolic execution, a technique proven to be effective in ﬁnding software implementation ﬂaws, can also be leveraged to expose noncompliance in X.509 implementations. Directly applying an off-the-shelf symbolic execution engine on SSL/TLS libraries is, however, not practical due to the problem of path explosion. To this end, we propose the use of SymCerts, which are X.509 certiﬁcate chains carefully constructed with a mixture of symbolic and concrete values. Utilizing SymCerts and some domain-speciﬁc optimizations, we symbolically execute the certiﬁcate chain validation code of each library and extract path constraints describing its accepting and rejecting certiﬁcate universes. These path constraints help us identify missing checks in different libraries. For exposing subtle but intricate noncom- pliance with X.509 standard, we cross-validate the constraints extracted from different libraries to ﬁnd further implementation ﬂaws. Our analysis of 9 small footprint X.509 implementations has uncovered 48 instances of noncompliance. Findings and suggestions provided by us have already been incorporated by developers into newer versions of their libraries.s about whether a given certiﬁcate chain is valid, even though it is not clear which implementation is noncompliant, we can conclude that one of the libraries is noncompliant. Precisely, for any two implementations I1 and I2 and their corresponding sets A1, R1, A2, and R2, any c ∈C such that (1) c ∈A1 ∧c ∈R2 or (2) c ∈A2 ∧c ∈R1 represents an instance of noncompliance. One can utilize the path constraints from two different im- plementations to ﬁnd inconsistent conclusions in the following two ways. In our analysis, we follow approach 2. Let us assume for any two given implementations Ip and Iq, we have the following sets: Ap = {ap 1, ap 2, . . . , ap n} (accepting certiﬁcate universe of Ip) Rp = {r p 1 , r p 2 , . . . , r p m} (rejecting certiﬁcate universe of Ip) Aq = {aq 1, aq 2, . . . , aq s} (accepting certiﬁcate universe of Iq) Rq = {r q 1 , r q 2 , . . . , r q t } (rejecting certiﬁcate universe of Iq) Approach 1: To detect inconsistencies between Ip and Iq, one can check to see whether either of the following formulas is satisﬁable: ¬(W (1≤i≤n) ap i ↔W (1≤j≤s) aq j ) and ¬(W (1≤i≤m) r p i ↔W (1≤j≤t) r q j ) (↔stands for logical equiva- lence). The ﬁrst (resp., second) formula asserts that the accept- ing (resp., rejecting) paths of Ip and Iq are not equivalent. Any model of either of the formulas will signify a noncompliant instance. We, however, do not utilize this approach to detect noncompliance for the following three reasons: (1) For each satisﬁability query the SMT solver will present one model (i.e., one noncompliant instance) even in the presence of multiple noncompliant instances (We desire as many noncompliant instances instead of just one at a time); (2) The resulting formulas are large and it may put heavy burden on the SMT solver; (3) Due to the incompleteness caused by techniques used to relieve path explosion, the extracted sets A and R may not be exhaustive (i.e., complete), yielding false positives. Approach 2: In this approach, we ﬁrst take each accepting path ap i from Ap and each rejecting path r q j from Rq where 1 ≤i ≤n, 1 ≤j ≤t, and check to see whether the formula ap i ∧r q j is satisﬁable by consulting an SMT solver. If the formula is satisﬁable, it signiﬁes that there is at least one certiﬁcate chain that Ip accepts but Iq rejects. The model obtained for the formula from the SMT solver, can be used to construct a concrete certiﬁcate chain signifying an evidence of inconsistency. We can then repeat the same process by taking each accepting path from Iq and each rejecting path from Ip. Note that, multiple pairs may induce inconsistencies due to the same noncompliant behavior and sometimes best- effort manual analysis of the source code is needed to detect the root cause. D. Scalability Challenges of Applying Symbolic Execution The application of symbolic execution in a straightforward way to extract the sets A and R, considering all certiﬁcates in the chains and other arguments to the CCVL function to have symbolic values, will not yield a scalable noncompliance detection approach. Our feasibility evaluation have veriﬁed this observation. We have also tried only one of the certiﬁcates in the chain to have symbolic values and even then the symbolic execution did not ﬁnish due to resource exhaustion. The scalability problem is predominantly due to symbolic value dependent loops—loops whose terminating conditions depend on symbolic values—in the certiﬁcate parsing im- plementation. One way to get around this challenge is to assume the correctness of the parsing code and just focus on the core CCVL logic. Ignoring the parsing logic, however, is not sufﬁcient to capture the majority of the CCVL logic as some of the sanity checks on the certiﬁcate ﬁelds are done during parsing. In addition, capturing only the CCVL logic would require one to manually modify the internal data structure where the certiﬁcate ﬁelds are stored after parsing. This approach requires signiﬁcant manual efforts (i.e., code comprehension) and is also error-prone. E. Our Solution—SymCerts and Problem Decomposition For addressing the scalability challenge we rely on carefully crafting symbolic certiﬁcates and also on our domain speciﬁc observations. Rather than extracting the complete sets A and R, we use domain-speciﬁc observations and specially crafted symbolic certiﬁcate chains to extract an approximation of the sets A and R, i.e., Aapprox and Rapprox. Our approximation has both under- and over-approximation. To overcome path explo- sion, we create a chain of SymCerts where some portions of each certiﬁcate have concrete values whereas the others have symbolic values. SymCerts along with the following observation aid in achieving scalability during the extractions of the sets Aapprox and Rapprox from an X.509 CCVL. One domain speciﬁc observation we use is the logical independence between certiﬁcate ﬁelds. For instance, the logic of checking whether a certiﬁcate is expired according to its notAfter ﬁeld is independent of the logic of checking whether a certiﬁcate’s issuer name matches with the subject name of the predecessor certiﬁcate in the chain. In this case, we can try to capture the logic of checking certiﬁcate expiration independently of the checking of issuer and subject names. Based on the notion of independence, we group the certiﬁcate ﬁelds into equivalence classes where the logic of ﬁelds in the same equivalence class should be extracted at the 7 same time, that is, ﬁelds in the same equivalence class should be marked to have symbolic values at the same time. We leverage this observation by generating a SymCert chain for each equivalence class where each element of the equivalence class has symbolic values whereas the rest of the ﬁelds have concrete values. Note that we certainly do not claim that the checking logic of all certiﬁcate ﬁelds are independent; there are obviously certiﬁcate ﬁelds whose value inﬂuences one another. For instance, the value of the isCA ﬁeld of an X.509 certiﬁcate (i.e., whether the certiﬁcate is a CA certiﬁcate) prescribes certain corresponding key usage purposes (i.e., affecting the KeyUsage extension). In this case, the isCA ﬁeld needs to be in the same equivalence class as KeyUsage. In our analysis, we conservatively partition the certiﬁcate ﬁelds into 2 equivalence classes. We refer to these two equivalence classes as EqC1 and EqC2, respectively. EqC1 has all the relevant certiﬁcate ﬁelds symbolic, except the Validity date time period ﬁelds which are symbolic only in EqC2. V. IMPLEMENTATION In this section, we discuss additional challenges of applying symbolic execution to CCVL code, and our approach to addressing these challenges. We also discuss other aspects of implementing our noncompliance ﬁnding approach. Challenge 1 (Complex Structure of X.509 Certiﬁcates): X.509 certiﬁcates are represented in the Abstract Syntax No- tation One (ASN.1) [91], [92] notation. X.509 certiﬁcates are typically transmitted in byte streams encoded following the DER (Distinguished Encoding Rules), which are binary in nature. Under the DER format, an X.509 certiﬁcate has the form ⟨t, ℓ, v⟩where t denotes a type, ℓdenotes the length of the values in bytes, and ﬁnally v represents the value. t can represent complex types such as a sequence where the value v can be recursively made of other ⟨t, ℓ, v⟩triplets. Such nesting of ⟨t, ℓ, v⟩triplets inside a v ﬁeld can be arbitrarily deep. The problem of marking the whole certiﬁcate byte-stream as symbolic is that, during certiﬁcate parsing, the symbolic ex- ecution engine will try different values for ℓas it is symbolic, and the parsing code will keep reading bytes without knowing when to stop. This will cause memory exhaustion. Approach—SymCerts (Certiﬁcates With Symbolic and Concrete Values): To avoid the scalability problem, instead of using a fully symbolic certiﬁcate chain, we develop a certiﬁcate chain in which each certiﬁcate byte-stream contains some of concrete values and some symbolic values. We call each such certiﬁcate a SymCert. We construct a SymCert in the following way: For each leaf ⟨t, ℓ, v⟩tuple (i.e., v contains a value instead of another ⟨t, ℓ, v⟩tuple) in a certiﬁcate byte-stream, we ensure that the ﬁelds t and ℓhave concrete values whereas only the v ﬁeld is symbolic. Concrete values of t can be obtained from actual certiﬁcates and we use them as the backbone for generating SymCerts. For the l ﬁeld, we consult the RFC document to select appropriate concrete values. For instance, when marking the OIDs used in the ExtKeyUsage extension symbolic, we give it a concrete length of 8, as most of the standard key usage purposes deﬁned in RFC 5280 [2] are 8-byte long. Due to the complexity of DER byte-streams, it is difﬁcult for a user to directly manipulate and construct SymCerts from scratch. In addition, due to nesting, changing the length ﬁeld (i.e., ℓ) of a child ⟨t, ℓ, v⟩triplet may require adjustment on the length ﬁeld (i.e., ℓ) of the parent ⟨t, ℓ, v⟩triplet. For this, we developed a Graphical User Interface (GUI), by extending the ASN.1 JavaScript decoder [93]. Our GUI allows a user to see and click on different certiﬁcate ﬁelds, so that they can be replaced with a desired number of symbolic bytes, and the new length will be correctly adjusted. The GUI will then automatically generate code that can be used for symbolic execution. We use OpenSSL to generate concrete certiﬁcate chains as the input to our GUI, which constitute the basis of our SymCerts. The philosophy here is that all major ﬁelds (e.g. optional extensions, criticality booleans) of a certiﬁcate need to be explicitly available on the base input certiﬁcate, as it is difﬁcult to mark nonexistent ﬁelds symbolic. Challenge 2 (System Time Handling): Given that our symbolic execution of the implementations would happen at different times, if we simply allow the implementations to use the local system time, then the constraints we have extracted would not be comparable, as the system time elapses. Approach—Constant Static Time: We consider a ﬁxed concrete time value for the system time. We use the same concrete value for these inputs during the analysis of all implementations. Using a symbolic variable is also possible, but using concrete values has the advantage of reducing the complexity of the path constraints which consequently improves scalability. Challenge 3 (Cryptographic Functions): The cryptographic functions (e.g., for verifying digital signatures) called by the CCVL contain loops dependent on symbolic data, which severely impact the scalability of symbolic execution. Approach—Cryptographic Stub Functions: We abstract away the cryptographic functions with stub functions. For instance, the function that matches the digital signature of a certiﬁcate is abstracted away by a stub function that returns True indicating the match was successful. In this work, we consider cryptographic correctness beyond our scope. Instead, we are interested in ﬁnding out what ﬁelds are checked and what restrictions are imposed on these ﬁelds. Challenge 4 (Complex String Operations): As part of the CCVL, implementations are sometimes required to perform complex string operations (e.g., wild card matching, null checking) on certiﬁcate ﬁelds such as subject name and issuer name. Faithfully capturing the string operations with QF BVA logic (i.e., QFFOL formulas with equality, bit vector, and array theories)—which is the underlying logic of the symbolic execution engine we use—does not scale well. Approach—Single Byte Strings: We consider names and other string-based certiﬁcate ﬁelds to have a single byte symbolic value, which signiﬁcantly improves the scalability. However, because of this, our analysis misses out on ﬁnding noncompliance due to erroneous string operations. 8 Challenge 5 (Hashing for Checking Multi-Field Equal- ity): When checking the equality of two name ﬁelds of certiﬁcates—name ﬁelds are compound ﬁelds containing the following sub-ﬁelds such as street address, city, state/province, locality, organizational name & unit, country, common name— some implementations take a hash of the concatenation of all the sub-ﬁelds and match the hash values, instead of checking the equality of each sub-ﬁeld. Trying to solve the constraints from such a match would be similar to attacking the hash collision problem, which is not scalable to analyze with symbolic execution due to symbolic data-dependent loops. Approach—Hash Stub: The hash function in question (i.e., SHA-1) returns a 20-byte hash value. We replace it with a SHA-1 stub which returns a 20-byte value where the (symbolic) name sub-ﬁelds are packed together. Because of the single byte approach we introduced to simplify string operations described in the previous challenge, 20-byte is more than enough to pack all name sub-ﬁelds of interests. Challenge 6 (Certiﬁcate Chain Length): While symboli- cally executing the CCVL of a given implementation, one natural question that arises is: “How many certiﬁcates in the symbolic certiﬁcate chain should we consider?” An X.509 CCVL implementation often parses the input X.509 certiﬁcate chain ﬁrst and then checks the validity of different ﬁelds in the certiﬁcates of the parsed chain. During symbolic execution, if the execution detects a loop whose terminating condition relies on a symbolic value, it faces the dilemma of how many times to unroll the loop. Such loops in the implementation often cause path explosion in symbolic execution, resulting in incompleteness and scalability challenges. If we consider the certiﬁcate chain length to be symbolic, then the symbolic execution, especially during parsing, would try all possible values for the chain length, causing memory exhaustion. Approach—Concrete Chain Length: For majority of our analysis, we consider a certiﬁcate chain of length 3 such that one of the certiﬁcates is the root CA certiﬁcate, the other is an intermediate CA certiﬁcate, and ﬁnally the remaining certiﬁcate is the certiﬁcate of the server currently being authenticated. While analyzing the logic of checking the path length constraint of the basic constraint extension, we also consider certiﬁcates with chain length 4 where we have two intermediate CA certiﬁcates. Challenge 7 (Other aspects of Path Explosion): After the simpliﬁcations described above, the symbolic execution engine still generates a large number of paths. We especially observed that making all the v values of ⟨t, ℓ, v⟩-tuples that represent certiﬁcate ﬁelds and extensions symbolic yields a lot of paths. Approach—Early Rejection and Grouping Fields: We observed that implementations sometimes do not return early even in the case one of the certiﬁcates cannot be parsed or one of the ﬁelds validity checks failed. This contributes to a multi- plicative factor to the number of paths. We judiciously applied early rejection when parsing or validation check fail. Finally, we applied the logical independence between certiﬁcate ﬁelds based on their semantics to decompose the noncompliance ﬁnding ﬁelds. We generated two equivalence classes, one consists of time ﬁelds related to the certiﬁcate Validity period checking, whereas the other contains all the remaining ﬁelds. One could possibly employ a more aggressive grouping of ﬁelds that need to be check together. We, however, make a conservative choice because if developer incorrectly introduces artiﬁcial dependencies in the implementation, we would like to capture them as well. Challenge 8 (Time Field Comparison): An X.509 certiﬁcate contains two time ﬁelds (i.e., notBefore and notAfter) which are compared to the current system time. A time ﬁeld can be represented in two formats (i.e., GeneralizedTime and UTCTime). In GeneralizedTime, the time ﬁeld contains a 15- byte ASCII string where day, month, hour, minute, second contribute 2 bytes each; year contributes 4 bytes, and 1 byte is used to represent the time zone. For UTCTime, the only difference is that year contributes 2 bytes instead of 4. Sanity checks are often performed to ensure the ﬁelds are well- formed (e.g., for minute, the most signiﬁcant digit cannot be larger than 6). Marking the format symbolic and let the symbolic execution engine choose the length of the ASCII string contributes to poor scalability. Approach—Decomposing Time Fields: In addition to checking noncompliance in time ﬁelds handling independently from other ﬁelds, we further decompose the analysis by ana- lyzing the two time formats separately. We use two different SymCerts during symbolic execution, one with UTCTime and the other with GeneralizedTime, using the concrete length of the date time ASCII string according to the format. Challenge 9 (Redundant Pair of Paths in Cross-Validation): When cross-validating two implementations Ip and Iq, the upper bound of discrepancies is |Ap| × |Rq| + |Aq| × |Rp|. Based on the number of paths in accepting (e.g., Ap and Aq) and rejecting (e.g., Rp and Rq) universes, the maximum number of noncompliance instances can be fairly large which creates a challenge for manual inspection to identify the root cause of the noncompliance. Approach—Iterative Pruning: We observe that many pairwise discrepancies are due to the same root cause. Suppose implementation Ip does not check a particular ﬁeld that Iq checks. In this case, the missing check in Ip’s accepting path will likely be enumerated through many rejecting paths of Iq, resulting in a large number of redundant noncompliance instances. To make it easier to analyze the results of cross- validation, once we have identiﬁed such a case, we can concretize the value of that speciﬁc ﬁeld, repeat the extraction step and continue cross-validation with a pruned search space. Challenge 10 (False Positives): Due to different domain speciﬁc simpliﬁcations and the fact that we are abstracting away cryptographic functions, our approach can yield false positives, predominantly due to the path constraint extrac- tion might not be capturing the real execution faithfully. In addition, the speciﬁcation (i.e., RFC document) states some ﬁelds should be checked by a certiﬁcate using system, without imposing whether the library or application (the two of them constitute the system) should perform each check. Conse- quently, SSL/TLS libraries have different API designs due to 9 such unclear separation of responsibility. Some libraries might enforce all the checks during certiﬁcate chain validation, while some might not and instead provide optional function calls for application developers desiring such checks, and the other libraries might completely delegate the task of implementing such checks to the application developers. As a clear boundary cannot be drawn easily, false positives can arise if some optional but provided checks are missed out during extraction. Approach—Concrete Replay: To avoid false positives, we use a real client-server setup to help us verify our ﬁndings. We capitalize on the fact that a minimalistic sample client code is often made available in the source tree by library developers to demonstrate how the library should be used in application development and use such clients to draw the baseline. To gain conﬁdence that our extracted path constraints adequately capture the real execution, for each accepted (resp., rejected) path constraint, we consult the SMT solver to obtain a concrete certiﬁcate chain and feed it to a real client-server setup to see whether the client would actually accept (resp., reject) the chain. This helps us to see whether the real execution concurs with our extraction. Similarly, during cross validation between implementations Ip and Iq, for the discrepancies we found (in the form of a model provided by the SMT solver), we construct a concrete certiﬁcate chain out of the model and use the client- server setup to verify it is indeed the case that Ip would accept and Iq would reject the chain. VI. EVALUATION AND RESULTS We applied our approach in testing 9 open-source imple- mentations from 4 major families of SSL/TLS library source trees, as shown in Table I. Implementations that have been tested in previous study by Brubaker et al. [43] are preﬁxed with an asterisk. These libraries have seen active deployments in embedded systems and IoT products to satisfy the security needs for connecting to the Internet (e.g. axTLS in Arduino [51] and MicroPython [52] for ESP8266, mbedTLS, tropicSSL and MatrixSSL on Particle hardware [49], [50], etc.), and are sometimes used even in building applications and libraries on conventional desktop platforms [57]–[61], due to their performance and small footprint advantage. We test multiple versions of a library from the same family in order to compare with previous work, and to see if the more recent versions implement a more complete and robust validation logic. In this section we ﬁrst show statistics that justify the practicality of our approach, and then present noncompliance ﬁndings grouped by how we uncovered them along the 3 approaches described in Section IV-C, together with other discrepancies and observations that we made while work- ing with the libraries. Findings on recent versions of the implementations, whenever applicable, are reported to the corresponding developers. Many of our reports had led to ﬁxes being implemented in newer versions. A. Implementation Efforts and Practicality For our analysis, we used the KLEE symbolic execution engine [45] and the STP SMT solver [65]. We added around 2000 lines of C++ code for implementing the path constraint extraction and cross validation engines, around 500 lines of Python for parsing path constraints and automating concrete test case generation, and around 400 lines of HTML plus less than 300 lines of JavaScript for the GUI that enables the easy construction of SymCerts. In order to implement the various optimizations described before, a limited amount of new code need to be added to the libraries that we tested. As shown in Table I, no more than 75 lines of code were added to each of the library. Most of the new code is used to implement a static system time (see Section V-Challenge 2) and a stub cryptographic signature check (Section V-Challenge 3). Additionally, for CyaSSL 2.7.0, wolfSSL 3.6.6, and MatrixSSL 3.7.2, some code was added to implement the hash stub (see Section V-Challenge 5). PolarSSL 1.2.8 and tropicSSL needed a simpliﬁed version of sscanf(), and axTLS (both 1.4.3 and 1.5.3) needed a simpliﬁed version of mktime(), to avoid symbolic-data dependent loops, both of which are used for reading in and converting the format of date-time inputs. Also shown in Table I are the performance statistics regard- ing path extraction. We ran our experiments on a commodity laptop equipped with an Intel i5-2520M CPU and 16GB RAM. Path extraction using EqC1 for most implementations ﬁnished in minutes, while for some heavier ones it completed in hours. The total number of paths ranges from hundreds to the level of ten thousands. For EqC2, we report the upper bound of the total number of paths, referred to in the table as “Total Paths”, because the actual number could vary within each library due to different treatments (and possibly missing checks) for UTCTime and GeneralizedTime (see Section VI-C and VI-D for examples). For each library, extraction using EqC2 yielded paths at the scale of tens, and ﬁnished within a minute. B. Errors Discovered By Symbolic Execution The ﬁrst opportunity our approach provides is that, during symbolic execution, certain low-level coding issues (e.g. mem- ory access errors, division by zeros, etc.) could be found. Finding 1 (Incorrect extension parsing in CyaSSL 2.7.0 1): As shown in Listing 1, due to a missing break statement after DecodeAltNames(), the execution falls through to the next case and also invokes DecodeAuthKeyId(). Consequently, some bytes of the subject alternative name extension, which we made symbolic, will overwrite the authority key identiﬁer (a pre-computed hash value) at the time of parsing. The error manifests later during certiﬁcate chain validation, when the authority key identiﬁer undergoes some bit shifting operations and modulo arithmetic, effectively turning it into an array accessing index, which is then used to fetch a CA certiﬁcate from a table of trusted CA certiﬁcates. Since some bytes of the authority key identiﬁer were incorrectly made symbolic during parsing, the execution engine caught potential memory access errors in fetching from the table. This was not reported in [43], which applied fuzzing to test CyaSSL 2.7.0. Our conjecture is 1This bug has been ﬁxed in newer versions of CyaSSL and wolfSSL. 10 TABLE I PRACTICALITY AND EFFICACY OF APPLYING THE SYMCERT APPROACH IN TESTING VARIOUS SMALL FOOTPRINT SSL/TLS LIBRARIES Library - version Released Lines of C code in library Lines Added Paths  EqC1  Extraction Time  EqC1  Total Paths  EqC2  Extraction Time  EqC2  Found Instances of Noncompliance axTLS - 1.4.3 Jul 2011 16,283 72 276 (419) ∼1 Minute ≤52 ≤1 minute 7 axTLS - 1.5.3 Apr 2015 16,832 69 276 (419) ∼1 Minute ≤52 ≤1 minute 6 * CyaSSL - 2.7.0 Jun 2013 51,786 33 32 (504) ∼2 Minutes ≤26 ≤1 minute 7 wolfSSL - 3.6.6 Aug 2015 103,690 40 256 (31409) ∼1 Hour ≤26 ≤1 minute 2 tropicSSL - (Github) Mar 2013 13,610 66 16 (67) ∼1 Minute ≤30 ≤1 minute 10 * PolarSSL - 1.2.8 Jun 2013 29,470 66 56 (90) ∼1 Minute ≤81 ≤1 minute 4 mbedTLS - 2.1.4 Jan 2016 53,433 15 13 (536) ∼1 Minute ≤41 ≤1 minute 1 * MatrixSSL - 3.4.2 Feb 2013 18,360 9 8 (160) ∼1 Minute 1 ≤1 minute 6 MatrixSSL - 3.7.2 Apr 2015 37,879 30 3240 (8786) ∼1 Hour ≤25 ≤1 minute 5 § The fourth column of the table refers to the lines of code we added to the libraries to make them amenable to our analysis. The ﬁfth and sixth columns display the number of accepting (rejecting) paths we obtained when we made the ﬁelds in equivalence class EqC1 symbolic, and the time it took to complete the extraction process, respectively. The seventh and eighth columns show the upper bound of total paths (including both accepting and rejecting) we observed when the ﬁelds in EqC2 are made symbolic, and the time it took for the path extraction process to complete, respectively. that it would be difﬁcult for concrete test cases to hit this bug, as the execution is likely to fall through without triggering any noticeable crashes. Listing 1. Extension Processing In CyaSSL 2.7.0 switch (oid) { ... case AUTH_INFO_OID: DecodeAuthInfo(&input[idx], length, cert); break; case ALT_NAMES_OID: DecodeAltNames(&input[idx], length, cert); case AUTH_KEY_OID: DecodeAuthKeyId(&input[idx], length, cert); break; ... } C. Findings From Simple Search of Path Constraints Fields of certiﬁcates, represented by symbolic variables in our approach, will appear on path constraints if they are in- volved in branching decisions either directly or indirectly (e.g. some other decision variables were calculated based on their values). Consequently, the second opportunity our approach offers is that immediately after extracting path constraints using symbolic execution, missing checks of ﬁelds can be discovered by performing “grep” on the path constraints. Finding 2 (pathLenConstraint ignored in CyaSSL 2.7.0, wolfSSL 3.6.6 2): We noticed that both of the aforementioned libraries fail to take pathLenConstraint into consideration, which means any such restrictions imposed by upper level issuing CAs would be ignored by the libraries. This was not reported in [43], where fuzzing was ap- plied to CyaSSL 2.7.0. Interestingly, [43] instead reported that CyaSSL 2.7.0 incorrectly rejects leaf CA certiﬁcates given the intermediate CA certiﬁcate has a pathLenConstraint of 0, and is noncompliant because such certiﬁcates should be accepted according to the RFC. Our ﬁndings, however, demonstrate that CyaSSL 2.7.0 could not possibly be re- jecting certiﬁcates for such a reason because it completely ignores pathLenConstraint. Testing CyaSSL 2.7.0 with con- crete certiﬁcates conﬁrmed our ﬁnding. Thus, the conclu- sion in [43] that CyaSSL 2.7.0 misinterprets RFC regarding 2wolfSSL 3.9.10 has implemented support for pathLenConstraint [94]. pathLenConstraint and leaf CA certiﬁcate is incorrect. We conjecture that this is because the frankencerts used as evi- dence for such conclusion also happen to contain other errors, and were thus rejected by CyaSSL 2.7.0. This demonstrates the difﬁculty of interpreting results obtained from fuzzing. Finding 3 (pathLenConstraint of intermediate CA certiﬁ- cates ignored in tropicSSL, PolarSSL 1.2.8 3): Our path con- straints show that even though both tropicSSL and PolarSSL 1.2.8 recognize the pathLenConstraint variable during parsing time, they check only the one that is on the trusted root certiﬁcate during chain validation, and ignores those that are on intermediate CA certiﬁcates of a given chain. In addition to the fact that PolarSSL 1.2.8 does not check pathLenConstraint on intermediate CA certiﬁcates, another simple search found that PolarSSL 1.2.8 does not check whether the leaf certiﬁcate is CA or not (which is not a noncompliant behavior). It was however reported in [43] that PolarSSL 1.2.8 violates the RFC by always rejecting leaf CA certiﬁcates if the intermediate CA certiﬁcate has a pathLenConstraint of 0. This is incorrect because PolarSSL 1.2.8 checks neither pathLenConstraint on intermediate CA certiﬁcates, nor whether the leaf certiﬁcate is CA or not. Finding 4 (Certain attribute types of distinguished names ignored in axTLS 1.4.3 and 1.5.3): Both axTLS 1.4.3 and 1.5.3 ignore the country, state/province and locality attribute types of the issuer and subject names. In other words, organizations from different countries and states having the same name would be considered equivalent during matching. This is a clear deviation from RFC 5280 (Section 4.1.2.4) [2]. We have this ﬁnding reported to the developer of axTLS, who acknowledged the existence of the problem and imple- mented a ﬁx in the new 2.1.1 release. Finding 5 (Inability to process GeneralizedTime in axTLS 1.4.3, tropicSSL): RFC 5280 (Section 4.1.2.5) [2] states “Con- forming applications MUST be able to process validity dates that are encoded in either UTCTime or GeneralizedTime.” However, given our SymCerts with GeneralizedTime, both 3The enforcement of pathLenConstraint from intermediate CA certiﬁcates has been introduced since PolarSSL 1.2.18 [95]. 11 tropicSSL and axTLS 1.4.3 returned only 1 concrete rejecting path with an empty path constraint, hence we conclude that the aforementioned libraries cannot handle GeneralizedTime, which is a non-conformance to the RFC. However, the same SymCerts managed to yield meaningful path constraints in axTLS 1.5.3, showing that support for GeneralizedTime has been added in the newer version of axTLS. Finding 6 (KeyUsage and ExtKeyUsage being ignored in MatrixSSL 3.4.2, CyaSSL 2.7.0, tropicSSL): The three aforementioned implementations do not check KeyUsage and ExtKeyUsage extensions. This noncompliance implies that certiﬁcates issued speciﬁcally for certain intended purposes (e.g. only for software code signing) can be used to authenti- cate a server in SSL/TLS handshakes. Honoring such restric- tions imposed by issuing CAs allows the PKI to implement different levels of trust, and help avoid certiﬁcate (and CA) misuse in general. Finding 7 (notBefore ignored in tropicSSL, PolarSSL 1.2.8; validity not checked in MatrixSSL 3.4.2): Our SymCerts revealed that PolarSSL 1.2.8 does not check the notBefore ﬁeld, and MatrixSSL 3.4.2 does not have an inbuilt validity check, as there is only 1 path, which is an accepting path with empty constraints, for each of the aforementioned libraries in their respective cases. This is coherent with the ﬁndings in [43]. MatrixSSL 3.4.2 delegates the task of checking certiﬁcate validity to application developers. tropicSSL has the same problem as PolarSSL 1.2.8, which is not a surprise considering the fact that tropicSSL is a fork of PolarSSL. Finding 8 (hhmmss of UTCTime ignored in tropic- SSL, axTLS 1.4.3 and 1.5.3; hhmmss of both UTCTime and GeneralizedTime ignored in MatrixSSL 3.7.2): Given UTCTime on certiﬁcates, even though axTLS 1.4.3 and 1.5.3 check for both notBefore and notAfter, they do not take the hour, minute and second into consideration, which means that there could be a shift for as long as a day in terms of rejecting future and expired certiﬁcates. This ﬁnding is particularly interesting for axTLS 1.5.3, as its implementation of GeneralizedTime support can actually handle hour, minute and second, but for some reason UTCTime is processed in a laxer manner. Following our report, the developer of axTLS has acknowledged the problem and is currently considering a ﬁx. Our extracted path constraints show and tropicSSL also suffer from the same problem. Unlike its older counterpart, MatrixSSL 3.7.2 has im- plemented validity checks that handle both UTCTime and GeneralizedTime. However, our extracted path constraints re- vealed that MatrixSSL 3.7.2 does not attempt to check the time portion of the validity ﬁelds, regardless of whether the date- time information is in UTCTime or GeneralizedTime. The developers of MatrixSSL had explained to us the decision to ignore the time portion was made due to its embedded origin, where a local timer might not always be available, and in their own words “having date set correctly is difﬁcult enough”. They have also admitted that as the result of such decision, a 24-hour shift in rejecting future and expired certiﬁcates is inevitable. Finding 9 (notAfter check applies only to leaf certiﬁcate in tropicSSL): Not just that future certiﬁcates are not rejected (e.g. missing check for notBefore as described above) in tropicSSL, our path constraints show that, given a chain of certiﬁcates, the check on notAfter only applies to the leaf one. This could lead to severe problems, for instance, if a retired private key of an intermediate issuing CA corresponding to an expired certiﬁcate got leaked, attackers would be able to issue new certiﬁcates and construct a new chain of certiﬁcate that will be accepted by tropicSSL. Finding 10 (Incorrect CA certiﬁcate and version number as- sumptions in axTLS 1.4.3 and 1.5.3, CyaSSL 2.7.0, MatrixSSL 3.4.2): The aforementioned implementations deviate from the RFC in how they establish whether certiﬁcates of various versions are CA certiﬁcates or not. As explained previously in Section III-A2, in case the certiﬁcate has a version older than 3, some out-of-band mechanisms would be necessary to verify whether it is a CA certiﬁcate or not. axTLS 1.4.3 and 1.5.3 assume certiﬁcates to be CA certiﬁcates regardless of the version number. CyaSSL 2.7.0 also does not check the version number, though whenever the basicConstraints extension is present, it will be used to determine whether the certiﬁcate is a CA certiﬁcate or not. MatrixSSL 3.4.2 does check the version number, and would check the basicConstraints exten- sion for version 3 certiﬁcates. However, it would just assume certiﬁcates older than version 3 to be CA certiﬁcates. The ﬁndings on CyaSSL 2.7.0 and MatrixSSL 3.4.2 are coherent with the relevant results reported in [43]. Finding 11 (Unrecognized critical extensions in MatrixSSL 3.4.2, CyaSSL 2.7.0, axTLS 1.4.3 and 1.5.3): Section 4.2 of RFC 5280 states “A certiﬁcate-using system MUST reject the certiﬁcate if it encounters a critical extension it does not recognize or a critical extension that contains information that it cannot process.” [2]. Not rejecting unknown critical exten- sions could lead to interoperability issues. For example, certain entities might deﬁne and issue certiﬁcates with additional non- standard custom extensions, and rely on the default rejection behavior as described in RFC 5280 to make sure that only a speciﬁc group of implementations can handle and process their certiﬁcates. However, we found that MatrixSSL v3.4.2 and CyaSSL 2.7.0 would accept certiﬁcates with unrecognized critical extensions, which is consistent to the ﬁndings in [43]. In addition, we found that axTLS 1.4.3 and 1.5.3 would also accept certiﬁcates with unrecognized critical extensions. In fact, based on the path constraints we have extracted, they do not recognize any of the standard extensions that we wanted to test at all, which deviates from RFC 5280, as Section 4.2 says the minimum requirement for applications conforming to the document MUST recognize extensions like key usage, basic constraints, name constraints, and extended key usage, etc. Similarly for mbedTLS 2.1.4, as we have noticed for not implementing support for the name constraints extension, is also noncompliant in that sense. The implication of this is that restrictions imposed by issuing CAs in the form of name constraints will not be honored by mbedTLS 2.1.4, resulting in potential erroneous acceptance of certiﬁcates. At 12 the time of writing, developers of mbedTLS have indicated that they currently have no plans on implementing support for this extension, and suggested that application developers can implement their own if desired. D. Findings From Cross-Validating Libraries The ﬁnal opportunity would be to cross-validate libraries, speciﬁcally, for each accepting path of library A and each rejecting path of library B, we perform a conjunction and see if the resulting constraints would be solvable or not. If yes, it signiﬁes a discrepancy exists between the two libraries. Finding 12 (ExtKeyUsage OID handling in wolfSSL 3.6.6, MatrixSSL 3.7.4): Our path constraints also unveiled that despite being two of the few libraries that support the extended key usage extension, both wolfSSL 3.6.6 and MatrixSSL 3.7.2 opted for a somewhat lax shortcut in handling the extension: given the object identiﬁer (OID) of a key usage purpose, they do a simple summation (referred colloquially as a non- cryptographic digest function by the developers of MatrixSSL) over all nodes of the OID, and then try to match only that sum. For example, under such scheme, the standard usage purpose “server authentication” (OID 1.3.6.1.5.5.7.3.1, DER-encoded byte values are 0x2B 0x06 0x01 0x05 0x05 0x07 0x03 0x01) would be treated as decimal 71. Notice that the extension itself is not restricted to only hold standard usage purposes that are deﬁned in the RFC, and custom key usage purposes are common 4. Since OIDs are only meant to be unique in a hierarchical manner, the sums over nodes of OIDs are not necessarily unique. Hypothetically some enterprises under the private enterprise arc (1.3.6.1.4.1) could deﬁne OIDs to describe their own key usage purposes, and if added to the extension, those OIDs might be incorrectly treated as some of the standard key usage purposes by the two libraries. This could be problematic for both interoperability and security, as custom key usage purposes would be misin- terpreted, and the standard ones could be spoofed. This ﬁnding is a good example of how our approach can be used to discover the exact treatments that variables undergo inside the libraries during execution. It might also be difﬁcult for unguided fuzzing to hit this particular problem. We contacted the corresponding developers of the 2 libraries regarding this, and both acknowledged the problem exists. wolfSSL has introduced a more rigorous OID bytes checking since version 3.7.3 5, and MatrixSSL is planning to incorporate additional checks of the OID bytes in a new release. Finding 13 (Incorrect interpretation of UTCTime year in MatrixSSL 3.7.2, axTLS 1.4.3 and 1.5.3, tropicSSL): Since UTCTime reserves only two bytes for representing the year, one needs to be cautious when interpreting it. RFC 5280 Section 4.1.2.5.1 [2] says that when the YY of a UTCTime is 4For example, Microsoft deﬁnes its own key usage purposes and the corresponding OIDs that are deemed meaningful to the Windows ecosystem [96] (the extension is referred to as “Application Policy” in Microsoft terminology, and is not to be confused with “Certiﬁcate Policy”). 5https://github.com/wolfSSL/wolfssl/commit/ d248a7660cc441b68dc48728b10256e852928ea3 larger than or equal to 50 then it should be treated as 19YY, otherwise it should be treated as 20YY. This essentially means that the represented range of year is 1950 to 2049 inclusively. During cross-validation, we noticed that in certain libraries, some legitimate years are being incorrectly rejected (and accepted). A quick inspection of the path constraints, concrete- value counterexamples, and ﬁnally the source code, found the following instances of noncompliance. Listing 2. UTCTime year adjustment in MatrixSSL 3.7.2 y = 2000 + 10 * (c[0] - ’0’) + (c[1] - ’0’); c += 2; /* Years from ’96 through ’99 are in the 1900’s */ if (y >= 2096) { y -= 100; } As shown in Listing 2, MatrixSSL 3.7.2 interprets any YY less than 96 to be in the twenty ﬁrst century. This means certiﬁcates that had expired back in 1995 would be considered valid, as the expiration date is incorrectly interpreted to be in 2095. On the other hand, long-living certiﬁcates that have a validity period began in 1995 would be treated as not valid yet. The developers acknowledged our report on this and have since implemented a ﬁx in a new release. Listing 3. UTCTime year adjustment in tropicSSL to->year += 100 * (to->year < 90); to->year += 1900; A similar instance of noncompliance was found in tropicSSL, as shown in Listing 3. tropicSSL interprets any YY less than 90 to be in the twenty ﬁrst century. Listing 4. UTCTime year adjustment in axTLS 1.4.3 and 1.5.3 if (tm.tm_year <= 50) { /* 1951-2050 thing */ tm.tm_year += 100; } A similar issue exists in both axTLS 1.4.3 and 1.5.3. As shown in Listing 4, there is an off-by-one error in the condition used to decide whether to adjust the year or not. In this case, the year 1950 would be incorrectly considered to mean 2050. Based on the inline comment, it seems to be a case where the developer misinterpreted the RFC. A ﬁx has been implemented in a new version of axTLS following our report. Finding 14 (Incorrect timezone adjustment in MatrixSSL 3.7.2): During cross-validation with other libraries, we noticed that the boundary of date checking in the path constraints of MatrixSSL 3.7.2 was shifted by one day. A quick inspection of the date time checking code found that MatrixSSL 3.7.2 uses the localtime_r() instead of gmtime_r() to convert the current integer epoch time into a time structure. The shift was due to the fact that in conventional libc implementa- tions, localtime_r() would adjust for the local time zone, which might not necessarily be Zulu, hence deviating from the RFC requirements. Assuming the date time on certiﬁcates are in the Zulu timezone, the implication of this subtle issue is that for systems in GMT-minus time-zones, expired certiﬁcates could be considered still valid because of the shift, and certiﬁcates that just became valid could be considered not yet valid. Similarly, for systems in GMT-plus time-zones, certiﬁcates that are still valid might be considered expired, and future certiﬁcates that are not yet valid would be considered valid. 13 We discussed this with the developers of MatrixSSL. They conjectured the reason for using localtime_r() instead of gmtime_r() was due to the latter being unavailable on certain embedded platforms. They have agreed, however, as MatrixSSL is gaining popularity on non-embedded platforms, in a new release, they will start using gmtime_r() on platforms that support it. Finding 15 (Overly restrictive notBefore check in CyaSSL 2.7.0 6): RFC 5280 Section 4.1.2.5 says “The validity period for a certiﬁcate is the period of time from notBefore through notAfter, inclusive.” However, when cross-validating CyaSSL 2.7.0 with other libraries, from the concrete counterexamples we noticed that discrepancy exists in how the same notBefore values would be accepted by other libraries but rejected by CyaSSL 2.7.0, while such discrepancy was not observed with notAfter. An inspection of the notBefore checking code yielded the following instance of noncompliance: Listing 5. Erroneous “less than” check in CyaSSL 2.7.0 static INLINE int DateLessThan(const struct tm* a, const struct tm* b) { return !DateGreaterThan(a,b); } Notice that the negation of > is ≤, not <, which explains why if the current date time happen to be the same as the one described in notBefore, the certiﬁcate would be considered future (not valid yet) and rejected. Hence the notBefore checking in CyaSSL 2.7.0 turns out to be overly restrictive than what the RFC mandates. This is again a new result, comparing to the previous work [43] that also studied CyaSSL 2.7.0. Our conjecture is that given a large number of possible values, it might be difﬁcult for unguided fuzzing to hit boundary cases, hence such a subtle logical error eluded their analysis. Finding 16 (KeyUsage and ExtKeyUsage being ignored in PolarSSL 1.2.8): The fact that PolarSSL 1.2.8 does not check KeyUsage and ExtKeyUsage, evaded our simple search approach but was caught during cross-validation, as the im- plementation actually parses the two extensions, hence some constraints were added as the result of several basic san- ity checks happened during parsing. However, during cross- validation, it became clear that apart from the parsing sanity checks, PolarSSL 1.2.8 does not do any meaningful checks on KeyUsage and ExtKeyUsage. In fact, this resulted in another instance of noncompliance, as PolarSSL 1.2.8 would not reject certiﬁcates with KeyUsage or ExtKeyUsage, even if those two extensions were made critical, and it does not perform any meaningful checks apart from merely parsing them. This is an example where a library is intended to handle an extension but was not able to, because of incomplete implementation. This is consistent with similar results reported in [43], although the ﬁnding that PolarSSL 1.2.8 does not check the KeyUsage extension on intermediate CA certiﬁcates was not reported in that paper. 6This has been ﬁxed in newer versions of CyaSSL and WolfSSL. Finding 17 (pathLenConstraint of trusted root misinter- preted in tropicSSL): During cross validation, it became clear to us that, in tropicSSL: (1) on one hand, some accepting paths would allow the pathLenConstraint variable to be 0; (2) on the other hand, some rejecting paths reject because the pathLenConstraint was deemed to be smaller than an unex- pectedly large boundary. In both cases, the pathLenConstraint variable appears to have been misinterpreted by tropicSSL. We suspect that this might be due to the value 0 in the internal parsed certiﬁcate data structure is used to capture the case where the pathLenConstraint variable is absent (i.e. no limit is imposed). A quick inspection of the parsing code revealed that our suspicion is indeed correct. In fact, the parsing code is supposed to always add 1 to the variable if it is present on the certiﬁcate, but a coding error 7 of missing a dereferencing operator (*) in front of an integer pointer means that the increment was applied to the pointer itself but not the value, hence the observed behavior described above. This subtle bug has a severe implication: it completely de- feats the purpose of imposing such restriction on a certiﬁcate, as a pathLenConstraint of 0 would be incorrectly treated to mean that the chain length could be unlimited. Finding 18 (Not critical means not a CA in tropicSSL): During cross validation, we also noticed that when the interme- diate CA certiﬁcate’s basicConstraints extension is set to non- critical, and the isCA boolean is set to True, tropicSSL would consider the intermediate CA certiﬁcate not a CA certiﬁcate. Additionally, in the path constraints, the symbolic variable representing the criticality of basicConstraints and the one that represents the isCA boolean are always in conjunction through a logical AND. A quick inspection found the following problem in the parsing code that handles the basicConstraints extension: Listing 6. Incorrect adjustment to the isCA boolean in tropicSSL *ca_istrue = is_critical & is_cacert; This interpretation of the basicConstraints extension de- viates from the speciﬁcation, as RFC 5280 says that clients should process extensions that they can recognize, regardless of whether the extension is critical or not. The criticality of basicConstraints should not affect the semantic meaning of attributes in the extension itself. This is an example of a CCVL being overly restrictive. E. Other ﬁndings Here we present other interesting ﬁndings that are not explicitly noncompliant behaviors deviating from RFC 5280. Extra 1 (Ineffective date string sanity check in MatrixSSL 3.7.2): During cross-validation, we noticed that date time byte values in MatrixSSL 3.7.2 are not bounded for exceedingly large or unexpectedly small values. However, in the con- straints, we see combinations of whether each byte is too small or not (though not affecting the acceptance decision), which looked suspiciously like a failed lower boundary check. A quick inspection of the certiﬁcate parsing code unveiled the 7This has been ﬁxed in later versions of PolarSSL and mbedTLS. 14 snippet shown in Listing 7 that is meant to vet a given date string from a certiﬁcate, and reject it with a parser error if the values are outside of an expected range. Unfortunately, due to incorrectly using the && operator instead of ||, the if conditions are never satisﬁable. This is also proven by the fact that if we symbolically execute the code snippet in Listing 7, all possible execution paths returns 1. Consequently that code snippet would actually never reject any given strings, hence completely defeating the purpose of having a sanity check. Listing 7. date string sanity check in MatrixSSL 3.7.2 if (utctime != 1) { /* 4 character year */ if (*c < ’1’ && *c > ’2’) return 0; c++; /* Year */ if (*c < ’0’ && *c > ’9’) return 0; c++; } if (*c < ’0’ && *c > ’9’) return 0; c++; if (*c < ’0’ && *c > ’9’) return 0; c++; if (*c < ’0’ && *c > ’1’) return 0; c++; /* Month */ if (*c < ’0’ && *c > ’9’) return 0; c++; if (*c < ’0’ && *c > ’3’) return 0; c++; /* Day */ if (*c < ’0’ && *c > ’9’) return 0; return 1; Following our report, the developers of MatrixSSL have ac- knowledged this is indeed a faulty implementation. Along with other ﬁxes being implemented to make date-time processing more robust, they have decided that this sanity check will no longer be used in newer versions of MatrixSSL. Extra 2 (notBefore and notAfter bytes taken “as is” in CyaSSL 2.7.0, WolfSSL 3.6.6, axTLS 1.4.3 and 1.5.3): For the four aforementioned implementations, we noticed during cross-validation that they do not perform any explicit boundary checks on the value of the date time value bytes of notBefore and notAfter, and just assumed that those bytes are going to be valid ASCII digits (i.e. 0–9). It is hence possible to put other ASCII characters in the date time bytes and obtain an exceptionally large (small) values for notAfter (notBefore), though this does not seem to be an imminent threat, nor does it violate the RFC, as the RFC did not stipulate what implementations should do. Extra 3 (Timezone Handling): Another discrepancy that we have observed during cross-validating path constraints of different libraries was how they impose/assume the time zone of notBefore and notAfter on certiﬁcates. Speciﬁcally, we notice that mbedTLS 2.1.4 and wolfSSL v2.3.3 would reject certiﬁcates that do not have the timezone ending with a ‘Z’. This is possibly due to the fact that RFC 5280 [2] mandates conforming CAs to express validity in Zulu time (a.k.a GMT or Zero Meridian Time) when issuing certiﬁcates, regardless of the type being UTCTime or GeneralizedTime. Other imple- mentations like MatrixSSL 3.7.2, axTLS 1.5.3 and PolarSSL 1.2.8 ignore the timezone character and simply assume the Zulu timezone is always being used. This is arguably an example of under-speciﬁcation, as it is not clear whether implementations should try to handle (with proper time zone adjustment) or reject certiﬁcates with a non- Zulu timezone, since RFC 5280 [2] did not explicitly mandate an expected behavior. VII. DISCUSSION A. Takeaway for Application Developers As a takeaway for application developers that need to use SSL/TLS libraries for processing X.509 certiﬁcates, a general rule of thumb is to upgrade to newer versions of the libraries if possible. As demonstrated by our ﬁndings, newer versions of implementations, even when originated from the same source tree as their legacy counterparts, are better equipped in terms of features and extension handling, as well as in general having more rigorous checks. Holding on to legacy code could potentially hurt both security and interoperability. Unfortunately, regular software patching, particularly for IoT devices, does not seem to happen widespread enough [97]. We understand that due to the needs to optimize for different application scenarios (e.g. small footprint for resource con- strained platforms), certain features might not be implemented in their entirety as described in the standard speciﬁcations. In order to help application developers to better understand the trade-offs and make a more well-informed decision in choosing which SSL/TLS library to use, we believe that one possibility would be to have a certiﬁcation program that tests for implementation conformance and interoperability, similar to that of the IPv6 Ready Logo Program [98], and the High Deﬁnition Logos [99]. For example, an “X.509 Gold” for libraries that implement most required features correctly, and an “X.509 Ready” for libraries that can only handle the bare minimum but are missing out on certain features. B. Limitations Since our noncompliance detection approach critically relies on symbolic execution which is known to suffer from path ex- plosion, especially in the presence of symbolic data-dependent loops, it is deliberately made to trade away completeness for practicality (i.e., our approach is not guaranteed to reveal all possible noncompliances in an implementation and can have false negatives). Our current scope of analysis does not include the logic for checking certiﬁcation revocation status and hostname match- ing. As noted in [43], for both revocation status checking and hostname matching, while some libraries provide relevant facilities, some delegate the task to application developers. In addition, a typical implementation of a hostname matching logic uses complex string operations and analyzing these require a dedicated SMT solver with support for the theory of strings [100]. We leave that for future work. Moreover, as we use concrete values in SymCerts, symbolic execution sway away from rigorously exercising the parsing logic. Though we have uncovered parsing bugs as reported in Section VI, our scrutiny on the parsing code is not meant to be comprehensive. Noticeably, low-level memory errors due to incorrect buffer management in the parsing code, as reported in a recent Vulnerability Note [101], can elude our analysis. C. Threat to Validity In some cases during certiﬁcate validation, it is not clear who is required to perform the validity check on a ﬁeld, i.e., 15 the underlying library or the application using the library. The RFC states that some speciﬁc validity check must be performed without clearly identifying the responsible party. This unclear separation of responsibilities have resulted in libraries opting for signiﬁcantly different API designs. We rely on example usage—often come with the source code in the form of a sample client—to draw a boundary for extracting the approximated certiﬁcate accepting (and rejecting) universes. Optional function calls to extra checking logics, if not demon- strated in the sample client programs, will be missed by our analysis. Additionally, if some of the checks performed on certiﬁcates are being pushed down to a different phase during SSL/TLS handshake instead of the server certiﬁcate validation phase, these checks might be missing from our extraction. We rely on the concrete client-server replay setup to catch them and iteratively include them in the extraction. Our optimization often rely on the expectation that the value of some ﬁelds are handled in the implementation in an uniform way. For checking validity of ﬁelds that can have variable lengths, we assume the implementation treats each regular length (not corner cases) uniformly. In addition, we also assume that the semantic independence of certain certiﬁcate ﬁelds are maintained in the implementation. For instance, we assume that the certiﬁcate validity ﬁelds are not dependent on any other ﬁelds. Although we have observed that this seems to be the case and the RFC supports it, hypothetically a developer can mistakenly create an artiﬁcial dependency. VIII. CONCLUSION AND FUTURE DIRECTION In this paper, we present a novel approach that leverages symbolic execution to ﬁnd noncompliance in X.509 implemen- tations. In alignment with the general consensus, we observe that due to the recursive nature of certiﬁcate representation, an off-the-shelf symbolic execution engine suffers from path explosion problem. We overcome this inherent challenge in two ways: (1) Focusing on real implementations with a small resource footprint; (2) Leveraging domain-speciﬁc insights, abstractions, and compartmentalization. We use SymCerts— certiﬁcate chains in which each certiﬁcate has a mix of symbolic and concrete values—such that symbolic execution can be made scalable on many X.509 implementations while meaningful analysis can be conducted. We applied our noncompliance approach to analyze 9 real implementations selected from 4 major families of SSL/TLS source base. Our analysis exposed 48 instances of noncom- pliance, some of which has severe security implications. We have responsibly shared our new ﬁndings with the respective library developers. Most of our reports have generated positive"
Cybersecurity and Privacy,William Robertson,https://www.khoury.northeastern.edu/people/william-robertson/,"William Robertson is an associate professor in the Khoury College of Computer Sciences and the College of Engineering at Northeastern University, based in Boston. He co-directs the Northeastern Systems Security Lab and is principal investigator for the Cybersecurity and Privacy Institute's Diverge Lab.",Systems security; Web security; Mobile security,"PhD in computer science, University of California, Santa Barbara; BS in computer science, University of California, Santa Barbara","August 1st, 2017",Game of Registrars: An Empirical Analysis of Post-Expiration Domain Name Takeovers,https://www.buyukkayhan.com/publications/sec2017registrars.pdf," Tobias Lauinger, Abdelberi Chaabane, Ahmet Salih Buyukkayhan, Kaan Onarlioglu, William Robertson. Game of Registrars: An Empirical Analysis of Post-Expiration Domain Name Takeovers. USENIX Security Symposium. August 2017.","Every day, hundreds of thousands of Internet domain names are abandoned by their owners and become avail- able for re-registration. Yet, there appears to be enough residual value and demand from domain speculators to give rise to a highly competitive ecosystem of drop-catch services that race to be the ﬁrst to re-register potentially desirable domain names in the very instant the old re- gistration is deleted. To pre-empt the competitive (and uncertain) race to re-registration, some registrars sell their own customers’ expired domains pre-release, that is, even before the names are returned to general availability. These practices are not without controversy, and can have serious security consequences. In this paper, we present an empirical analysis of these two kinds of post- expiration domain ownership changes.We ﬁnd that 10 % of all com domains are re-registered on the same day as their old registration is deleted. In the case of org, over 50 % of re-registrations on the deletion day occur during only 30 s. Furthermore, drop-catch services control over 75 % of accredited domain registrars and cause more than 80 % of domain creation attempts, but represent at most 9.5 % of successful domain creations. These ﬁndings highlight a signiﬁcant demand for expired domains, and hint at highly competitive re-registrations. Our work sheds light on various questionable practices in an opaque ecosystem. The implications go beyond the annoyance of websites turned into “Internet grafﬁti” [26], as domain ownership changes have the potential to cir- cumvent established security mechanisms. 1s Our analysis has shown that there is signiﬁcant demand for expired domain names (e.g., over 10 % of all com domains re-registered immediately on the day that they were deleted), and that there is a highly competitive envir- onment of drop-catch services that race to be the ﬁrst to re-register a domain in the very instant that it is deleted (e.g., over half of org re-registrations on the deletion day take place within a 30 s time frame). In the current system, the drop-catch service with most technical resources and the best insight into details of the drop is going to be most successful in re-registering deleted domains for their cus- tomers. However, the uncertainty of this process and lack of transparency as to which service is most successful res- ult in the common recommendation that customers place orders with all services [7, 12, 31]. The re-registration race is open to all registrars, and manual re-registration is at least a theoretical possibility, but it is quite wasteful of resources as drop-catch services cause a daily ﬂood of requests as a byproduct of determining the next owner. Pre-release domain sales typically take place as auc- tions, thus they are efﬁcient from a technical point of view. However, there are administrative concerns, as pre-release sales do not allow buyers to freely choose their registrar, prevent the former domain owner from using the 30-day redemption period to recover the expired domain, and might incentivise registrars to make late domain renewals more difﬁcult (or expensive) for their customers because of the potentially more lucrative pre-release sales. From a security perspective, domain ownership changes are problematic because of their potential to break domain-based trust mechanisms [44], abuse resid- ual trust [33], and more generally proﬁt from residual trafﬁc in various ways that are not necessarily illegal, but often undesirable. While banning domain ownership changes altogether may not be practicable, we argue that the process should be made more transparent. State-of- the-art anti-abuse systems may ﬁnd it challenging to de- tect domain ownership changes such as pre-release sales because they do not result in a new domain creation. As a policy-based approach, registrars could be required to maintain a public log of ownership changes, similar to Certiﬁcate Transparency [30], so that security mechan- isms can “reset” trust in a reliable way: Whitelists can drop domains after certain changes of ownership, web browsers can purge cached website permissions, and web- sites can remove links pointing to a deleted domain. What exactly drives that demand for expired domain names, whether it is intended “productive” use, abuse [22, 33], monetisation through advertising [48], or speculation with the goal of reselling the domain name, is still an open question, and an interesting direction for future work."
Cybersecurity and Privacy,William Robertson,https://www.khoury.northeastern.edu/people/william-robertson/,"William Robertson is an associate professor in the Khoury College of Computer Sciences and the College of Engineering at Northeastern University, based in Boston. He co-directs the Northeastern Systems Security Lab and is principal investigator for the Cybersecurity and Privacy Institute's Diverge Lab.",Systems security; Web security; Mobile security,"PhD in computer science, University of California, Santa Barbara; BS in computer science, University of California, Santa Barbara","August 16th, 2016",Tracing Information Flows Between Ad Exchanges Using Retargeted Ads,https://cbw.sh/static/pdf/bashir-usenix16.pdf," Muhammad Ahmad Bashir, Sajjad Arshad, William Robertson, Christo Wilson. ""Tracing Information Flows Between Ad Exchanges Using Retargeted Ads"". In Proceedings of Usenix Security. Austin, TX, August, 2016.","Numerous surveys have shown that Web users are con- cerned about the loss of privacy associated with online tracking. Alarmingly, these surveys also reveal that peo- ple are also unaware of the amount of data sharing that occurs between ad exchanges, and thus underestimate the privacy risks associated with online tracking. In reality, the modern ad ecosystem is fueled by a ﬂow of user data between trackers and ad exchanges. Al- though recent work has shown that ad exchanges rou- tinely perform cookie matching with other exchanges, these studies are based on brittle heuristics that cannot detect all forms of information sharing, especially under adversarial conditions. In this study, we develop a methodology that is able to detect client- and server-side ﬂows of information be- tween arbitrary ad exchanges. Our key insight is to lever- age retargeted ads as a tool for identifying information ﬂows. Intuitively, our methodology works because it re- lies on the semantics of how exchanges serve ads, rather than focusing on speciﬁc cookie matching mechanisms. Using crawled data on 35,448 ad impressions, we show that our methodology can successfully categorize four different kinds of information sharing behavior between ad exchanges, including cases where existing heuristic methods fail. We conclude with a discussion of how our ﬁndings and methodologies can be leveraged to give users more control over what kind of ads they see and how their in- formation is shared between ad exchanges. 1s or recommendations expressed in this mate- rial are those of the authors and do not necessarily reﬂect the views of the NSF."
Cybersecurity and Privacy,William Robertson,https://www.khoury.northeastern.edu/people/william-robertson/,"William Robertson is an associate professor in the Khoury College of Computer Sciences and the College of Engineering at Northeastern University, based in Boston. He co-directs the Northeastern Systems Security Lab and is principal investigator for the Cybersecurity and Privacy Institute's Diverge Lab.",Systems security; Web security; Mobile security,"PhD in computer science, University of California, Santa Barbara; BS in computer science, University of California, Santa Barbara","August 11th, 2016","UNVEIL: A Large-Scale, Automated Approach to Detecting Ransomware",https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_kharraz.pdf," A. Kharraz, S. Arshad, C. Mulliner, W. Robertson, E. Kirda. ""UNVEIL: A Large-Scale, Automated Approach to Detecting Ransomware"". In USENIX Security Symposium Austin, TX US, Aug 2016.","Although the concept of ransomware is not new (s In this paper we presented UNVEIL, a novel approach to detecting and analyzing ransomware. Our system is the first in the literature to specifically identify typical behavior of ransomware such as malicious encryption of files and locking of user desktops. These are behaviors that are difficult for ransomware to hide or change. The evaluation of UNVEIL shows that our approach was able to correctly detect 13,637 ransomware samples from multiple families in a real-world data feed with zero false positives. In fact, UNVEIL outperformed all ex- isting AV scanners and a modern industrial sandboxing technology in detecting both superficial and technically sophisticated ransomware attacks. Among our findings was also a new ransomware family that no security com- pany had previously detected before we submitted it to VirusTotal. 9"
Cybersecurity and Privacy,Daniel Wichs,https://www.khoury.northeastern.edu/people/daniel-wichs/,"Daniel Wichs is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Modern cryptography; Information security,"PhD in Computer Science, New York University; MS in Computer Science, Stanford University; BS in Mathematics, Stanford University","October 15th, 2017",Obfuscating Compute-and-Compare Programs under LWE,http://ieee-focs.org/FOCS-2017-Papers/3464a600.pdf, Daniel Wichs and Giorgos Zirdelis,Failed to download
Data Management,Mirek Riedewald,https://www.khoury.northeastern.edu/people/mirek-riedewald/,"Mirek Riedewald is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Databases; Data mining,"PhD in Computer Science, University of California, Santa Barbara; BS in Computer Science, Saarland University — Germany","May 1st, 2023",Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals,https://www.vldb.org/pvldb/vol16/p2377-chen.pdf," Zixuan Chen, Panagiotis Manolios, Mirek Riedewald. (2023). Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals Proc. VLDB Endow., 16, 2377-2390. https://www.vldb.org/pvldb/vol16/p2377-chen.pdf","This work considers why-not questions in the context of top-k queries and score-based ranking functions. Following the popular linear scalarization approach for multi-objective optimization, we study rankings based on the weighted sum of multiple scores. A given weight choice may be controversial or perceived as unfair to certain individuals or organizations, triggering the question why some entity of interest has not yet shown up in the top-k. We introduce various notions of such why-not-yet queries and for- mally define them as satisfiability or optimization problems, whose goal is to propose alternative ranking functions that address the placement of the entities of interest. While some why-not-yet prob- lems have linear constraints, others require quantifiers, disjunction, and negation. We propose several optimizations, ranging from a monotonic-core construction that approximates the complex con- straints with a conjunction of linear ones, to various techniques that let the user control the tradeoff between running time and approximation quality. Experiments with real and synthetic data demonstrate the practicality and scalability of our technique, show- ing its superiority compared to the state of the art (SOA). PVLDB Reference Format: Zixuan Chen, Panagiotis Manolios, and Mirek Riedewald. Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals. PVLDB, 16(9): 2377 - 2390, 2023. doi:10.14778/359858We propose the first general exact solution for problems SAT, BEST, and POINT. Adopting sampling approaches from related work can only provide approximate answers or results in infeasible running time for BEST. In general, sampling becomes ineffective when only a small fraction of the space of possible weight vectors ranks the expected tuples among the top-𝑘. For BOX, we propose the first known solution. To make it practical and scalable, we propose the notion of a monotonic core. Our clustering approach enables the user to improve running time for all problems as desired by controlling the number of clusters, with moderate loss in result quality even for large data. Interesting avenues for future work are computing a compact description of the entire set of weight vectors that rank the expected tuples among the top-𝑘and generalizing the approach to noisy and unreliable data."
Data Management,Mirek Riedewald,https://www.khoury.northeastern.edu/people/mirek-riedewald/,"Mirek Riedewald is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Databases; Data mining,"PhD in Computer Science, University of California, Santa Barbara; BS in Computer Science, Saarland University — Germany","January 1st, 2021","Beyond Equi-joins: Ranking, Enumeration and Factorization",http://www.vldb.org/pvldb/vol14/p2599-tziavelis.pdf," Nikolaos Tziavelis, Wolfgang Gatterbauer, Mirek Riedewald. (2021). Beyond Equi-joins: Ranking, Enumeration and Factorization Proc. VLDB Endow., 14, 2599-2612. http://www.vldb.org/pvldb/vol14/p2599-tziavelis.pdf","We study theta-joins in general and join predicates with conjunc- tions and disjunctions of inequalities in particular, focusing on ranked enumeration where the answers are returned incrementally in an order dictated by a given ranking function. Our approach achieves strong time and space complexity properties: with 𝑛denot- ing the number of tuples in the database, we guarantee for acyclic full join queries with inequality conditions that for every value of 𝑘, the 𝑘top-ranked answers are returned in O(𝑛polylog𝑛+ 𝑘log𝑘) time. This is within a polylogarithmic factor of O(𝑛+ 𝑘log𝑘),S AND FUTURE WORK Theta- and inequality-joins of multiple relations are generally con- sidered “hard” and even state-of-the-art commercial DBMSs strug- gle with their efficient computation. We developed the first ranked- enumeration techniques that achieve non-trivial worst-case guar- antees for a large class of these joins: For small 𝑘, returning the 𝑘 top-ranked join answers for full acyclic queries takes only slightly- more-than-linear time and space (O(𝑛polylog𝑛)) for any DNF of inequality predicates. For general theta-joins, time and space com- plexity are quadratic in input size. These are strong worst-case guar- antees, close to the lower time bound of O(𝑛) and much lower than the O(𝑛ℓ) size of intermediate or final results traditional join algo- rithms may have to deal with. Our results apply to many cyclic joins (modulo higher pre-processing cost depending on query width) and all acyclic joins, even those with selections and many types of pro- jections. In the future, we will study parallel computation and more general cyclic joins and projections."
Data Science,Albert-László Barabási,https://www.khoury.northeastern.edu/people/albert-laszlo-barabasi/,"Albert-László Barabási is the Robert Gray Dodge Professor of Network Science and a Distinguished University Professor at Northeastern University, based in Boston. He directs the Center for Complex Network Research and holds appointments in the College of Science and the Khoury College of Computer Sciences.",Network science; Properties of sub-cellular networks in understanding human disease; Control theory,"PhD in Physics, Boston University; MS in Theoretical Physics, Eötvös Loránd University — Hungary; BS in Physics and Engineering, University of Bucharest — Romania","September 8th, 2020",3D Topology Transformation with Generative Adversarial Networks,http://computationalcreativity.net/iccc20/papers/052-iccc20.pdf," Luca Stornaiuolo, Nima Dehmamy, Albert-László Barabási, Mauro Martino. (2020). 3D Topology Transformation with Generative Adversarial Networks ICCC, 461-468. http://computationalcreativity.net/iccc20/papers/052-iccc20.pdf","Generation and transformation of images and videos using artiﬁcial intelligence have ﬂourished over the past few years. Yet, there are only a few works aim- ing to produce creative 3D shapes, such as sculptures. Here we show a novel 3D-to-3D topology transfor- mation method using Generative Adversarial Networks (GAN). We use a modiﬁed pix2pix GAN, which we call Vox2Vox, to transform the volumetric style of a 3D ob- ject while retaining the original object shape. In par- ticular, we show how to transform 3D models into two new volumetric topologies - the 3D Network and the Ghirigoro. We describe how to use our approach to con- struct customized 3D representations. We believe that the generated 3D shapes are novel and inspirational. Fi- nally, we compare the results between our approach and a baseline algorithm that directly convert the 3D shapes, without using our GAN.and Future Direction In this paper, we presented a novel 3D-to-3D topology trans- fer paradigm based on transformations in 3D space. In particular, we built a 3D conditional GAN, Vox2Vox, that performs volumetric transformations to modify the internal structure of any 3D object, while maintaining its overall shape. We described our complete pipeline to apply our ap- proach to two different topologies: the 3D Network and the Ghirigoro. The results obtained by employing our method- ology are novel and inspirational. We compared the out- puts of the pipeline while using or not the 3D-cGAN and found that using the Vox2Vox output as a prior distribution results in much nicer outcomes where features are placed in strategic positions in the 3D shape preserving its struc- tural features. As a future direction, we plan to improve the 3D-to-3D topology transfer by given also the topology as a conditional input of the generative network. To do that, the machine learning algorithm has to learn itself the abstraction of the topology from a given 3D object."
Data Science,Albert-László Barabási,https://www.khoury.northeastern.edu/people/albert-laszlo-barabasi/,"Albert-László Barabási is the Robert Gray Dodge Professor of Network Science and a Distinguished University Professor at Northeastern University, based in Boston. He directs the Center for Complex Network Research and holds appointments in the College of Science and the Khoury College of Computer Sciences.",Network science; Properties of sub-cellular networks in understanding human disease; Control theory,"PhD in Physics, Boston University; MS in Theoretical Physics, Eötvös Loránd University — Hungary; BS in Physics and Engineering, University of Bucharest — Romania","April 18th, 2020","Give more data, awareness and control to individual citizens, and they will help COVID-19 containment",http://www.tdp.cat/issues16/tdp.a389a20.pdf," Mirco Nanni, Gennady L. Andrienko, Albert-László Barabási, Chiara Boldrini, Francesco Bonchi, Ciro Cattuto, Francesca Chiaromonte, Giovanni Comandé, Marco Conti, Mark Coté, Frank Dignum, Virginia Dignum, Josep Domingo-Ferrer, Paolo Ferragina, Fosca Giannotti, Riccardo Guidotti, Dirk Helbing, Kimmo Kaski, János Kertész, Sune Lehmann, Bruno Lepri, Paul Lukowicz, Stan Matwin, David Megías, Anna Monreale, Katharina Morik, Nuria Oliver, Andrea Passarella, Andrea Passerini, Dino Pedreschi, Alex Pentland, Fabio Pianesi, Francesca Pratesi, Salvatore Rinzivillo, Salvatore Ruggieri, Arno Siebes, Vicenç Torra, Roberto Trasarti, Jeroen van den Hoven, Alessandro Vespignani. (2020). Give more data, awareness and control to individual citizens, and they will help COVID-19 containment Trans. Data Priv., 13, 61-66. http://www.tdp.cat/issues16/tdp.a389a20.pdf","The rapid dynamics of COVID-19 calls for quick and effective tracking of virus transmis- sion chains and early detection of outbreaks, especially in the “phase 2” of the pandemic, when lockdown and other restriction measures are progressively withdrawn, in order to avoid or mini- mize contagion resurgence. For this purpose, contact-tracing apps are being proposed for large scale adoption by many countries. A centralized approach, where data sensed by the app are all sent to a nation-wide server, raises concerns about citizens’ privacy and needlessly strong digital surveillance, thus alerting us to the need to minimize personal data collection and avoiding location tracking. We advocate the conceptual advantage of a decentralized approach, where both contact and location data are collected exclusively in individual citizens’ “personal data stores”, to be shared separately and selectively (e.g., with a backend system, but possibly also with other citizens), voluntarily, only when the citizen has tested positive for COVID-19, and with a privacy preserving level of granularity. This 61 62 Mirco Nanni et al. approach better protects the personal sphere of citizens and affords multiple beneﬁts: it allows for detailed information gathering for infected people in a privacy-preserving fashion; and, in turn this enables both contact tracing, and, the early detection of outbreak hotspots on more ﬁnely-granulated geographic scale. The decentralized approach is also scalable to large populations, in that only the data of positive patients need be handled at a central level. Our recommendation is two-fold. First to extend existing decentralized architectures with a light touch, in order to manage the collection of lo- cation data locally on the device, and allow the user to share spatio-temporal aggregates – if and when they want and for speciﬁc aims – with health authorities, for instance. Second, we favour a longer- term pursuit of realizing a Personal Data Store vision, giving users the opportunity to contribute to collective good in the measure they want, enhancing self-awareness, and cultivating collective efforts for rebuilding society. Keywords. COVID-19, Personal Data Store, mobility data analysis, contact tracing. 1conclusion not found"
Data Science,Mirek Riedewald,https://www.khoury.northeastern.edu/people/mirek-riedewald/,"Mirek Riedewald is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Databases; Data mining,"PhD in Computer Science, University of California, Santa Barbara; BS in Computer Science, Saarland University — Germany","May 1st, 2023",Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals,https://www.vldb.org/pvldb/vol16/p2377-chen.pdf," Zixuan Chen, Panagiotis Manolios, Mirek Riedewald. (2023). Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals Proc. VLDB Endow., 16, 2377-2390. https://www.vldb.org/pvldb/vol16/p2377-chen.pdf","This work considers why-not questions in the context of top-k queries and score-based ranking functions. Following the popular linear scalarization approach for multi-objective optimization, we study rankings based on the weighted sum of multiple scores. A given weight choice may be controversial or perceived as unfair to certain individuals or organizations, triggering the question why some entity of interest has not yet shown up in the top-k. We introduce various notions of such why-not-yet queries and for- mally define them as satisfiability or optimization problems, whose goal is to propose alternative ranking functions that address the placement of the entities of interest. While some why-not-yet prob- lems have linear constraints, others require quantifiers, disjunction, and negation. We propose several optimizations, ranging from a monotonic-core construction that approximates the complex con- straints with a conjunction of linear ones, to various techniques that let the user control the tradeoff between running time and approximation quality. Experiments with real and synthetic data demonstrate the practicality and scalability of our technique, show- ing its superiority compared to the state of the art (SOA). PVLDB Reference Format: Zixuan Chen, Panagiotis Manolios, and Mirek Riedewald. Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals. PVLDB, 16(9): 2377 - 2390, 2023. doi:10.14778/359858We propose the first general exact solution for problems SAT, BEST, and POINT. Adopting sampling approaches from related work can only provide approximate answers or results in infeasible running time for BEST. In general, sampling becomes ineffective when only a small fraction of the space of possible weight vectors ranks the expected tuples among the top-𝑘. For BOX, we propose the first known solution. To make it practical and scalable, we propose the notion of a monotonic core. Our clustering approach enables the user to improve running time for all problems as desired by controlling the number of clusters, with moderate loss in result quality even for large data. Interesting avenues for future work are computing a compact description of the entire set of weight vectors that rank the expected tuples among the top-𝑘and generalizing the approach to noisy and unreliable data."
Data Science,Mirek Riedewald,https://www.khoury.northeastern.edu/people/mirek-riedewald/,"Mirek Riedewald is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Databases; Data mining,"PhD in Computer Science, University of California, Santa Barbara; BS in Computer Science, Saarland University — Germany","January 1st, 2021","Beyond Equi-joins: Ranking, Enumeration and Factorization",http://www.vldb.org/pvldb/vol14/p2599-tziavelis.pdf," Nikolaos Tziavelis, Wolfgang Gatterbauer, Mirek Riedewald. (2021). Beyond Equi-joins: Ranking, Enumeration and Factorization Proc. VLDB Endow., 14, 2599-2612. http://www.vldb.org/pvldb/vol14/p2599-tziavelis.pdf","We study theta-joins in general and join predicates with conjunc- tions and disjunctions of inequalities in particular, focusing on ranked enumeration where the answers are returned incrementally in an order dictated by a given ranking function. Our approach achieves strong time and space complexity properties: with 𝑛denot- ing the number of tuples in the database, we guarantee for acyclic full join queries with inequality conditions that for every value of 𝑘, the 𝑘top-ranked answers are returned in O(𝑛polylog𝑛+ 𝑘log𝑘) time. This is within a polylogarithmic factor of O(𝑛+ 𝑘log𝑘),S AND FUTURE WORK Theta- and inequality-joins of multiple relations are generally con- sidered “hard” and even state-of-the-art commercial DBMSs strug- gle with their efficient computation. We developed the first ranked- enumeration techniques that achieve non-trivial worst-case guar- antees for a large class of these joins: For small 𝑘, returning the 𝑘 top-ranked join answers for full acyclic queries takes only slightly- more-than-linear time and space (O(𝑛polylog𝑛)) for any DNF of inequality predicates. For general theta-joins, time and space com- plexity are quadratic in input size. These are strong worst-case guar- antees, close to the lower time bound of O(𝑛) and much lower than the O(𝑛ℓ) size of intermediate or final results traditional join algo- rithms may have to deal with. Our results apply to many cyclic joins (modulo higher pre-processing cost depending on query width) and all acyclic joins, even those with selections and many types of pro- jections. In the future, we will study parallel computation and more general cyclic joins and projections."
Formal Methods,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","June 1st, 2019",MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing,http://www.ccs.neu.edu/home/gene/papers/hpdc19.pdf," ""MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing"", Rohan Garg, Gregory Price, and Gene Cooperman, Proc. of 28th Int. Symp. on High Performance Parallel and Distributed Computing, Phoenix, AZ, USA, ACM, pp. 49--60, June, 2019","Transparently checkpointing MPI for fault tolerance and load bal- ancing is a long-standing problem in HPC. The problem has been complicated by the need to provide checkpoint-restart services for all combinations of an MPI implementation over all network interconnects. This work presents MANA (MPI-Agnostic Network- Agnostic transparent checkpointing), a single code base which sup- ports all MPI implementation and interconnect combinations. The agnostic properties imply that one can checkpoint an MPI appli- cation under one MPI implementation and perhaps over TCP, and then restart under a second MPI implementation over InfiniBand on a cluster with a different number of CPU cores per node. This tech- nique is based on a novel split-process approach, which enables two separate programs to co-exist within a single process with a single address space. This work overcomes the limitations of the two most widely adopted transparent checkpointing solutions, BLCR and DMTCP/InfiniBand, which require separate modifications to each MPI implementation and/or underlying network AP2 MANA: DESIGN AND IMPLEMENTATION Multiple aspects of the design of MANA are covered in this sec- tion. Section 2.1 discusses the design for supporting a split-process. Section 2.2 discusses the need to save and restore persistent MPI opaque objects, such as communicators, groups and topologies. Section 2.3 briefly discusses the commonly used algorithm to drain point-to-point MPI messages in transit prior to intiaiting a check- point. Sections 2.4 and 2.5 present a new two-phase algorithm (Algorithm 2), which enables checkpointing in-progress MPI collec- tive communication calls in a fully agnostic environment. Finally, Sections 2.6 and 2.7 present details of the overall implementation of MANA. 2.1 Upper and Lower Half: Checkpointing with an Ephemeral MPI Library In this section, we define the lower half of a split-process as the memory associated with the MPI library and dependencies, includ- ing network libraries. The upper half is the remaining Linux process memory associated with the MPI application’s code, data, stack, and other regions (e.g., environment variables). The terms lower half and upper half are in analogy with the upper and lower half of a device driver in an operating system kernel. This separation into lower and upper half does not involve additional threads or processes. Instead, it serves primarily to tag memory so that only upper half memory will be saved or restored during checkpoint and restart. Section 2.6 describes an additional “helper thread”, but that thread is active only during checkpoint and restart. Libc and other system libraries may appear in both the lower half as a dependency of the MPI libraries, and the upper half as an independent dependency of the MPI application. This split-process approach allows MANA to balance two con- flicting objectives: a shared address space; and isolation of upper and lower halves. The isolation allows MANA to omit the lower half memory (an “ephemeral” MPI library) when it creates a checkpoint image file. The shared address space allows the flow of control to pass efficiently from the upper-half MPI application to the lower- half MPI library through standard C/Fortran calling conventions, including call by reference. As previously noted, Remote Produce Calls (RPC) are not employed. Isolation is needed so that at checkpoint time, the lower half can be omitted from the checkpoint image, and at the time of restart, replaced with a small “bootstrap” MPI program with new MPI li- braries. The bootstrap program calls MPI_Init() and each MPI process discovers its MPI rank via a call to MPI_Rank(). The mem- ory present at this time becomes the lower half. The MPI process then restores the upper-half memory from a checkpoint image file corresponding to the MPI rank id. Control is then transferred back to the upper-half MPI application, and the stack in the lower half is never used again. Shared address space is needed for efficiency. A dual-process proxy approach was explored in [16, Section IV.B] and in [35, Sec- tion IV.A]. The former work reported a 6% runtime overhead for real-world CUDA applications, and the latter work reported run- time overheads in excess of 20% for some OpenCL examples from the NVIDIA SDK 3.0. In contrast, Section 3 reports runtime over- heads less than 2% for MANA under older Linux kernels, and less than 1% runtime overhead for recent Linux kernels. Discarding the lower half greatly simplifies the task of check- pointing. By discarding the lower half, the MPI application in the upper half appears as an isolated process with no inter-process communication. Therefore, a single-process checkpointing package can create a checkpoint image. A minor inconvenience of this split-process approach is that calls to sbrk() will cause the kernel to extend the process heap in the data segment. Calls to sbrk() can be caused by invocations of malloc(). Since the kernel has no concept of a split-process, the kernel may choose, for example, to extend the lower half data segment after restart since that corresponds to the original program seen by the kernel before the upper-half memory is restored. MANA resolves this by interposing on calls to sbrk() in the upper-half libc, and then inserts calls to mmap() to extend the heap of the upper-half. Finally, MANA employs coordinated checkpointing, and a check- point coordinator sends messages to each MPI rank at the time of checkpoint (see Sections 2.3, 2.4 and 2.5). MPI opaque objects (communicators, groups, topologies) are detected on creation and restored on restart (see Section 2.2). This is part of a broader strat- egy by which MPI calls with persistent effects (such as creation of these opaque objects) are recorded during runtime and replayed on restart. 2.2 Checkpointing MPI Communicators, Groups, and Topologies An MPI application can create communication subgroups and topolo- gies to group processes for ease of programmability and efficient communication. MPI implementations provide opaque handles to the application as a reference to a communicator object or group. MANA interposes on all calls that refer to these opaque identi- fiers, and virtualizes the identifiers. At runtime, MANA records any MPI calls that can modify the MPI communication state, such as MPI_Comm_create, MPI_Group_incl, etc. On restart, MANA recre- ates the MPI communicator state by replaying the MPI calls using a new MPI library. The runtime virtualization of identifiers allows the application to continue running with consistent handles across checkpoint-restart. A similar checkpointing strategy also works for other opaque identifiers, such as, MPI derived datatypes, etc. 2.3 Checkpointing MPI Point-to-Point Communication Capturing the state of MPI processes requires quiescing the process threads, and preserving the process memory to a file on the disk. However, this alone is not sufficient to capture a consistent state of the computation. Any MPI messages sent but not yet received at the time of quiescing processes must also be saved as part of the checkpoint image. MANA employs a variation of an all-to-all bookmark exchange algorithm to reach this consistent state. LAM/MPI [31] demon- strated the efficacy of a such a Chandy/Lamport [11] algorithm for checkpointing MPI applications. Hursey et al. [22] lifted this mechanism out of interconnect drivers and into the MPI library. MANA further lifts this mechanism outside the MPI library, and into a virtualized MPI API. An early prototype of MANA demonstrated a naïve application of this bookmark exchange algorithm was sufficient for handling pre-checkpoint draining for point-to-point communication; how- ever, collective-communication calls may have MPI implementation effects that can determine when it is “safe” to begin a checkpoint. For this reason, a naïve application to the entire API was insufficient to ensure correctness. This is discussed in Section 2.4. 2.4 Checkpointing MPI Collectives: Overview The MPI collective communications primitive involves communi- cation amongst all or a program-defined subset of MPI ranks (as specified by the MPI communicator argument to the function). The internal behavior of collectives are specific to each MPI implemen- tation, and so it is not possible to make guarantees about their behavior, such as when and how messages are exchanged when ranks are waiting for one or more ranks to enter the collective. In prior work [22, 31], internal knowledge of the MPI library state was required to ensure that checkpointing would occur at a “safe” state. In particular, Hursey et al. [22] required interconnect drivers be classified as “checkpoint-friendly” or “checkpoint-unfriendly”, changing behavior based on this classification. As MANA lives outside the MPI library, a naive application of the Hursey et al. algorithm can have effects that cross the upper and lower half boundaries of an MPI rank (for example, when shared memory is being used for MPI communication). This problem occurs because of the truly network-agnostic trait of MANA. As MANA has no concept of transport level constructs, it cannot determine what “safe” means in context of collectives. To correct this, MANA’s support for collective communication requires it to maintain the following invariant: No checkpoint must take place while a rank is inside a collective communication call. There exists one exception to this rule: a trivial barrier. A trivial barrier is a simple call to MPI_Barrier(). This call produces no side effects on an MPI rank, and so it can be safely interrupted during checkpoint, and then re-issued when restarting the MPI application. This is possible due to the split-process architecture of MANA, as trivial barrier calls occur exclusively in the lower half, which is discarded and replaced across checkpoint and restart. MANA leverages this exception to build a solution for all other collective calls. As we discuss MANA’s algorithm for checkpointing collective calls, we take into consideration three subtle, but important, con- cerns. Challenge I (consistency): In the case of a single MPI collec- tive communication call, there is a danger that rank A will see a request to checkpoint before entering the collective call, while rank B will see the same request after entering the collective call, in violation of MANA’s invariant. Both ranks might report that they are ready to checkpoint, and the resulting inconsistent snapshot will create problems during restart. This situation could arise, for example, if the mes- sage from the checkpoint coordinator to rank B is excessively delayed in the network. To resolve this, MANA introduces a two-pass protocol in which the coordinator makes a re- quest (sends an intend-to-checkpoint message), each MPI rank acknowledges with its current state, and finally the coordinator posts a checkpoint request (possibly preceded by extra iterations). Challenge II (progress and latency): Given the aforementioned solution for consistency, long delays may occur before a checkpoint request can be serviced. It may be that rank A has entered the barrier, and rank B will require several hours to finish a task before entering the barrier. Hence, the two-pass protocol may create unacceptable delays before a checkpoint can be taken. Algorithm 2 addresses this by introducing a trivial barrier prior to the collective communication call. We refer to this as a two-phase algorithm since each collective call is now replaced by a wrapper function that invokes a trivial barrier call (phase 1) followed by the original collective call (phase 2). Challenge III (multiple collective calls): Until now, it was assumed that at most one MPI collective communication call was in progress at the time of checkpoint. It may happen that there are multiple ongoing collective calls. During the time that some MPI ranks exit from a collective call, it may happen that MPI ranks associated with an independent col- lective call have left the MPI trivial barrier (phase 1) and have now entered the real collective call (phase 2). As a result, servicing a checkpoint may be excessive delayed. To solve this, we introduce an intend-to-checkpoint message, such that no ranks will be allowed to enter phase 2, and extra itera- tions will be inserted into the request-acknowledge protocol between coordinator and MPI rank. 2.5 Checkpointing MPI Collectives: Detailed Algorithm Here we present a single algorithm (Algorithm 2) for checkpointing MPI collectives which contains the elements described in Section 2.4: a multi-iteration protocol; and a two-phase algorithm incorporating a trivial barrier before any collective communication call. From the viewpoint of an MPI application, any call to an MPI collective communication function is interposed on by a wrapper function, as shown in Algorithm 1. Algorithm 1 Two-Phase collective communication wrapper. (This wrapper function interposes on all MPI collective com- munication functions invoked by an MPI application) 1: function Collective Communication Wrapper 2: # Begin Phase 1 3: Call MPI_Barrier() # trivial barrier 4: # Begin Phase 2 5: Call original MPI collective communication function 6: end function Recall that a trivial barrier is an extra call to MPI_Barrier() prior to a collective call. A collective MPI call can intuitively be divided into two parts: the participating MPI ranks “register” themselves as ready for the collective communication; and then the “work” of communication is carried out. Where the time for the collective communication calls of an MPI program is significant, it is typically due to significant “work” in the second part of the calls. Adding a trivial barrier requires the MPI ranks to register themselves once for the trvial barrier (but no work is involved), and then register themselves again for the actual MPI collective communication. The overhead due to registering twice is tiny in practice. Evidence for this can be seen in the experiments in Section 3.2.3, which show small overhead. The purpose of Algorithm 1 is to enforce the following extension of the invariant presented in Section 2.4: No checkpoint must take place while a rank is inside the collective communication call (Phase 2) of a wrapper function for collective communication (Algorithm 1). We formalize this with the following theorem, which guarantees Algorithm 2 satisfies this invariant. Theorem 1. Under Algorithm 2, an MPI rank is never inside a collective communication call when a checkpoint message is received from the checkpoint coordinator. The proof of this theorem is deferred until the end of this sub- section. We begin the path to this proof by stating an axiom that serves to define the concept of a barrier. Axiom 1. For a given invocation of an MPI barrier, it never happens that a rank A exits from the barrier before another rank B enters the barrier under the “happens-before” relation. Next, we present the following two lemmas. Checkpoint Coordinator Rank A Rank B Barrier (1) (3) (2) (4) Figure 1: Fundamental “happens-before” relation in commu- nication between the checkpoint coordinator and the MPI ranks involved in an MPI barrier. Lemma 1. For a given MPI barrier, if the checkpoint coordinator sends a message to each MPI rank participating in the barrier, and if at least one of the reply messages from the participating ranks reports that its rank has exited the barrier, then the MPI coordinator can send a second message to each participating rank, and each MPI rank will reply that it has entered the barrier (and perhaps also exited the barrier). Proof. We prove the lemma by contradiction. Suppose that the lemma does not hold. Figure 1 shows the general case in which this happens. At event 4, the checkpoint coordinator will conclude that event 1 (rank A has exited the MPI barrier) happened before event 2 (the first reply by each rank), which happened before event 3 (in which rank B has not yet entered the barrier). But this contradicts Axiom 1. Therefore, our assumption is false, and the lemma does indeed hold. □ Lemma 2. Recall that an MPI collective communication wrapper makes a call to a trivial barrier and then makes an MPI collective communication call. For a given invocation of an MPI collective com- munication wrapper, we know that one of four cases must hold: (a) an MPI rank is in the collective communication call, and all other ranks are either in the call, or have exited; (b) an MPI rank is in the collective communication call, and no rank has exited, and every other rank has at least entered the trivial barrier (and possibly proceeded further); (c) an MPI rank is in the trivial barrier and no other rank has exited (but some may not yet have entered the trivial barrier); (d) either no MPI rank has entered the trivial barrier, or all MPI ranks have exited the MPI collective communication call. Proof. The proof is by repeated application of Lemma 1. For case a, if an MPI rank is in the collective communication call and another rank has exited the collective call, then Lemma 1 says that there cannot be any rank that has not yet entered the collective call. For case b, note that if an MPI rank is in the collective communi- cation call, then that rank has exited the trivial barrier. Therefore, by Lemma 1, all other ranks have at least entered the trivial barrier. Further, we can assume that no ranks that have exited the collec- tive call, since we would otherwise be in case a, which is already accounted for. For case c, note that if an MPI rank is in the trivial barrier and no rank has exited the trivial barrier, then Lemma 1 says that there cannot be any rank that has not yet entered the trivial barrier. Finally, if we are not in case a, b, or c, then the only remaining possibility is case d: all ranks have not yet entered the trivial barrier or all ranks have exited the collective call. □ Algorithm 2 Two-Phase algorithm for deadlock-free check- pointing of MPI collectives 1: Messages: {intend-to-checkpoint, extra-iteration, do-ckpt} 2: MPI states: {ready, in-phase-1, exit-phase-2} 3: Process Checkpoint Coordinator do 4: function Begin Checkpoint 5: send intend-to-ckpt msg to all ranks 6: receive responses from each rank 7: while some rank in state exit-phase-2 do 8: send extra-iteration msg to all ranks 9: receive responses from each rank 10: end while 11: send do-ckpt msg to all ranks 12: end function 13: Process MPI Rank do 14: upon event intend-to-ckpt msg or extra-iteration msg do 15: if not inCollectiveWrapper then 16: reply to ckpt coord: state ←ready 17: end if 18: if inCollectiveWrapper and in Phase 1 then 19: reply to ckpt coord: state ←in-phase-1 20: end if 21: if inCollectiveWrapper and in Phase 2 then 22: # guaranteed ckpt coord won’t request ckpt here 23: finish executing coll. comm. call 24: reply to ckpt coord: state ←exit-phase-2 25: # ckpt coord can request ckpt after this 26: set state ←ready 27: end if 28: continue, but wait before next coll. comm. call 29: upon event do-ckpt msg do 30: # guaranteed now that no rank is in phase 2 during ckpt 31: do local checkpoint for this rank 32: # all ranks may now continue executing 33: if this rank is waiting before coll. comm. call then 34: unblock this rank and continue executing 35: end if We now continue with the proof of the main theorem (Theo- rem 1), which was deferred earlier. Proof. (Proof of Theorem 1 for Algorithm 2.) Lemma 2 states that one of four cases must hold in a call by MANA to an MPI collective communication wrapper. We wish to exclude the possibility that an MPI rank is in the collective communication call (case a or b of the lemma) when the checkpoint coordinator invokes a checkpoint. In Algorithm 2, assume that the checkpoint coordinator has sent an intend-to-ckpt message, and has not yet sent a do-ckpt message. An MPI rank will either reply with state ready or in-phase-1 (show- ing that it is not in the collective communication call and that it will stop before entering the collective communication call), or else it must be in Phase 2 of the wrapper (potentially within the collective communication call), and it will not reply to the coordinator until exiting the collective call. □ Theorem 2. Under Algorithm 2, deadlock will never occur. Further, the delay between the time when all ranks have received the intend- to-checkpoint message and the time when the do-ckpt message has been sent is bounded by the maximum time for any individual MPI rank to enter and exit the collective communication call, plus network message latency. Proof. The algorithm will never deadlock, since each rank must either make progress based on the normal MPI operation or else it stops before the collective communication call. If any rank replies with the state exit-phase-2, then the checkpoint coordinator will send an additional extra-iteration message. So, at the time of check- point, all ranks will have state ready or in-phase-1. Next, the delay between the time when all ranks have received the intend-to-checkpoint message and the time when the do-ckpt message has been sent is clearly bounded by the maximum time for an individual MPI rank to enter and exit the collective commu- nication call, plus the usual network message latency. This is the case since once the intend-to-checkpoint message is received, no MPI rank may enter the collective communication call. So, upon re- ceiving the intend-to-checkpoint message, either the rank is already in Phase 2 or else it will remain in Phase 1. □ Implementation of Algorithm 2: At the time of process launch for an MPI rank, a separate checkpoint helper thread is also in- jected into each rank. This thread is responsible for listening to checkpoint-related messages from a separate coordinator process and then responding. This allows the MPI rank to asynchronously process events based on messages received from the checkpoint coordinator. Furthermore at the time of checkpoint, the existing threads of the MPI rank process are quiesced (paused) by the helper thread, and the helper thread carries out the checkpointing require- ments, such as copying the upper-half memory regions to stable storage. The coordinator process does not participate in the check- pointing directly. In the implementation, a DMTCP coordinator and DMTCP checkpoint thread [1] are modified to serve as checkpoint coordinator and helper thread, respectively. 2.6 Verification with TLA+/PlusCal To gain further confidence in our implementation for handling collective communication (Section 2.5), we developed a model for the protocol in TLA+ [25] and then used the PlusCal model checker of TLA+ based on TLC [38] to verify Algorithm 2. Specifically, PlusCal was used to verify the algorithm invariants of deadlock- free execution and consistent state when multiple concurrent MPI processes are executing. The PlusCal model checker did not report any deadlocks or broken invariants for our implementation. 2.7 Checkpoint/Restart Package Any single-process checkpointing package could be utilized for the basis of implementing MANA. This work presents a prototype implemented by extending DMTCP [1] and by developing a DMTCP plugin [2]. Cao et al. [9] demonstrated that DMTCP can checkpoint MPI-based HPCG over 32,752 CPU cores (38 TB) in 11 minutes, and MPI-based NAMD over 16,368 cores (10 TB) in 2.6 minutes. DMTCP uses a helper thread inside each application process, and a coordinated checkpointing protocol by using a centralized coor- dinator daemon. Since this was close to the design requirements of MANA, we leveraged this infrastructure and extended the DMTCP coordinator to implement the two-phase algorithm. The same approach could be extended to base MANA on top of a different underlying transparent checkpointing package. For example, one could equally well have modified an existing MPI co- ordinator process to communicate with a custom helper thread in each MPI rank that then invokes the BLCR checkpointing package when it is required to execute the checkpoint. In particular, all sock- ets and other network communication objects are inside the lower half, and so even a single-process or single-host checkpointing package such as BLCR would suffice for this work. 3 EXPERIMENTAL EVALUATION This section seeks to answer the following questions: Q1: What is the runtime overhead of running MPI applications under MANA? Q2: What are the checkpoint and restart overheads of transparent checkpointing of MPI applications under MANA? Q3: Can MANA allow transparent switching of MPI implementa- tions across checkpoint-restart for the purpose of load balancing? 3.1 Setup We first describe the hardware and software setup for MANA’s evaluation. 3.1.1 Hardware. The experiments were run on the Cori supercom- puter [13] at the National Energy Research Scientific Computing Center (NERSC). As of this writing, Cori is the #12 supercomputer in the Top-500 list [36]. All experiments used the Intel Haswell nodes (dual socket with a 16-core Xeon E5-2698 v3 each) connected via Cray’s Aries interconnect network. Checkpoints were saved to the backend Lustre filesystem. 3.1.2 Software. Cori provides modules for two implementations of MPI: Intel MPI and Cray MPICH. The Cray compiler (based on an Intel compiler) and Cray MPICH are the recommended way to use MPI, presumably for reasons of performance. Cray MPICH version 3.0 was used for the experiments. 3.1.3 Application Benchmarks. MANA was tested with five real- world HPC applications from different computational science do- mains: (1) GROMACS [4]: Versatile package for molecular dynamics, often used for biochemical molecules. (2) CLAMR [12, 29]: Mini-application for CelL-based Adaptive Mesh Refinement. 90 95 100 Normalized Performance (%) GROMACS miniFE HPCG CLAMR LULESH 1 2 4 8 16 32 0 1 2 4 8 16 32 1 2 4 8 16 32 # MPI Rank(s) (Single node) 1 2 4 8 16 32 1 9 27 Figure 2: Single Node: Runtime overhead under MANA for different real-world HPC benchmarks with an unpatched Linux kernel. (Higher is better.) 90 95 100 Normalized Performance (%) GROMACS miniFE HPCG CLAMR LULESH 2 4 8 16 32 64 0 2 4 8 16 32 64 2 4 8 16 32 64 # Compute Nodes (32 ranks/node, except LULESH) 2 4 8 16 32 64 2 4 8 16 32 64 Figure 3: Multiple Nodes: Runtime overhead under MANA for different real-world HPC benchmarks with an un- patched Linux kernel. In all cases, except LULESH, 32 MPI ranks were executed on each compute node. (Higher is bet- ter.) (3) miniFE [20]: Proxy application for unstructured implicit fi- nite element codes. (4) LULESH [24]: Unstructured Lagrangian Explicit Shock Hy- drodynamics (5) HPCG [14] (High Performance Conjugate Gradient): Uses a variety of linear algebra operations to match a broad set of important HPC applications, and used for ranking HPC systems. 3.2 Runtime Overhead 3.2.1 Real-world HPC Applications. Next, we evaluate the perfor- mance of MANA for real-world HPC applications. It will be shown that the runtime overhead is close to 0 % for miniFE and HPCG, and as much as 2 % for the other three applications. The higher overhead has been tracked down to an inefficiency in the Linux ker- nel [27] in the case of many point-to-point MPI calls (send/receive) with messages of small size. This worst case is analyzed further in Section 3.3, where tests with an optimized Linux kernel show a worst case runtime overhead of 0.6 %. The optimized Linux kernel is based on a patch under review for a future Linux version. Single Node: Since the tests were performed within a larger clus- ter where the network use of other jobs could create congestion, we first eliminate any network-related overhead by running the benchmarks on a single node with multiple MPI ranks, both under 0 1000000 2000000 3000000 4000000 Size (Bytes) 0 5000 10000 15000 20000 Bandwidth (MB/s) Without MANA With MANA (native kernel) With MANA (patched kernel) Figure 4: Point-to-Point Bandwidth under MANA with patched and unpatched Linux kernel. (Higher is better.) MANA and natively (without MANA). This experiment isolates the single-node runtime overhead of MANA by ensuring that all communication among ranks is intra-node. Figure 2 shows the results for the five different real-world HPC applications running on a single node under MANA. Each run was repeated 5 times (including the native runs), and the figure shows the mean of the 5 runs. The absolute runtimes varied from 4.5 min to 15 min, depending on the configuration. The worst case overhead incurred by MANA is 2.1 % in the case of GROMACS (with 16 MPI ranks). In most cases, the mean overhead is less than 2 %. Multiple Nodes: Next, the scaling of MANA across the network is examined for up to 64 compute nodes and with 32 ranks per node (except for LULESH, whose configuration restricts the number of ranks/node based on the number of nodes). Hence, the number of MPI ranks ranges from 64 to 2048. Figure 3 shows the results of five different real-world HPC ap- plications running on multiple nodes under MANA. Each run was repeated 5 times, and the mean of 5 runs is reported. We observe a trend similar to the single node case. MANA imposes an overhead of typically less than 2 %. The highest overhead observed is 4.5 % in the case of GROMACS (512 ranks running over 16 nodes). However, see Section 3.3 where we demonstrate a reduced overhead of 0.6 % with GROMACS. 3.2.2 Memory Overhead. The upper-half libraries were built with mpicc, and hence include additional copies of the MPI library that are not used. However, the upper-half MPI library is never ini- tialized, and so no network library is ever loaded into the upper half. Since a significant portion of the lower half is comprised only of the MPI library and its dependencies, the additional copy of the libraries (with one copy residing in the upper half) imposes a constant memory overhead. This text segment (code region) was 26 MB in all of our experiments on Cori with the Cray MPI library. In addition to the code, the libraries (for example, the networking driver library) in the lower half also allocate additional memory regions (shared memory regions, pinned memory regions, memory- mapped driver regions). We observed that the shared memory re- gions mapped by the network driver library grow in proportion with the number of nodes (up to 64 nodes): from 2 MB (for 2 nodes) to 40 MB for (64 nodes). We expect MANA to have a reduced check- point time compared to DMTCP/InfiniBand [10], as MANA discards these regions during checkpointing, reducing the amount of data that’s written out to the disk. 0 1000000 2000000 3000000 4000000 Size (Bytes) 0 50 100 150 200 250 300 Latency (µs) Without MANA With MANA (a) Point-to-Point Latency 0 200000 400000 600000 800000 1000000 Size (Bytes) 0 50 100 150 200 250 300 Latency (µs) Without MANA With MANA (b) Collective MPI_Gather 0 200000 400000 600000 800000 1000000 Size (Bytes) 0 100 200 300 400 500 Latency (µs) Without MANA With MANA (c) Collective MPI_Allreduce Figure 5: OSU Micro-benchmarks under MANA. (Results are for two MPI ranks on a single node.) 3.2.3 Microbenchmarks. To dig deeper into the sources for the run- time overhead, we tested MANA with the OSU micro-benchmarks. The benchmarks stress and evaluate the bandwidth and latency of different specific MPI subsystems. Our choice of the specific micro-benchmarks was motivated by the MPI calls commonly used by our real-world MPI applications. Figure 5 shows the results with three benchmarks from the OSU micro-benchmark suite. These benchmarks correspond with the most frequently used MPI subsystems in the set of real-world HPC applications. The benchmarks were run with 2 MPI ranks running on a single compute node. The results show that latency does not suffer under MANA, for both point-to-point and collective communication. (The latency curves for application running under MANA closely follow the curves when the application is run natively.) 3.3 Source of Overhead and Improved Overhead for Patched Linux Kernel All experiments in this section were performed on a single node of our local cluster, where it was possible to directly install a patched Linux kernel in the bare machine. Further investigation revealed two sources of runtime overhead. The larger source of overhead is due to the use of the “FS” register during transfer of flow of control between the upper and lower half and back during a call to the MPI library in the lower half. The “FS” register of the x86-64 CPU is used by most compilers to refer to the thread-local variables declared in the source code. The upper and lower half programs each have their own thread-local storage region. Hence, when switching between the upper and lower half programs, the value of the “FS” register must be changed to point to the correct thread-local region. Most Linux kernels today require a kernel call to invoke a privileged assembly instruction to get or set the “FS” register. In 2011, Intel Ivy Bridge CPUs introduced a new, unprivileged FSGSBASE assembly instruction for modifying the “FS” register, and a patch to the Linux kernel [27] is under review to allow other Linux programs to use this more efficient mechanism for managing the “FS” register. (Other architectures, such as ARM, use unprivileged addressing modes for thread-local variables that do not depend on special constructs, such as the x86 segments.) A second (albeit smaller) source of overhead is the virtualization of MPI communicators and datatypes, and recording of metadata for MPI sends and receives. Virtualization requires a hash table lookup and locks for thread safety. The first and larger source of overhead is then eliminated by using the patched Linux kernel, as discussed above. Point-to-point bandwidth benchmarks were run both with and without the patched Linux kernel (Figure 4). A degradation in runtime performance is seen for MANA for small message sizes (less than 1 MB) in the case of a native kernel. However, the figure shows that the patched kernel yields much reduced runtime overhead for MANA. Note that the Linux kernel community is actively reviewing this patch (currently in its third version), and it is likely to be incorporated in future Linux releases. Finally, we return to GROMACS, since it exhibited a higher runtime overhead (e.g., 2.1 % in the case of 16 ranks) in many cases. We did a similar experiment, running GROMACS with 16 MPI ranks on a single node with the patched kernel. With the patched kernel, the performance degradation was reduced to 0.6 %. 3.4 Checkpoint-restart Overhead In this section, we evaluate MANA’s performance when checkpoint- ing and restarting HPC applications. Figure 6 shows the checkpoint- ing overhead for five different real-world HPC applications running on multiple nodes under MANA. Each run was repeated 5 times, and the mean of five runs is reported. For each run, we use the fsync system call to ensure the data is flushed to the Lustre backend storage. The total checkpointing data written at each checkpoint varies from 5.9 GB (in the case of 64 ranks of GROMACS running over 2 nodes) to 4 TB (in the case of 2048 ranks of HPCG running over 64 nodes). Note that the checkpointing overhead is proportional to the total amount of memory used by the benchmark. This is also reflected in the size of the checkpoint image per MPI rank. While Figure 6 reports the overall checkpoint time, note that there is significant variation in the write times for each MPI rank during a given run. (The time for one rank to write its checkpoint data can be up to 4 times more than that for 90 % of the other ranks.) This phenomenon of stragglers during a parallel write has also been noted by other researchers [2, 37]. Thus, the overall checkpoint time is bottlenecked by the checkpoint time of the slowest rank. Next, we ask what are the sources of the checkpointing overhead? Does the draining of MPI messages and the two-phase algorithm impose a significant overhead at checkpoint time? 2 4 8 16 32 64 1 5 10 15 20 25 30 35 40 Checkpoint Time (s) (93 MB) (93 MB) (92 MB) (92 MB) (94 MB) (92 MB) GROMACS 2 4 8 16 32 64 (2.0 GB) (1.3 GB) (806 MB) (1.3 GB) (902 MB) (1.3 GB) miniFE 2 4 8 16 32 64 # Compute Nodes (32 ranks/node, except LULESH) (2.0 GB) (2.0 GB) (2.0 GB) (2.0 GB) (2.0 GB) (2.0 GB) HPCG 2 4 8 16 32 64 (656 MB) (594 MB) (552 MB) (501 MB) (594 MB) (552 MB) CLAMR 2 4 8 16 32 64 (276 MB) (164 MB) (114 MB) (91 MB) (85 MB) (88 MB) LULESH Figure 6: Checkpointing overhead and checkpoint image sizes under MANA for different real-world HPC bench- marks running on multiple nodes. In all cases, except LULESH, 32 MPI ranks were executed on each compute node. For LULESH, the total number of ranks was either 64 (for 2, 4, and 8 nodes), or 512 (for 16, 32, and 64 nodes). Hence, the maximum number of ranks (for 64 nodes) was 2048. The numbers above the bars (in parentheses) indicate the check- point image size for each MPI rank. 2 4 8 16 32 64 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Restart Time (s) GROMACS 2 4 8 16 32 64 miniFE 2 4 8 16 32 64 # Compute Nodes (32 ranks/node, except LULESH) HPCG 2 4 8 16 32 64 CLAMR 2 4 8 16 32 64 LULESH Figure 7: Restart overhead under MANA for different real- world HPC benchmarks running on multiple nodes. In all cases, except LULESH, 32 MPI ranks were executed on each compute node. Ranks/node is as in Figure 6. Figure 8 shows the contribution of different components to the checkpointing overhead for the case of 64 nodes for the five different benchmarks. In all cases, the communication overhead for handling MPI collectives in the two-phase algorithm of Section 2.5 is found to be less than 1.6 s. In all cases, the time to drain in-flight MPI messages was less than 0.7 s. The total checkpoint time was dominated by the time to write to the storage system. The next big source of checkpointing GROMACS miniFE HPCG CLAMR LULESH Benchmark (64 nodes; 32 ranks/node, except LULESH) 0 20 40 60 80 100 Contribution to Checkpoint Time (%s) Write Time Drain Time Comm. overhead Figure 8: Contribution of different factors to the checkpoint- ing overhead under MANA for different real-world HPC benchmarks running on 64 nodes. Ranks/node is as in Fig- ure 6. The “drain time” is the delay in starting a checkpoint while MPI message in transit are completed. The communi- cation overhead is the time required in the protocol for net- work communication between the checkpoint coordinator and each rank. overhead was the communication overhead. The current imple- mentation of the checkpointing protocol in DMTCP uses TCP/IP sockets for communication between the MPI ranks and the central- ized DMTCP coordinator. The communication overhead associated with the TCP layer is found to increase with the number of ranks, especially due to metadata in the case of small messages that are exchanged between MPI ranks and the coordinator. Finally, Figure 7 shows the restart overhead under MANA for the different MPI benchmarks. The restart time varies from less than 10 s to 68 s (for 2048 ranks of HPCG running over 64 nodes). The restart times increase in proportion to the total amount of checkpointing data that is read from the storage. In all the cases, the restart overhead is dominated by the time to read the data from the disk. The time to recreate the MPI opaque identifiers (see Section 2.2) is less than 10 % of the total restart time. 3.5 Transparent Switching of MPI libraries across Checkpoint-restart This section demonstrates that MANA can transparently switch between different MPI implementations across checkpoint-restart. This is useful for debugging programs (even the MPI library) as it allows a program to switch from a production version of an MPI library to a debug version of the MPI library. The GROMACS application is launched using the production version of CRAY MPI, and a checkpoint is taken 55 s into the run. The computation is then restarted on top of a custom-compiled debug version of MPICH (for MPICH version 3.3). MPICH was chosen because it is a reference implementation whose simplicity makes it easy to instrument for debugging. 3.6 Transparent Migration across Clusters Next, we consider cross-cluster migration for purposes of wide- area load balancing either among clusters at a single HPC site or even among multiple HPC sites. This is rarely done, since the two common vehicles for transparent checkpoint (BLCR as the base of 190 200 210 220 Runtime (s) Native Restarted (migrated from Cori) Open MPI/IB (2x4) MPICH/TCP (2x4) MPICH (8x1) Restart Configuration 0 Figure 9: Performance degradation of GROMACS after cross- cluster migration under three different restart configura- tions. The application was restarted after being check- pointed at the half-way mark on Cori. (Lower is better.) an MPI-specific checkpoint-restart service; or DMTCP/InfiniBand) both save the MPI library within the checkpoint image and continue to use that same MPI library on the remote cluster after migration. At each site and for each cluster, administrators typically configure and tune a locally recommended MPI implementation for perfor- mance. Migrating an MPI application along with its underlying MPI library destroys the benefit of this local performance tuning. This experiment showcases the benefits of MPI-agnostic, network- agnostic support for transparent checkpointing. GROMACS is run under MANA, initially running on Cori with a statically linked Cray MPI library running over the Cray Aries network. GROMACS on Cori is configured to run with 8 ranks over 4 nodes (2 ranks per node). Each GROMACS rank is single-threaded. A checkpoint was then taken exactly half way into the run. The checkpoints were then migrated to a local cluster that uses Open MPI over the InfiniBand network. The restarted GROMACS under MANA was compared with three other configurations: GROMACS using the local Open MPI, con- figured to use the local InfiniBand network (8 ranks over 2 nodes); GROMACS/MPICH, configured to use TCP (8 ranks over 2 nodes); and GROMACS/MPICH, running on a single node (8 ranks over 1 node). The network-agnostic nature of MANA allowed the Cori version of GROMACS to be restarted on the local cluster with any of three network options. We wished to isolate the effects due to MANA from the effects due to different compilers on Cori and the local cluster. In order to accomplish this, the native GROMACS on the local cluster was com- piled specially. The Cray compiler of Cori (using Intel’s C compiler) was used to generate object files (.o files) on Cori. Those object files were copied to the local cluster. The native GROMACS was then built using the local mpicc, but with the (.o files) as input instead of the (.c files). The local mpicc linked these files with the local MPI implementation, and the native application was then launched in the traditional way. Figure 9 shows that GROMACS’s performance degrades by less than 1.8% post restart on the local cluster for the three different restart configurations (compared to the corresponding native runs). Also, note that the performance of GROMACS under MANA post restart closely tracks the performance of the native configuration. 4 DISCUSSION AND FUTURE WORK Next, we discuss both the limitations and some future implications of this work concerning dynamic load balancing. 4.1 Limitations While the split-process approach for checkpointing and process migration is quite flexible, it does include some limitations inherited by any approach based on transparent checkpointing. Naturally, when restarting on a different architecture, the CPU instruction set must be compatible. In particular, on the x86 architecture, the MPI application code must be compiled to the oldest x86 sub-architecture among those remote clusters where one might consider restarting a checkpoint image. (However, the MPI libraries themselves may be fully optimized for the local architecture, since restarting on a remote cluster implies using a new lower half.) Similarly, while MPI implies a standard API, any local extensions to MPI must be avoided. The application binary interface (ABI) used by the compiled MPI application must either be compatible or else a “shim” layer of code must be inserted in the wrapper functions for calling from the upper half to the lower half. And of course, the use of a checkpoint coordinator implies coor- dinated checkpointing. If a single MPI rank crashes, MANA must restore the entire MPI computation from an earlier checkpoint. 4.2 Future Work MPI version 3 has added nonblocking collective communication calls (e.g., MPI_Igather). In future work, we propose to extend the two-phase algorithm for collective communication of Section 2.5 to the nonblocking case. The approach to be explored would be to employ a first phase that uses a nonblocking trivial barrier (MPI_Ibarrier), and to then convert the actual asynchronous col- lective call to a synchronous collective call (e.g., MPI_Gather to MPI_Igather) for the second phase. Nonblocking variations of col- lective communication calls are typically used as performance op- timizations in an MPI application. If an MPI rank reaches the col- lective communication early, then instead of blocking, it can con- tinue with an alternate compute task while occasionally testing (via MPI_Test/MPI_Wait) to see if the other ranks have all reached the barrier. In the two-phase analog, a wrapper around the nonblocking collective communication causes MPI_Ibarrier to be invoked. When the ranks have all reached the nonblocking trivial barrier and the MPI_Test/MPI_Wait calls of the original MPI application reports completion of the MPI_Ibarrier call of phase 1, then this implies that the ranks are all ready to enter the actual collective call of phase 2. A wrapper around MPI_Test/MPI_Wait can then invoke the actual collective call of phase 2. The split-process approach of MANA opens up some impor- tant new features in managing long-running MPI applications. An immediately obvious feature is the possibility of switching in the middle of a long run to a customized MPI implementation. Hence, one can dynamically substitute a customized MPI for performance analysis (e.g., using PMPI for profiling or tracing); or using a spe- cially compiled “debug” version of MPI to analyze a particular but occurring in the MPI library in the middle of a long run. This work also helps support many tools and proposals for opti- mizing MPI applications. For example, a trace analyzer is sometimes used to discover communication hotspots and opportunities for bet- ter load balancing. Such results are then fed back by re-configuring the binding of MPI ranks to specific hosts in order to better fit the underlying interconnect topology. MANA can enable new approaches to dynamically load balance across clusters and also to re-bind MPI ranks in the middle of a long run to create new configurations of rank-to-host bindings (new topology mappings). Currently, such bindings are chosen statically and used for the entire lifetime of the MPI application run. This added flexibility allows system managers to burst current long- running applications into the Cloud during periods of heavy usage or when the the MPI application enters a new phase for which a different rank-to-host binding is optimal. Finally, MANA can enable a new class of very long-running MPI applications — ones which may outlive the lifespan of the original MPI Implementation, cluster, or even the network interconnect. Such temporally complex computations might be discarded as in- feasible today without the ability to migrate MPI implementations or clusters. 5 RELATED WORK Hursey et al. [22] developed a semi-network-agnostic checkpoint service for Open-MPI. It applied an “MPI Message” abstraction to a Chandy/Lamport algorithm [11], greatly reducing the complexity to support checkpoint/restart for many multiple network intercon- nects. However, it also highlighted the weakness of implementing transparent checkpointing within the MPI library, since porting to an additional MPI implementation would likely require as much software development as for the first MPI implementation. Addi- tionally, its dependence on BLCR imposed a large overhead cost, as it lacks support for SysV shared memory. Separate proxy processes for high- and low-level operations have been proposed both by CRUM (for CUDA) and McKernel (for the Linux kernel). CRUM [16] showed that by running a non-reentrant library in a separate process, one can work around the problem of a library “polluting” the address space of the application process — i.e., creating and leaving side-effects in the application process’s address space. This decomposition of a single application process into two processes, however, forces the transfer of data between two processes via RPC, which can cause a large overhead. McKernel [17] runs a “lightweight” kernel along with a full- fledged Linux kernel. The HPC application runs on the lightweight kernel, which implements time-critical system calls. The rest of the functionality is offloaded to a proxy process running on the Linux kernel. The proxy process is mapped in the address space of the main application, similar to MANA’s concept of a lower half, to min- imize the overhead of “call forwarding” (argument marshalling/un- marshalling). In general, a proxy process approach is problematic for MPI, since it can lead to additional jitter as the operating system tries to schedule the extra proxy process alongside the application pro- cess. The jitter harms performance since the MPI computation is constrained to complete no faster than its slowest MPI rank. Process-in-process [21] has in common with MANA that both approaches load multiple programs into a single address space. However, the goal of process-in-process was intra-node communi- cation optimization, and not checkpoint-restart. Process-in-process loads all MPI ranks co-located on the same node as separate threads within a single process, but in different logical “namespaces”, in the sense of the dlmopen namespaces in Linux. It would be diffi- cult to adapt process-in-process for use in checkpoint-restart since that approach implies a single “ld.so” run-time linker library that managed all of the MPI ranks. In particular, difficulties occur when restarting with fresh MPI libraries while “ld.so” retains pointers to destructor functions in the pre-checkpoint MPI libraries. In the special regime of application-specific checkpointing for bulk synchronous MPI applications, Sultana et al. [33] supported checkpointing by separately saving and restoring MPI state (MPI identifiers such as communicators, and so on). This is combined with application-specific code to save the application state. Thus, when a live process fails, it is restored using these two components, without the need restart the overall MPI job. SCR [32], and FTI [3] are other application-specific checkpoint- ing techniques. An application developer declares memory regions they’d like to checkpoint and checkpointing can only be done at specific points in the program determined by the application devel- oper. Combining these techniques with transparent checkpointing is outside the scope of this work, though it is an interesting avenue for further inquiry. In general, application-specific and transparent checkpointing each have their merits. Both application-specific and transparent checkpointing are used in practice. At the high end of HPC, application-specific checkpointing is preferred since the labor for supporting this is small compared to the labor already invested in supporting an extreme HPC application. At the low and medium end of HPC, developers prefer trans- parent checkpointing because the development effort for the soft- ware is more moderate, and the labor overhead of a specialized application-specific checkpointing solution would then be signif- icant. System architectures based on burst buffers (e.g., Cray’s DataWarp [19]) can be used to reduce the checkpointing overhead for both application-specific and transparent checkpointing. 6 CONCLUSION This work presents an MPI-Agnostic, Network-Agnostic transpar- ent checkpointing methodology for MPI (MANA), based on a split- process mechanism. The runtime overhead is typically less than 2%, even in spite of the overhead incurred by the current Linux ker- nel when the “FS” register is modified each time control passes between upper and lower half. Further, Section 3.3 shows that a commit (patch) to fix this by the Linux kernel developers is un- der review and that this commit reduces the runtime overhead of GROMACS from 2.1% to 0.6% using the patched kernel. Similar reductions to about 0.6% runtime overhead are expected in the general case. An additional major novelty is the demonstration of practical, efficient migration between clusters at different sites using differ- ent networks and different configurations of CPU cores per node. This was considered impractical in the past because a checkpoint image from one cluster will not be tuned for optimal performance on the second cluster. Section 3.6 demonstrates that this is now feasible, and that the migration of a GROMACS job with 8 MPI ranks experiences an average runtime overhead of less than 1.8% as compared to the native GROMACS application (without MANA) on the remote cluster. As before, even this overhead of 1.8% is likely to be reduced to about 0.6% in the future, based on the results of Section 3.3 with a patched Linux kernel."
Formal Methods,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","September 10th, 2018",CRUM: Checkpoint-Restart Support for CUDA’s Unified Memory,http://www.ccs.neu.edu/home/gene/papers/cluster18.pdf," ""CRUM: Checkpoint-Restart Support for CUDA's Unified Memory"", Rohan Garg, Apoorve Mohan, Michael Sullivan, Gene Cooperman, Proc. of IEEE Int. Conf. on Cluster Computing</em> (Cluster'18), pp. 302--313, 2018","—Uniﬁed Virtual Memory (UVM) was recently intro- duced on recent NVIDIA GPUs. Through software and hardware support, UVM provides a coherent shared memory across the en- tire heterogeneous node, migrating data as appropriate. The older CUDA programming style is akin to older large-memory UNIX applications which used to directly load and unload memory segments. Newer CUDA programs have started taking advantage of UVM for the same reasons of superior programmability that UNIX applications long ago switched to assuming the presence of virtual memory. Therefore, checkpointing of UVM will become increasingly important, especially as NVIDIA CUDA continues to gain wider popularity: 87 of the top 500 supercomputers in the latest listings are GPU-accelerated, with a current trend of ten additional GPU-based supercomputers each year. A new scalable checkpointing mechanism, CRUM (Checkpoint-Restart for Uniﬁed Memory), is demonstrated for hybrid CUDA/MPI computations across multiple computer nodes. CRUM supports a fast, forked checkpointing, which mostly overlaps the CUDA computation with storage of the checkpoint image in stable storage. The runtime overhead of using CRUM is 6% on average, and the time for forked checkpointing is seen to be a factor of up to 40 times less than traditional, synchronous checkpointing. Index Terms—CUDA, GPU, UVM, checkpoint-restart, DMTCPII. BACKGROUND AND MOTIVATION A. History and Motivation for Uniﬁed Virtual Memory (UVM) Uniﬁed Virtual Memory (UVM) and its predecessor, Uniﬁed Virtual Addressing (UVA), are major CUDA features that are incompatible with prior CUDA checkpointing approaches. Yet, UVM is an important innovation for future CUDA applica- tions. Through software and hardware support, UVM provides a coherent shared memory across the entire heterogeneous node [14], [15]. The use of UVM-managed memory greatly simpliﬁes data sharing and movement among multiple GPUs. This is especially useful given that the most energy-efﬁcient supercomputers place multiple compute accelerators per node—for instance, TSUBAME3.0 [16], Coral Summit [17], and the NVIDIA SATURNV [18] supercomputer use 4, 6, and 8 GPUs per node, respectively. The features and progression of UVM are brieﬂy described below. UVA Cuda 4 Fermi GPUs UVM-Lite Cuda 6 Kepler GPUs UVM-Full Cuda 8 Pascal GPUs 2011 2013 2016 Fig. 2: The technology advancement of CUDA uniﬁed virtual memory. Historically, in CUDA 4 (2011), Fermi-class GPUs added support for Uniﬁed Virtual Addressing (UVA) with zero-copy memory. UVA allows transparent zero-copy accesses to mem- ory across a heterogeneous node using a partitioned address space. UVA never migrates data, and so non-local memory accesses suffer from less bandwidth and longer latency. To reduce the performance penalty of non-local zero-copy memory accesses, ﬁrst-generation Uniﬁed Virtual Memory (UVM-Lite) was introduced in CUDA 6 (2013) for Kepler- class GPUs [19]. UVM-Lite shares a single memory space across a heterogeneous node, and it transparently migrates all memory pages that are attached to the CUDA streams associ- ated with each kernel. This simpliﬁes deep copies with pointer- based structures and it allows GPUs to transparently migrate UVM-managed memory to the device, nearly achieving the performance of CUDA programs using explicit memory man- agement. Due to hardware restrictions, however, UVM-Lite does not allow concurrent access to the same memory from both CPU and GPU—host-side access is only allowed once all GPU-side accesses to a CUDA stream have completed. Concurrent access to UVM-managed memory from different GPUs is allowed, but data are never migrated between devices and non-local memory is accessed in a zero-copy fashion. Second-generation UVM (UVM-Full) was introduced in CUDA 8 (2016) for Pascal-class GPUs [20]. It eliminates the concurrent-access constraints of the prior UVM generation and adds support for system-wide atomic memory operations, providing an unrestricted coherent shared memory across the heterogeneous node. On-demand data migration is supported by UVM-Full across all CPUs and GPUs in a node, with the placement of any piece of data being determined by a variety of heuristics [15]. Pascal-era UVM also adds support for memory over- subscription, meaning that UVM-managed regions that are larger than the GPU device memory can be accessed without explicit data movement. This is important for applications with large data. In particular, it greatly simpliﬁes the programming of large-memory jobs, and avoids the need to explicitly marshal data to and from the GPU [21]. For instance, GPU- capacity-exceeding deep neural network training has been accomplished in the past through explicit data movement [22], but it can also be performed with less programmer effort by UVM over-subscription [23]. B. GPUs for Exascale: DUEs and GPU Reliability The advantages of using GPUs for high-performance com- puting have been realized and a steep rise in their use in large- scale HPC systems has been observed (see Figure 1). Eighty- seven (87) systems in the Top500 list were reported to be powered by NVIDIA GPUs in November 2017, as compared to one (1) in November 2009 [1]. Thus, it is important that both hardware and the software stack (pertaining to the use of GPUs) should be highly available and reliable to maximize large-scale HPC systems productivity. While this makes GPUs attractive for exascale computing, the high GPU detectable-uncorrectable error rate (as compared to CPUs) remains an issue. Checkpointing plays an important role in mediating this issue. Various studies have been con- ducted for understanding the reliability aspects of using GPU’s in large-scale HPC systems. The studies suggest that the newer generation GPU’s are more reliable, as are the large-scale HPC systems using them (i.e., the observed MTBF of systems using newer GPU’s is much longer than their estimated MTBF) [2]– [7]. However, one factor that motivates efﬁcient checkpoint- restart on GPU accelerated systems is that GPU memory currently tends to have more DUEs (Detected Unrecoverable Errors) per GB than CPU memory. Memory in CPU nodes is composed of narrow 4-bit or 8-bit wide DRAM devices that are grouped together into DIMMs, meaning certain ECC codes (often called chipkill ECC) can correct the data that comes from an entire DRAM device. In contrast, GPU memory is much wider (32-bit wide for GDDR5/GDDR5X and 128-bit for HBM2) such that chipkill-level protection is not possi- ble without a prohibitively large memory access granularity; accordingly, current GPUs use single-bit correcting SEC-DED ECC for DRAM [24], [25]. These lesser correction capabilities lead to a relative increase in detected errors. For example, a ﬁeld study of the Blue Waters system [26] found that the DUE rate per GB of Kepler-era GDDR5 was roughly 5 times that of the chipkill-protected CPU memory. Given the high rate of DUEs expected in the future exascale systems, checkpoints will be more frequent, and so it is im- perative to design checkpointing mechanisms that can reduce the time that applications spend in checkpointing. C. Checkpointing Large-memory CUDA-UVM Applications UVM acts as an enabler for easily developing large-memory CUDA applications. UVM enables a GPU to transparently access host CPU and remote GPU memory, and hence solves the problem of otherwise manually managing data transfers. All of the host CPU’s memory is available, on-demand, by the GPU device. Conversely, all of the UVM memory on the GPU device is available to the CPU. In this situation, the CUDA application may use much more memory than is present on the device. The capacity of GPU memory is currently from 16 to 32 GB for a high- end GPU, while CPU memory often ranges from 128 to 256 GB. In the past, this forced GPU application developer to choose between: scaling out to many nodes and GPUs (hence incurring communication overhead); or manually managing the data transfers on a single GPU. Later, UVM made possible a third choice: transparently transferring data on a single GPU via UVM. However, the ease of developing such large- memory CUDA-UVM applications now places a larger burden on transparent checkpointing to support this large-memory overhead. III. CRUM: DESIGN AND IMPLEMENTATION To address the challenges described in Section II, this paper proposes CRUM, a novel framework that provides a checkpointing-based fault-tolerance mechanism. CRUM en- ables transparent, system-level checkpointing for CUDA and CUDA UVM applications. Figure 3 shows a high-level schematic of CRUM’s archi- tecture. Note especially the organization into two processes: a CUDA program (the user’s application), and a CUDA proxy (the only process that uses the CUDA library to communicate with the GPU). The ﬂow of control is: (i) to interpose on CUDA library calls made by the application process; (ii) to forward the requests to the proxy process; (iii) which then executes the calls via its CUDA library and GPU, on behalf of the application; and (iv) ﬁnally returns the results back to the application. (a) CUDA Original (b) CUDA Proxy Fig. 3: High-level architecture of CRUM In this section, we present the key subsystems in the design of CRUM. The ﬁrst research challenge is the propagation of UVM memory pages (already shared between GPU hardware and proxy process) to make them visible to the application process. Section III-B describes a shadow page scheme (sum- marized in Algorithm 1) for this purpose. The second research challenge is to extend this scheme to overlap checkpointing and computation for the sake of fast, forked checkpoint and future exascale needs. This is discussed in Section III-C. Finally, the implementation details of integrating CRUM with proxy processes is discussed in III-D. A. Post-CUDA 4: The Need for a Proxy Process Ideally, a single-process approach toward checkpointing seems simpler. But this approach for CUDA became non- viable with CUDA 4 and beyond, when NVIDIA implemented uniﬁed virtual addressing with zero-copy, an antecedent of uniﬁed memory [23]). At that point, it was no longer possible to re-initialize the CUDA library at the time of restart. We assume that this is due to the lack of clear semantics about what it means to re-initialize a CUDA library that still retains pointers to uniﬁed memory regions on host and device. One must choose either to free the host memory (thus sabotaging any CUDA application that retains a pointer to the uniﬁed memory region), or else to leave the host memory region intact (thus sabotaging any application assumptions about uniﬁcation of host and device memory). Note that a fresh restart will restore all host memory, but any uniﬁcation of host with device memory has already been lost. The core issue is that the CUDA uniﬁed memory model was developed for standard CUDA applications — and naturally did not include extensions for transparent checkpointing. An alternative workaround would have been, at restart time, to overwrite the text and data memory segments of any CUDA libraries with a fresh, uninitialized CUDA library (matching a freshly booted GPU), and then to call cudaInit(). Unfor- tunately, the CUDA library/driver appeared to have additional state, which made this workaround infeasible. B. Shadow Pages for the Support of UVM Recall the use of a proxy process, as seen in Figure 3(b). The core research challenge in this architecture is that UVM dictates that pages are transparently shared between the GPU hardware and the proxy process, but these shared UVM pages are not visible to the application process. The zero-copy memory of CUDA 4 implies that there are no CUDA calls on which to interpose. In direct-mapped memory, the device may read or write to the host mapped pinned memory of the proxy process at any time. But the separate ap- plication process remains unaware of modiﬁcations to memory in the proxy process. Thus, an approach using CUDA proxies is unable to support the newer and potentially more efﬁcient zero-copy memory for UVA. To overcome this situation, a new, transparent checkpointing approach for CUDA’s zero- copy memory is proposed, in which proxy and application reﬂect a single application with two “personalities”. The CUDA application process and the CUDA proxy pro- cess invoke the same application binary but execute two differ- ent state machines. The application process goes through three different states: CUDA call, read from device-mapped UVM memory, write to device-mapped UVM memory. Note that the state transitions are not dictated by the CRUM framework, but rather by the application logic. On the other hand, the CUDA proxy process is simply a passive listener for requests from the application process and executes the CUDA calls and the memory reads and writes as dictated by the application. Based on these observations, we introduce the concept of “shadow UVM pages”. For every CUDA UVM allocation request by the application, CRUM creates a corresponding shadow UVM region in the context of the application process. At the same time, the CUDA proxy process requests for a “real” UVM region from the device driver. The two processes, the application and the proxy, see two different views of the memory and data at any given point. Since there are no API calls to interpose on, this opens up the requirement for tracking the changes to the application process’s memory in order to keep the two sets of pages in sync. CRUM relies on the use of user-space page-fault tracking to accomplish this. There are currently two available mechanisms for page-fault tracking in Linux: userfaultfd; and segfault handler and page protection bits. While there are certain performance beneﬁts with the use of userfaultfd, the current work uses segfault handler and page protection bits to allow for evaluation on clusters employing older Linux kernels. The algorithm for synchronizing the data on shadow and real UVM pages is described in Algorithm 1. Algorithm 1 Shadow page synchronization algorithm upon event Page Fault do if addr ∈AllShadowPages then if isReadFault() then ReadDataFromRealPage() else MarkPageAsDirty() end if end if upon event CUDA call do if hasDirtyPages then SendDataToRealPages() ClearDirtyPages() end if upon event CUDA Create UVM region do uvmAddr ←CreateUvmRegionOnProxy() reg ←CreateShadowPage(uvmAddr) AllShadowPages ←AllShadowPages ∪reg When an application process requests for a new UVM region, a new shadow UVM region is created in the process’s memory (using the mmap system call). The shadow UVM region is given read-write permissions initially, and all the pages in the regions are marked as “dirty”. When the application makes a CUDA call where the device could potentially read or modify the UVM data (for example, a CUDA kernel launch), the data from dirty pages is “ﬂushed” to the real UVM pages on the proxy process, the dirty ﬂag is cleared for the UVM region, and the read-write permissions are removed (using the mprotect system call). This allows CRUM to interpose on either a read or write to uniﬁed memory. Standard Linux code for segfault handlers allows CRUM to detect an attempt to read or to write, and to distinguish the two cases. In the case of a read, PROT READ permission is set for all of the memory in the application process corresponding to uniﬁed memory. In the case of a write, PROT WRITE permission is set for all of the memory in the application process corresponding to uniﬁed memory. (See Section III-B1 for further discussion.) At a later time, when the application process tries to read the results of the GPU computation back from the shadow UVM regions, a read page fault is triggered; the permissions of the shadow UVM region are changed to read-only, and the results are read in from the corresponding real UVM region on the proxy. 1) Page permissions on Linux: Note that write to shadow UVM memory region requires PROT WRITE permission. Unfortunately, on Linux, PROT WRITE permission implies PROT READ permission also. Linux does not support write- only permission, but rather read-write permission instead. This has consequences for the three-state algorithm to support uniﬁed memory in CRUM. We make the assumption here that most applications will cycle through the three states in order (possibly omitting the read-only or write-only phase). Hence, a typical cycle would be invoked: CUDA-call/read- uniﬁed-memory/write-uniﬁed-memory. In fact, CRUM also supports overlapped execution of a CUDA call with reading and writing uniﬁed memory. The essential assumption is that read access must precede write access and a read-write cycle cannot be followed with a second read unless there is an intervening CUDA kernel. Normal CUDA calls such as cudaMemcpy are allowed at all times. As discussed earlier, unfortunately, Linux’s write-only per- mission for memory actually grants read-write permission. It is for this reason that a transition from write-uniﬁed-memory directly to read-uniﬁed-memory cannot be detected efﬁciently. Possible solutions are discussed at the end of this section. This assumption has been found to hold in each of the real- world applications that we have found for testing CRUM with uniﬁed memory. Nevertheless, it is important to also build in a (possibly slower) veriﬁed execution mode that will test an application to see if it violates this assumed cycle of CUDA- call/read-uniﬁed-memory/write-uniﬁed-memory. There is more than one way to implement a veriﬁed execu- tion mode. One of the difﬁculties is that a Linux segfault handler does not allow us to reset the page permission to allow only the pending read or write, and then reset the permission back to PROT NONE. Linux’s user-space page fault handling, userfaultfd, introduced with Linux 4.3, can ﬁx this, but that introduces other technical difﬁculties. (For example, it was only with Linux 4.11 that this was extended partially to support fork and shared memory.) Another alternative is to parse the pending read or write (load or store assembly instruction), temporarily allow read-write permission to the desired memory page, and then use the parsed information to read or write the data between register and memory, and ﬁnally to restore the previous memory permission. This might be more efﬁcient than user-space page faulting since it might have fewer transitions to a kernel system call. Linux kernel modiﬁcation to support write-only permissions for UVM shadow pages is another possibility. C. Fast, Forked Checkpoints UVM enables CUDA applications to use all of the host and GPU device memory transparently. This can make check- pointing, which is dominated by the time to write to the disk, prohibitively expensive. So while one could employ copy- on-write-based asynchronous checkpointing, UVM memory is incompatible with shared memory and fork on Linux. Fortunately, CRUM’s proxy-based architecture can be used to address this challenge. Note that the device state and the UVM memory regions are not directly a part of the application process’s context, but rather they are associated with the proxy process. This frees up the application process to use forked checkpointing for copy-on-write-based associated checkpointing for the application process. Forked checkpointing allows CRUM to invoke a minimal checkpointing delay in order to “drain the GPU device” of its data, after which, a child process of each MPI process can write to stable storage. This allows the system to overlap the CUDA computation with storage of the checkpoint image in stable storage. D. Checkpoint-Restart Methodology and Integration with Proxies Finally, for completeness, we discuss how CRUM integrates proxy concepts into the CUDA implementation requirements. Proxies have also been used by previous authors (see Sec- tion VI-d). At checkpoint time, CRUM suspends the user application threads, and “drains” the GPU kernel queue. It issues a device synchronize call (cudaDeviceSynchronize) to ensure that the kernels have ﬁnished execution and the memory state is consistent. Then, for all the active CUDA-MALLOC and CUDA-UVM memory regions, data is read in from the GPU to the host. The data is ﬁrst transferred from the GPU into the proxy process’s memory, and then from the memory of the proxy process into the memory of the user application process. The user application process then disconnects from the proxy process. This ensures that the problem reduces to the trivial problem of checkpointing a single-process application. Finally, the state of the process is saved to a checkpoint image ﬁle on stable storage. At the time of restart, CRUM starts a new process and recreates the user application threads. Then, the memory of the new process gets replaced by the saved state from the checkpoint image ﬁle. CRUM, then, starts a new proxy process, which starts a new GPU context. It recreates the active CUDA-MALLOC and CUDA-UVM memory regions by replaying the allocation calls. CUDA streams and events are similarly handled. (See Section V for further discussion.) Finally, CRUM transfers the data into the actual CUDA and CUDA-UVM regions through the proxy process and resumes the application threads. IV. EXPERIMENTAL EVALUATION The goal of this section is to present a detailed analysis of the performance of CRUM. In particular, this section answers the following questions: Q1 What’s the overhead of running a CUDA (or a CUDA UVM) application under CRUM? Q2 Does CRUM provide the ability to checkpoint CUDA (and CUDA UVM) applications? Q3 Can CRUM improve a CUDA UVM based application’s throughput by reducing the checkpointing overhead? Q4 Is the approach scalable? A. Setup To answer the above questions, we ﬁrst brieﬂy describe our experimental setup and methodology. 1) Hardware: The experiments were run on a local cluster with 4 nodes. Each node is equipped with 4 NVIDIA PCIe- attached Tesla P100 GPU devices, each with 16 GB of RAM. The host machine is running a 16-core Intel Xeon E5-2698 v3 (2.30 GHz) processor with 256 GB of RAM. Each node runs CentOS-7.3 with Linux kernel version 3.10. 2) Software: Each GPU runs NVIDIA CUDA version 8.0.44 with driver 396.26. Experiments use DMTCP [27] ver- sion 3.0. We developed a CRUM-speciﬁc DMTCP plugin [28] for checkpoint-restart of NVIDIA CUDA UVM applications. The DMTCP CRUM plugin (referred to as the CRUM plugin from here onwards) interposes on the CUDA calls made by the application. The interposition code is generated in a semi-automated way, where a user speciﬁes the prototype of a CUDA function, and whether the call needs to be logged. This not only allows us to cover the extensive CUDA API, but also allows for ease of maintainability and for future CUDA extensions. The plugin forwards the requests, over a SysV shared memory region, to a proxy process running on the same node. The forwarded request is then executed by the proxy process, which then returns the results back to the appli- cation. To improve the performance, we use well-studied concepts from pipelining of requests, to allow the application to send requests without blocking. Blocking requests, such as, cudaDeviceSynchronize, result in a pipeline ﬂush. For data transfers (both for UVM shadow page data and for cudaMalloc data) we use Linux’s Cross Memory Attach (CMA) to allow for data transfers using a single copy opera- tion. 3) Application Benchmarks: We use Rodinia 3.1 [29] benchmarks for evaluating CRUM for CUDA applications. Note that the Rodinia benchmarks do not use UVM, and can be run even with CUDA 2.x. They are included here to show comparability of the new approach with the older work from 2011 and earlier using CUDA 2.x [10], [12], [30]. We note that CheCUDA [10] does not work for modern CUDA (i.e., CUDA version 4 and above) because it relies on a single-process checkpoint-restart approach. CheCL [30] only supports OpenCL and does not work with CUDA. We tried compiling the CRCUDA [13] version available online [31], but it failed to compile with CUDA version 8. It didn’t work for the benchmarks used in our experiments, after applying our compilation ﬁxes. To evaluate CRUM using UVM-managed memory alloca- tion, we run a GPU-accelerated build of two DOE benchmarks: a high-performance geometric multigrid proxy application (HPGMG-FV [32]), and a test application using a production linear system solver library (HYPRE [33]). For the HYPRE library, we run the test driver for an unstructured matrix interface using the AMG-PCG solver. For HPGMG-FV, we evaluate two versions: the standard HPGMG-FV benchmark with one grid (the master branch, as described in [34]), and an AMR proxy modiﬁcation with multiple AMR levels (the amr proxy branch, as described in [21]). We focus on HPGMG-FV and HYPRE because they are scientiﬁc applications and libraries with potential importance in future exascale computing [35], and they have publicly available ports to UVM-enabled multi-GPU CUDA. HPGMG- FV has also been used as a benchmark for ranking the speeds of the top supercomputers [36]. To evaluate the relative performance of HPGMG-FV runs, we quote its throughput in degrees-of-freedom per second — the same metric used to rank supercomputer speeds [36]. Thus, larger numbers indicate higher performance. To evaluate the relative performance of HYPRE runs, we measure the wall clock time taken by each program execution. B. Runtime Overhead While the ability to checkpoint is important for improving the throughput of an application on a system with frequent failures, a checkpointing system that imposes excessive run- time overhead can render the framework ineffective, and in the worst case, reduce the throughput. We, therefore, bench- mark and analyze the sources of runtime overhead. For these experiments, no checkpoint or restart was invoked during the run of the application. The results demonstrate that CRUM is able to run the CUDA application with a worst case overhead of 12%, and a 6% overhead on average. We note that this is a prototype implementation and a production system could incorporate many optimizations to further reduce the overhead. TABLE I: Runtime parameters for Rodinia applications. Application Conﬁguration Parameter LUD “-s 2048 -v” Hotspot3D “512 8 1000 power 512x8 temp 512x8” Gaussian “-s 8192” LavaMD “-boxes1d 40” Figure 4(a) shows the runtimes for four applications (LUD, Hotspot3D, Gaussian, and LavaMD) from the Rodinia benchmark suite with and without CRUM. The applications mostly use the CUDA API’s from CUDA 2.x: cudaMalloc, cudaMemcpy, and cudaLaunch. Table I shows the conﬁg- uration parameters used for the experiments. We observe that the runtime overhead varies from 1% (for LUD) to 3% (in the case of LavaMD). The runtime overhead is dominated by the LUD Hotspot3D Gaussian LavaMD 0 20 40 60 80 Runtime (s) Native With CRUM (a) Rodinia Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 2 4 6 8 10 12 14 DOF/s Overhead (%) Level-1 Level-2 Level-3 (b) HPGMG-FV Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 400 500 600 700 800 900 Runtime (s) Native With CRUM (c) HYPRE Benchmark. Fig. 4: Runtime overheads for different benchmarks under CRUM. cost of data transfers from the application process to the proxy process. In a different experiment, using Unix domain sockets for data transfer, we observed overheads varying from 1.5% to 16.5%. The use of CMA reduces the overhead signiﬁcantly. Figure 4(b) shows the runtime results for the HPGMG- FV benchmark with increasing number of nodes and MPI ranks. As noted in Section IV-A3, we use the HPGMG-FV throughput metric DOF/s as a proxy for performance. We note that the DOF/s reported by the application running under CRUM are less than the native numbers by 6% to 12%. We present a more in-depth analysis below. In our experiments, we observed that a single MPI rank of the HPGMG-FV benchmark runs about 9 million CUDA kernels during its runtime of 3 minutes. This implies that each CUDA kernel runs for approximately 20 microseconds on average. Note that the cost of executing a cudaLaunch call itself can be up to 5 microseconds. The program allocates many CUDA UVM regions, sets up the data, and runs a series of kernels to operate on the data. Each MPI rank then exchanges the results with its neighbors. While the size of the UVM regions vary from 12 KB to 128 KB, the frequent reads and writes the application process, stresses the CRUM framework in two dimensions: (a) frequent interrupts and data transfer; and (b) frequent context switches and the need to synchronize with proxy process (because of the many CUDA calls that need to be executed). While the use of CMA (cross-memory attach) reduces the cost of data transfers, interestingly, we observed a lot of variability in the cost of a single CMA operation for the same data transfer size. The cost of a single page transfer varies from 1 microsecond to 1 millisecond, a difference of three orders of magnitude. We attribute this to two sources: (a) O/S jitter; (b) the pre-fetching algorithm employed by the UVM driver. In many cases, reading a UVM page is slowed down because of a previous read on a large UVM region, spanning several pages, because the driver gets busy pre-fetching the data for the large UVM region. To address the second source of overhead, we optimized the CRUM implementation to: (a) use a lock-free, inter- process synchronization mechanism over shared-memory; and (b) pipeline non-blocking CUDA calls from the application. A CUDA call, such has cudaLaunch, cudaMemsetAsync, is pipelined and the application is allowed to move ahead in its execution, while the proxy ﬁnishes servicing the request. At a synchronization point, like cudaDeviceSynchronize, the application must wait for a pipeline ﬂush, i.e., for the pending requests to be completed. Figure 4(c) shows the runtimes for the HYPRE benchmark for a different number of MPI ranks running on a varying num- ber of nodes. The benchmark observes up to 6.6% overhead when running under CRUM compared to native execution. The HYPRE benchmark presents different checkpointing challenges than HPGMG-FV. While the HYPRE benchmark invokes only about 100 CUDA kernels per second (10 millisec- onds on average per kernel) during its execution, it uses many large UVM regions (up to 900 MB). Thus, the overhead is dominated by the cost of data transfers between the application process and the proxy. In addition to CMA, CRUM employs a simple heuristic to help reduce the data transfer overhead. For small shadow UVM regions, it reads in all of the data from the real UVM pages on the proxy. However, for a read fault on a large shadow UVM region, it starts off by only reading the data for just one page containing the faulting address. On subsequent read faults on the same region, while in the read phase (see Section III), we exponentially increase (by powers of 2) the number of pages read in from the real UVM region on the proxy. This heuristic relies on the spatial and temporal locality of accesses. While there will be pathological cases where an application does “seemingly” random reads from different UVM regions, we have found this assumption to be valid in the two applications we tested. C. Checkpointing CUDA Applications: Rodinia and MPI Next, we evaluate the ability of CRUM to provide fault tolerance for CUDA and CUDA UVM applications using checkpoint-restart. Figure 5(a) shows the checkpoint times, restart times, and the checkpoint image sizes for the four applications from the Rodinia benchmark suite. The checkpointing overhead is dependent on the time to transfer the data from the device memory to the host memory, then transferring it from the LUD Hotspot3D Gaussian LavaMD 0 1 2 3 4 5 6 7 8 Time (s) 101MB 84MB 542MB 1.1GB Checkpoint Restart (a) Rodinia Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 0 1 2 3 4 5 6 7 8 Time (s) 113MB 113MB 113MB Checkpoint Restart (b) HPGMG-FV Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 0 5 10 15 20 25 30 Time (s) 3.5GB 1.6GB 790MB Checkpoint Restart (c) HYPRE Benchmark. Fig. 5: Checkpoint-restart times and checkpoint image sizes for different benchmarks under CRUM. proxy process to the application process using CMA, and then ﬁnally writing to the disk. We observe that the time to write dominates the checkpointing time. Figure 5(b) shows the checkpoint times, restart times, and the checkpoint image sizes for HPGMG. The results are shown with increasing number of MPI ranks (and the number the nodes). We observe that as the total amount of checkpointing data increases from 904 MB (8 × 113 MB) to 3.6 GB (32 × 113 MB), the checkpoint time increases from 3 seconds to 8 seconds. We attribute the small checkpoint times to the buffer cache on Linux. We observed that forcing the ﬁles to be synced (by using an explicit call to fsync increased the checkpoint times by up to 3 times. The results for HYPRE are shown in Figure 5(c). The application divides a ﬁxed amount of data (approx. 28 GB in total) equally among its ranks. So, we observe that the checkpoint image size reduces by almost half every time we double the number of ranks. This helps improve the checkpoint cost especially with smaller process sizes, as the Linux buffer caches the writes, and the checkpoint times reduce from 31 seconds (for 8 ranks on 1 node) to 8 seconds (for 32 ranks over 4 nodes). D. Reducing the Checkpointing Overhead: A Synthetic Bench- mark for a Single GPU To showcase the beneﬁts of using CRUM to reduce check- pointing overhead for CUDA UVM applications, we develop a CUDA UVM synthetic benchmark. The synthetic benchmark allocates two vectors of 232 4-byte ﬂoating point numbers (32 GB in total) and computes the dot product of the two vectors. The ﬂoating point numbers are generated at random. Note that the total memory requirements are double of what is available on the GPU device (16 GB). However, UVM allows an application to use more than the available memory on the GPU device. The host memory, in this case, acts as “swap storage” for the device and the pages are migrated to the device or to the host on demand. Table II shows the checkpoint times for three different cases: (a) using a na¨ıve checkpointing approach; (b) using three different compression schemes, Gzip, Parallel Gzip, and LZ4, before writing to the disk; and (c) using CRUM’s forked TABLE II: Checkpoint times using different strategies for the synthetic benchmark. Strategy Ckpt Time Ckpt Size Data Migration Time Na¨ıve 45 s 33 GB (100% random) 4 s Gzip 1296 s 29 GB (100% random) 4 s Parallel gzip 86 s 29 GB (100% random) 4 s LZ4 62 s 33 GB (100% random) 4 s Forked Ckpting 4.1 s 32 GB (100% random) 4 s Gzip 749 s 15 GB (50% random) 4 s Parallel gzip 56 s 15 GB (50% random) 4 s LZ4 45 s 17 GB (50% random) 4 s checkpointing approach. The ﬁrst two approaches, na¨ıve and compression, use CRUM’s CUDA UVM checkpointing frame- work. The third approach adds the forked checkpointing opti- mization to the base CUDA UVM checkpointing framework. The three compression schemes use Gzip’s lowest compression level (-1 ﬂag). While parallel Gzip uses the same compression algorithm as Gzip, it launches as many threads as the number of cores on a node to compress input data. We observe that the forked checkpointing approach out- performs the other two approaches by up to three orders of magnitude. Since the program uses random ﬂoating point numbers, compression is ineffective at reducing the size of the checkpointing data (Table II). We note that the time taken by the compression algorithm is also correlated with the ran- domness of data. As an experiment, we introduced redundancy in the two input vectors to improve the “compressibility”. Of the 232 ﬂoating point elements in a vector, only half (216) of the elements were generated randomly and the rest were assigned the same ﬂoating point number. This improves the compression time and reduces the checkpoint time to 749 seconds and the checkpoint image size is reduced to 15 GB by using the Gzip-based strategy. Note that parallel Gzip may not be a practical option in many HPC scenarios, where an application often uses one MPI rank per core on a node. On the other hand, LZ4 provides a computationally fast compression algorithm at the cost of a lower compression ratio. E. Reducing the Checkpoint Overhead: Real-world MPI Ap- plications Finally, we present the results from using CRUM with the forked checkpointing optimization for the real-world CUDA UVM application benchmarks. The results reported here cor- respond to the largest scale of 4 CPU nodes, with 16 GPU devices, running 8 MPI ranks per node (32 processes in total). TABLE III: Checkpoint times using different strategies for real-world CUDA UVM applications. The numbers reported corresponds to running 32 MPI ranks over 4 nodes. The checkpoint size reported is for each MPI rank. The checkpoint times are normalized to the time for the na¨ıve checkpointing approach (1x). App. Strategy Ckpt Time Ckpt Size HPGMG-FV Gzip 0.78x 14 MB HPGMG-FV Parallel gzip 0.60x 14 MB HPGMG-FV LZ4 0.30x 16 MB HPGMG-FV Forked ckpting 0.025x 113 MB HYPRE Gzip 2x 176 MB HYPRE Parallel gzip 1x 176 MB HYPRE LZ4 1x 296 MB HYPRE Forked ckpting 0.032x 868 MB Table III shows the results for checkpointing time (and checkpoint image sizes) normalized to the checkpointing time using the na¨ıve checkpointing approach (as shown in Fig- ures 5(b) and 5(c)). The results are shown for HPGMG-FV and HYPRE. We observe trends similar to the synthetic benchmark case. While in the na¨ıve checkpointing approach, the checkpointing overhead is dominated by the cost of I/O, i.e., writing the data to the disk, under forked checkpointing, the overhead is dominated by the cost of in-memory data transfers: from the GPU to the proxy process, and from the proxy process’s address space to the application process’s address space. Fur- ther, the cost of quiescing the application process, quiescing the network (for MPI), and “draining” and saving the in-ﬂight network messages is 0.01% of the total cost. However, unlike the synthetic benchmark, using in-memory compression to reduce the size of data for writing is better in this case for both HPGMG and HYPRE. This indicates that the compression algorithm is able to efﬁciently reduce the size of the data, which helps lower the I/O overhead. Note that this is still worse than using forked checkpointing by an order of magnitude. V. DISCUSSION Driver support for restart: In order to restart a computa- tion, CRUM must re-allocate memory in the same locations as during the original execution—otherwise the correctness of pointer-based code cannot be guaranteed during re-execution. The current CRUM prototype relies on deterministic CUDA memory allocation, which we verify to work with the CUDA driver libraries via experimentation (for both explicit device memory and UVM-managed memory allocation). The as- sumption of deterministic memory re-allocation is shared by previous GPU checkpointing efforts [12]. Memory Overhead: In a CUDA program with large data resident on the host, the memory overhead due to an addi- tional proxy process could be a concern. In the special case of asynchronous checkpointing, the overhead could be even higher, although copy-on-write does prevent it from going too high. This could be ameliorated by future support for shared memory UVM pages between application and a proxy running CUDA. Advanced CUDA language features: Dynamic parallelism allows CUDA kernels to recurse and launch nested work; it is supported by CRUM without change. Device-side memory allocation allows kernel code to allocate and de-allocate mem- ory. It is partially supported by CRUM, with one important distinction—no live device-side allocations are allowed at a checkpoint time. Thus, device-side memory allocations are to be freed before the system is considered quiesced and ready for a checkpoint. We do not anticipate this constraint to be particularly difﬁcult to satisfy, since device-side mallocs tend to be used to store temporary thread-local state within a single kernel, whereas host-side CUDA memory allocation (which is supported by CRUM without restriction) is more often used for persistent storage. Using mprotect: Currently, in a Linux kernel, PROT_WRITE protection for a memory region implies read-write memory permission rather than write-only memory permission. Because of this, some compromises were needed in the implementation. This work has demonstrated the practical advantages of a write-only memory permission for ordinary Linux virtual memory. It is hoped that in the future, the kernel developers at NVIDIA will be encouraged to support write-only memory permission for this purpose. Another issue with an mprotect-based approach is that when kernel-space code page faults on a read/write protected page, it returns an error code to the user, EFAULT, rather than a segfault. This forces the implementation to be extended to handle such failures; the implementation cannot rely solely on a segfault handler [37]–[40]. Other APIs and Languages: This work provides checkpoint-restart capabilities for programs written in C/C++ with the CUDA runtime library. In our experience, the CRUM prototype should support the majority of GPU-accelerated HPC workloads; however, there are other APIs to that may be valuable for some users. Given the current framework of code auto-generation for CRUM, we believe that it will be straightforward to extend the implementation to support other APIs, such as OpenACC. The ability of CRUM to support UVM-managed memory would be especially useful for Ope- nACC programs, as PGI’s OpenACC compiler provides native and transparent support for high-performance UVM-managed programs, making UVM-accelerated OpenACC programs a low-design-effort route to performant GPU acceleration [41]. Future Versions of CUDA: Just as prior checkpointing methods for GPUs were unable to cope with versions of CUDA since CUDA 4 (released in 2011), it is likely that CRUM will need to be updated to support language fea- tures after CUDA 8. One such development is Heterogeneous Memory Management (HMM) [42], which is a kernel feature introduced in Linux 4.14 that removes the need for explicit cu- daMallocManaged calls (or use of the managed keyword) to denote UVM-managed data. Rather, with HMM the GPU is able to access any program state, including the entire stack and heap. Because the current CRUM prototype relies on wrapping cudaMallocManaged calls, it will need to be redesigned to support HMM. VI. RELATED WORK a) Use of proxy process: Zandy et al. [43] demonstrated the use of a “shadow” process for checkpointing currently running application processes that were not originally linked with a checkpointing library. This allows the application process to continue to access its kernel resources, such as open ﬁles, via RPC calls with the shadow process. Kharbutli et al. [44] use a proxy process for isolation of heap accesses by a process and for containment of attacks to the heap. b) GPU virtualization: A large number of previous HPC studies have focused on virtualizing the access to the GPU [8]– [10], [12], [13], [30], [45], [46]. Here we describe some of those studies, with an emphasis on the use for GPU checkpointing and GPU-as-a-Service in the cloud and HPC environments. Lagar-Cavilla et al. [45], Shi et al. [8], Gupta et al. [9], and Giunta et al. [46] focus on providing access to the GPU for processes running in a virtual machine (VM), as an alternative to PCI pass-through. The access is provided by forwarding GPU calls to a proxy process that runs outside the VM and has direct access to the GPU. c) GPU-as-a-Service: Two other efforts, DS-CUDA [47] and rCUDA [48], have focused on providing access to a remote GPU for the purposes of GPU-as-a-Service [49]–[55]. They also rely on a proxy process. Using the proxy process is similar to the one described in this work; however, the focus is on efﬁcient remote access by using the InﬁniBand’s RDMA API. To the best of our knowledge, none of the previous studies solve the problem of efﬁcient checkpointing of modern CUDA applications that use UVM. We note that the optimizations described in these works can be used in conjunction with CRUM for providing efﬁcient access to remote GPUs. d) GPU Checkpointing: Early work on virtualizing or checkpointing GPUs was based on CUDA 2.2 and earlier [8]– [12]. Those approaches stopped working with CUDA 4 (intro- duced in 2011), which introduced Uniﬁed Virtual Addressing (UVA). Presumably, it is the introduction of UVA that made it impossible to re-initialize CUDA 4. In 2016, CRCUDA [13], employed a proxy-based approach, similar to the 2011 approach of CheCL [30] that targeted OpenCL [56] (as opposed to CUDA) for GPUs. OpenCL does not support uniﬁed memory, and so CheCL and CRCUDA do not support NVIDIA’s uniﬁed memory [23] targeted here. VOCL-FT [57] aims to provide resilience against soft er- rors. VOCL-FT leverages the OpenCL programming model to reduce the amount of data movement: both to/from the device from/to the host, and to/from the disk. This allows them to do fast checkpointing and recovery. HiAL-Ckpt [58], HeteroCheckpoint [59], and cudaCR [60] use application-speciﬁc approaches for providing GPU check- pointing. None of the approaches described above work for CUDA UVM. CRUM focuses on providing efﬁcient runtime and checkpointing support for CUDA and CUDA-UVM based programs. We note that the techniques described in above approaches are complementary to CRUM and can be used to further optimize the runtime and checkpointing overheads. VII. CONCLUSION This paper introduced CRUM, a novel framework for checkpoint-restart for CUDA’s uniﬁed memory. The frame- work employs a proxy-based architecture along with a novel shadow page synchronization mechanism to efﬁciently run and checkpoint CUDA UVM applications. Furthermore, the architecture enables fast, copy-on-write-based, asynchronous checkpointing for large-memory CUDA UVM applications. Evaluation results with a prototype implementation show that average runtime overhead imposed is less than 6%, while improving the checkpointing overhead by up to 40 times."
Formal Methods,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","March 15th, 2018",Transparently Checkpointing Software Test Benches to Improve Productivity of SoC Verification in an Emulation Environment,http://www.ccs.neu.edu/home/gene/papers/dvcon-us-18.pdf," ""Transparently Checkpointing Software Test Benches to Improve Productivity of SoC Verification in an Emulation Environment"", Ankit Garg, Suresh Krishnamurthy, Gene Cooperman, Rohan Garg, and Jeff Evans, <em>2018 Design and Verification Conference and Exhibition</em> (DVCON-US 2018)","Traditionally hardware emulation has been used in in-circuit emulation (ICE) mode where the design under test (DUT) executes inside the emulator and connected to the real target, which acts as a testbench. Over time, the software testbench-based emulation environments have become very popular, since the users can control its operation remotely from their desktops. This makes emulators an enterprise resource, accessible to a multitude of users spread across continents and multiple time zones. And since emulators are expensive resources, it is important to utilize them efficiently. Full checkpoint save/restore capability of emulation jobs helps the utilization by enabling flexible job scheduling, shortening of jobs by jumping ahead to interesting points for debug, carrying out what-if analysis, etc. Emulators have native save/restore capabilities for the model on the emulator. The software testbenches can be complex in multiple dimensions. For example, they may be using C/C++, SystemC, SystemVerilog, etc. They may be multi- threaded and based on multiple processes, they may be using IPC, and so on. It then becomes a challenge to save the states of such sophisticated software testbenches both transparently and through a uniform, reliable, mechanism. Since the objective is to solve the problem at an enterprise level, it is critical to find a uniform solution for a diverse set of software testbenches throughout the enterprise. The DMTCP (Distributed MultiThreaded Checkpointing) package supports such a uniform solution. This paper describes the integration of DMTCP with a virtual testbench-based emulation. This brings large benefits to a real life environment that includes multiple emulators within the larger set of enterprise resources. There are other, additional applications of full checkpoint save/restore for emulation jobs that become apparent only after having gained experience with its use in job management. For example, after having observed the behavior of an application prior to checkpoint, additional triggers can be inserted at the time of restore, to enhance debugging of an exception or other unusual behavior.s. II. BACKGROUND: CO-MODELING AND DMTCP A. Review of Co-Modeling Architecture Co-emulation, or (transaction-level) Co-Modeling, is the process of modeling cycle-accurate synthesizable hardware models (DUTs) running on an emulator, communicating with testbenches at transaction level via a high- speed link between the emulator and the host system. The reusable testbenches are interfaced to synthesizable transactors co-located with the DUT in the emulator. These “accelerated” transactors convert high-level transactions to signal-level stimuli to drive the DUT. During an emulation run, the hardware communicates with the software testbench using a high-speed link. For every DUT clock or for each time point in the model execution, communication may be required by one or more synthesizable transactors. In the general case, the DUT clocks will be suspended at times to complete all the communication requests for a given point in the model execution. So at any point of time, there could be inflight data in the link. Figure 1. Co-Modeling Architecture B. Review of DMTCP flow DMTCP is an open-source package that provides a capability for Checkpoint/Restart in applications involving multiple processes/threads distributed across multiple hosts and connected by socket connections. The package can be downloaded from: http://dmtcp.sourceforge.net It operates under Linux, with no modifications to the Linux kernel or to the user code, and it can be used by unprivileged users (no root privilege needed). One can later restart from a checkpoint, or even migrate the processes by moving the checkpoint files to another host prior to restarting. Figure 2 shows the typical flow of a user job under DMTCP. Figure 2. Typical flow of a user job under DMTCP A detailed description on DMTCP (Distributed MultiThreaded Checkpointing) internals can be found in [1]. DMTCP also provides a flexible plugin model that supports the ability to write an add-on library that can: support DMTCP event hooks; add custom wrappers around system calls; and add a custom distributed name service facility [2, 3]. III. MOTIVATING USE CASES Checkpoint-restore of emulation jobs opens a wide of range of applications, which will help users build a fault- tolerant emulation system, powerful debug mechanisms, and increase usage efficiency of critical resources like hardware emulators. This section describes different use cases for such capability in emulation. The following section, Section IV, then presents the Checkpoint/Restore Framework in detail. A. Skipping repeated initial sequences Designs may have an initialization phase that is always executed for each test. This may be a hardware reset phase or boot-up, which takes a great deal of time before an actual test can start. We can save much of the emulation runtime by taking a checkpoint right after this repeated initial sequence. New tests can then just restart immediately after this initial sequence, thus saving a great deal of regression time. Another application of this is to do what-if analysis after reaching an interesting point in the execution. In Figure 3, each test Test0, Test1, Test2 executes the same initial sequence, which takes C1 time. Figure 3. Job Progress with fixed initialization sequence The tests then follow their own unique paths, completing the test in different time intervals (T0, T1, and T2). This C1 time is saved if we checkpoint just after completion of the initial sequence and then restore from the checkpoint at the original state in each test. B. Better Job Management Policies One of the advantages of virtualization of test environments is the ability to use emulators remotely. This allows emulation to be moved to the data center, with jobs being managed by a workload management platform such as LSF. However, the non-pre-emptive nature of emulation jobs forbids pre-emptive scheduling policies. This prevents a high-priority job from acquiring the resources occupied by a currently executing low-priority job. For example, a currently running long job cannot be removed even though a short job has just arrived. The short job has to wait until the long job exits and frees up the resources. Hence, the non-pre-emptive nature of emulation jobs can easily lead to inefficient use of emulators. Consider the scenario depicted in Figure 4: Figure 4. Job Management Job1 occupies 4 slots on Emulator1 and Job2 occupies 2 slots on Emulator2. Further a high-priority request from Job3 arrives for 6 resources. As resources are fragmented, irrespective of its priority, Job3 has to wait for either of Job1 or Job2 to finish. The Checkpoint/Restore capability can remove this shortcoming and enable a pre-emptive scheduling policy. We could have checkpointed either of Job1 or Job2 and freed up the resources for Job3. Whenever resources become available, the checkpointed job can then be restarted using the DMTCP restart scripts. Further, this situation also leads to inefficient use of emulators due to resource fragmentation within each emulator. Using Checkpoint/Restore we could have migrated an interfering job to a different emulator, thereby making contiguous resources available for the new job. In Figure 5, the checkpoint of Job2 is taken first, and then Job2 is restarted on Emulator1. This frees up all 6 slots on Emulator2 for Job3. Job3 can now be allocated using contiguous resources on Emulator2. Figure 5. Resource free with Job Migration Another interesting example concerns the issue of “fairness” for large-capacity jobs. In a distribution system dominated by high-priority, small-capacity jobs, a low-priority large-capacity job may never actually get a chance to run if the policy is to wait for required slots to become available. Further, a high-priority long job can lead to underutilization of emulation resources as freed slots cannot be allocated for other small jobs if they have to be pooled for a high-priority long job waiting in the queue. With checkpoint-restart, one can save all the small jobs at once, run the large job and then restart the small jobs. A long low-priority job might otherwise never get an opportunity to complete, if not for the use of checkpoint-restart. There are several such instances where checkpoint- restart can offer this kind of flexibility in an emulation job scheduling system. C. Debugging from past simulation time Debugging is an important requirement for verification engineers. Much time is spent in debugging functional issues in design as well as integration issues with software during validation of a full SoC. The ability to take a checkpoint of the full system can provide capabilities to start debugging just before an issue occurs, by restarting from the point of a previous checkpoint state. For example, one can take periodic checkpoints and when a problem is seen: start from the last checkpoint, run again and capture more debug information. This is a tremendous advantage when a debugging issue occurs only after a run of long duration. A large amount of time is saved, since one no longer needs to wait for the test to arrive at the point of interest. IV. CHECKPOINT/RESTORE FRAMEWORK In this section we describe the implementation of checkpoint-restore for emulation jobs. We address checkpoint and restore separately. Figure 6 shows the typical flow of an emulation job under DMTCP. Note that in Figure 2, the checkpoint was invoked by the user at an arbitrary point in time. However, in the emulation flow (Figure 6), the user invokes the checkpoint programmatically from within the user code, when the simulation reaches certain number of cycles. Figure 6. Typical flow of an emulation job under DMTCP. A. Checkpoint Checkpointing of emulation jobs involves both the emulator and the testbench on the workstation. One must save the simulation state for the design running on an emulator, while also employing binary-level checkpointing using DMTCP for the processes representing the testbench side and running on the workstation. As always, we have to make sure that the hardware side stops generating further transactions when a checkpoint is in progress. In addition, checkpointing requires that we make sure that there is no in-flight data inside the high-speed interface between the emulator and the testbench at the time of checkpointing. In the case that a checkpoint is taken without flushing the in-flight data, those transactions will be lost at the time of restoring from the checkpoint, thus resulting both in incorrect hardware state and incorrect state on the testbench side. Figure 7. Flowchart for Checkpoint Algorithm The following is the integrated algorithm for checkpointing of an emulation job. These steps are also depicted graphically in Figure 7. 1) A Checkpoint Request is received by the testbench either through an external utility such as dmtcp_command or through a user-exposed API, ScheduleCheckpoint, made by testbench itself. The request is sent to the emulator in order to freeze the running design. This will stop both the user design clocks and the simulation time itself. 2) Once a design is frozen, additional transactions from the hardware side will no longer be generated. At this point the data in flight inside the link is flushed until all of them have been processed and the emulator reaches a quiescent state. 3) When the emulator has reached a quiescent state, the system initiates the checkpoint of the emulator model by using the checkpoint technology native to the emulator. This will store the current design state to disk. 4) The system then disconnects from the emulator and makes sure that all emulator-related processes are terminated. This step will ensure that any external connections not running under DMTCP are removed from consideration. For example, there may be connections to a licensing server, to some waveform dump servers, and so on. This also reduces the overall checkpoint time by freeing up memory used by these external connections that the binary checkpointing procedure would otherwise have to save, as described in the next step. Further, checkpoints taken this way are independent of which specific emulator the session was running on, and this makes it easy to relocate saved emulation jobs to other emulators. 5) Next, the system initiates DMTCP-based checkpointing on the testbench side. This involves calling dmtcp_checkpoint, a part of the DMTCP API. Here, one writes an emulation-specific external plugin for DMTCP, which will specify which files must be checkpointed. The DMTCP plugin is specific to the emulator and testbench infrastructure, and must also specify the path to the checkpoint database, where DMTCP checkpoint image files are saved alongside the hardware database, as a consolidated database within the user specified path. . DMTCP also allows a user to have certain processes be explicitly excluded from checkpointing. This might be required for the case where processes have been spawned from an external library linked into the user testbench. Those processes are not required as part of a correct checkpoint state. And further, attempting to save such superfluous processes leads to other issues when restoring from a checkpoint if those processes were communicating with external processes that were not running under DMTCP. The details of DMTCP external plugins can be found in [2, 3]. 6) After the invocation of dmtcp_checkpoint returns, we can choose either to exit the current emulation or to resume the current run. 7) After resuming from a checkpoint, the following steps are taken: ● Re-connect to same emulator and configure the same design again. ● Restart the tool-specific processes and re-connect to servers from which they exited in step (4). This step is isolated to the emulation tool’s internal workings, and does not require the collaboration of DMTCP. ● Perform a hardware design restore from the same checkpoint database and start the design clocks. ● Note that after being restored from a checkpoint, the testbench side resumes in a correct state at the end of this procedure, and so nothing more is required on the testbench side. B. Restore/Restart The DMTCP package dumps a restart script at the time of checkpoint. This script takes care of restarting the entire testbench tree of processes. This script also takes care of restarting processes that were on remote machines. After restart, the system will “wake up” in step (6) in the checkpoint sequence above, as if we have just returned from the call to dmtcp_checkpoint. So the remaining steps to restart are the same as those described in step (7) above. Figure 8. Flowchart for Restore The following is the algorithm for the restart/resume. These steps are also depicted graphically in Figure 8. 1) Use the restart script generated by DMTCP that relies on the checkpoint database (the checkpoint image files) in order to bring the whole testbench tree of processes up and running. 2) After DMTCP’s restore/restart, we return from the call to dmtcp_checkpoint. The next steps bring the HDL side up and fork any tool-specific processes. 3) The system then re-connects the emulator resource and downloads the design. 4) The system then restarts the tool-specific processes and re-connects to external servers such as license servers, waveform collection servers, etc. This step is completely isolated from the internal workings of the emulator tool. 5) The system then performs the hardware design restore using the same checkpoint database, and it starts the design clocks. This is the last step in successfully restoring the emulation. V. CASE STUDY: SKIPPING THE OS BOOT IN AN OEM COMPANY’S SOC VALIDATION ENVIRONMENT For this case study, the SoC validation environment instantiates a SoC containing a CPU, Memory Subsystem, Switching Fabric, and peripherals. The SoC is modeled in a hybrid environment whereby part of the SoC is modeled on the workstation and part of the SoC is modeled in the emulator, and the Switching Fabric is the bridge between the models. Many of the SoC validation use cases require firstly booting an operating system (OS) in order to run the applications that are being validated. The boot of the OS takes on the order of hours to days. Often the applications that are being validated execute in a fraction of the time that it takes to boot the OS. This means that only a fraction of the emulation time was used to validate the application and the larger portion of the time is spent booting the OS in order to be able to run the application. Thus the overall boot time has an important impact not only on how many applications can be run per user per day, but also, from an emulation efficiency standpoint, how much emulation time is spent just getting the OS booted versus running the applications that are used to validate the SoC and/or the application. The ideal scenario would be to eliminate the time it takes to boot the OS. A solution close to the ideal scenario is to checkpoint the SoC validation environment after the boot of the OS. This OS boot can then be delivered as part of a checkpoint image that is bundled with the rest of the SoC validation environment. The checkpoint of the hardware can be taken by technology native to the emulator. The complexity is in the checkpointing of the part of the SoC validation environment that is executing on the workstation. There were two early attempts to checkpoint, before settling on DMTCP as the preferred solution. A first attempt at checkpointing, prior to the use of DMTCP, was to capture all of the stimulus from the hardware during the OS boot along with a checkpoint of the hardware, and then to restore the OS boot using the stimulus. This stimulus was replayed into the software test bench to re-establish the state of the software, and then finally restore the state of the emulator using the hardware checkpoint. This method worked, except it had two notable drawbacks: (1) Depending on how much stimulus needed to be captured, the size of the replay database could become quite large. (2) The time it took the software testbench to execute, controlled the time it took to perform the restoration of the software testbench. A second attempt at checkpointing prior to employing DMTCP was to leverage the Boost C++ libraries to make each of the software components of the SoC validation environment checkpoint-able. This method worked except that it had a major drawback in that each SoC validation software component had to be developed with checkpointing in mind and if there was just one component that didn¹t support checkpointing or did the checkpointing incorrectly, the SoC validation environment was not checkpoint-able. Hence, the ideal solution would be to transparently checkpoint the software in the same way as we checkpoint the hardware. The DMTCP-based approach was able to overcome the limitations of these first two attempts by transparently checkpointing the software on the workstation, which includes the part of the SoC modeled on the workstation, the software testbench, and the emulation software. The checkpoint is taken without concern for how the software has been modeled and also without concern for the speed at which the software testbench executes. This resulted in making an “OS boot” checkpoint available to the users along with the SoC verification environment. This allows those users to restore the checkpoint in less than 5 minutes, and to then to run their applications for SoC validation in an environment after the OS boot. Now, users requiring this use case can focus their emulation time on running their application, and skipping the lengthy OS boot time. The DMTCP-based approach has accelerated the time to run an application by close to a factor of two. Additionally, it has freed up the emulation time that would have been spent in “OS boot”, thus saving many hours of emulation time. In addition, the DMTCP approach also saves the state of the emulation runtime software itself. This feature provides added value, as compared to the two earlier checkpointing approaches. The checkpoint-restore flow can now set up to address additional aspects of the emulation runtime environment, such as triggers, which are important for debugging in the case of an exception. This enables the designer to create an environment closer to a “turnkey solution”, in which the end user no longer has to remember to load the trigger prior to starting the run of their application. First use case of DMTCP at an OEM company was on a regression suite consisting of 40 jobs. The 40 jobs had a similar profile in that they required around 1 hour to boot the OS and then 1 hour of execution of test content. DMTCP was used to create a checkpoint just after the boot of the OS. This checkpoint then becomes part of the collateral of that database. Any future user of that database and software configuration can restore the checkpoint rather than re-running the boot of the OS. The restoration of database using the checkpoint takes around 5 minutes. That is a time savings of 55 minutes per job for this regression. As a reminder, this environment is a hybrid environment and pending the configuration of whether the CPU is in software or RTL and also the type of OS can greatly impact the ""OS boot"" time from an hour to days. For this regression the 40 jobs were able to run in around 22 hours versus previously they would have taken 40 hour. Job throughput was increased close to 2x. This results not only in a substantial time savings but also a substantial cost savings. Figure 9. Regression Suite Emulation Time Comparison Figure 9 is highlighting the changes introduced by adding DMTCP into this use case; the following points are related to the numbers in the figure 9. 1) Checkpoint is created for OS boot as part of preparing the database for users. 2) Restoring OS boot from checkpoint saves 55 minutes for each job. 3) First job is completed after 1.1 hours with DMTCP versus 2 hours without DMTCP. 4) Full regression is completed after 22.9 hours with DMTCP versus 40 hours without DMTCP. During the integration of DMTCP with the emulation and SoC validation environment, we faced several challenges: (1) Very large read-only files were being saved as part of the checkpoint. This had an impact on both checkpoint/restore time as well as the memory footprint on disk. This was solved by having the DMTCP Emulation plugin detect and decide not to checkpoint such read-only files. The tool-specific file list was written into the DMTCP plugin to identify just those files that needed to be checkpointed. (2) Checkpoints were not portable to another site, due to some file paths that were preserved as part of the checkpoint. This was solved by using the existing file path virtualization plugin of DMTCP, allowing these paths to be changed at restore time. Limitations found when deploying DMTCP 1) Some use cases have a remote process connected via TCP on Windows machine. Additional work would be required to support this kind of use case both within the DMTCP community as well as in the testbench infrastructure. 2) Some use cases require changing the initial state of the environment which is part of the DMTCP checkpoint. For instance, if you had a SoC that had a programmable number of DIMMs and the number of DIMMs was changing per test you wouldn't be able to re-use a single checkpoint. DMTCP was found to be easy to integrate, and it required minimal changes to emulation environment. This has demonstrated the success of this approach toward OS-based use case validation during emulation. VI. FUTURE WORK We intend to work on developing preemptive capabilities for emulation jobs in a workload management system such as LSF. A case study in this environment is needed in order to discover the practical challenges thereof, and to help deploy this technology for more flexible job scheduling management. Finally, verification of a network switch presents an additional interesting case study for the future. In this scenario, a virtual machine is often required, so that simulated Ethernet traffic can be injected by the virtual machine into the environment for verification test coverage. This makes transparent checkpointing on the testbench side more difficult, since even though the virtual machine can be modified to inject Ethernet traffic, the virtual machine snapshot facility is difficult to modify, and so the state of the Ethernet traffic generator will not be checkpointed. On restore, some steps in the state diagram for Ethernet packets may be lost. As part of future work, it is proposed to use the ability of DMTCP to checkpoint a virtual machine from the outside. DMTCP has the ability to carry out a snapshot from the outside for the case of a QEMU virtual machine over KVM [4]. That same work [4] also presents DMTCP's ability to checkpoint a network of virtual machines. This latter case makes possible verification for end-to-end Ethernet test coverage between two virtual machines. VII. CONCLUSION This paper describes the approach to transparently checkpoint/restore emulation jobs and various key benefits it brings along with it. This integration was successfully tried in an OEM company’s SOC validation environment, which not only reduced total regression time but also increased Emulator efficiency. Emulation time is precious and any saving of this time directly affects one’s total verification cost. Experimental results have shown that this approach, when integrated with job management system, has increased the emulator utilization and has also increased the productivity of the verification engineers by providing them with a window to look back in time. The integration has also been successfully tried for a variety of software testbenches including C/C++/SystemC, and a SystemVerilog testbench running on a simulator. These testbenches can have multiple threads or multiple processes spread across different machines."
Formal Methods,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","November 1st, 2016",Recent Developments in Geant4,http://www.ccs.neu.edu/home/gene/papers/geant4-16.pdf," ""Recent Developments in Geant4"", J. Allison et al. (99 co-authors in total, including G. Cooperman), <em>Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</em> <b>835</b>, pp. 186--225, Nov 1, 2016","classes for both detector sensitivity and hit had thus far been provided in the toolkit. A user was therefore required to have the expertise ne- cessary to implement the details of how hits were deﬁned, col- lected and stored. To relieve many users of this burden, concrete primitive scorers of physics quantities such as dose and ﬂux have been provided which cover general-use simulations. Flexibly designed base classes allow users to implement their own primitive scorers for use anywhere a sensitive detector needs to be simulated. Primitive scorers were built upon three classes, G4Multi- FunctionalDetector, G4VPrimitiveScorer and G4VSDFil- ter. G4MultiFunctionalDetector is a concrete class derived from G4VSensitiveDetector and attached to the detector component. Primitive scorers were developed on top of the base class G4VPrimitiveScorer, and as such represent classes to be registered to the G4MultiFunctionalDetector. G4VSDFilter is an abstract class for a track ﬁlter to be associated with a G4MultiFunctionalDetector or a primitive scorer. Concrete track ﬁlter classes are also provided. One example is a charged track ﬁlter and a particle ﬁlter that accept for scoring only charged tracks and a given particle species, respectively. A primitive scorer creates a G4THitsMap object for storing one physics quantity for an event. G4THitsMap is a template class for mapping an integer key to a pointer value. Since a physics quantity such as dose is generally accumulated in each cell of a detector component during an event or run, a primitive scorer generates a < > G4THitsMap G4double object that maps a pointer to a G4double for a physics quantity, and uses the cell number as the integer key. If a cell has no value, the G4THitsMap object has no corresponding entry and the pointer to the physics quantity returns a null. This was done to reduce memory consumption, and to distinguish an unﬁlled cell from one that has a value of zero. The integer key of the cell is taken from the copy number of the G4LogicalVolume of the de- tector component by default. GEANT4 also provides primitive scorers for three-dimensional structured geometry, in which copy numbers are taken at each of the depth levels at which of the logical volumes are nested in the geometric structure. These copy numbers are then serialized into integer keys. 3.conclusion not found"
Formal Methods,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","May 7th, 2014",Transparent Checkpoint-Restart over InfiniBand,http://www.ccs.neu.edu/home/gene/papers/hpdc14.pdf," ""Transparent Checkpoint-Restart over InfiniBand"", Jiajun Cao, Gregory Kerr, Kapil Arya and Gene Cooperman, ACM Symposium on High Performance Parallel and Distributed Computing (HPDC'14), pp. 13--24, ACM Press, 2014.","Transparently saving the state of the InﬁniBand network as part of distributed checkpointing has been a long-standing challenge for researchers. The lack of a solution has forced typical MPI implementations to include custom checkpoint- restart services that “tear down” the network, checkpoint each node in isolation, and then re-connect the network again. This work presents the ﬁrst example of transpar- ent, system-initiated checkpoint-restart that directly sup- ports InﬁniBand. The new approach simpliﬁes current prac- tice by avoiding the need for a privileged kernel module. The generality of this approach is demonstrated by applying it both to MPI and to Berkeley UPC (Uniﬁed Parallel C), in its native mode (without MPI). Scalability is shown by check- pointing 2,048 MPI processes across 128 nodes (with 16 cores per node). The run-time overhead varies between 0.8% ands (Section 7) are presented. 2. BACKGROUND Section 2.1 reviews some concepts of InﬁniBand, necessary for understanding the checkpointing approach described in Section 3. Section 2.2 describes the use of plugins in DMTCP. 2.1 InﬁniBand Verbs API In order to understand the algorithm, we review some concepts from the Verbs API of InﬁniBand. While there are several"
Formal Methods,Panagiotis (Pete) Manolios,https://www.khoury.northeastern.edu/people/panagiotis-manolios/,"Pete Manolios is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He leads the computer-aided reasoning group.","Computer-aided modeling, verification, analysis, and synthesis of systems; Formal methods; Distributed computing; Programming languages; Software engineering; Aerospace","PhD in computer science, University of Texas at Austin; BS in computer science, Brooklyn College","May 1st, 2023",Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals,https://www.vldb.org/pvldb/vol16/p2377-chen.pdf," Zixuan Chen, Panagiotis Manolios, Mirek Riedewald. (2023). Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals Proc. VLDB Endow., 16, 2377-2390. https://www.vldb.org/pvldb/vol16/p2377-chen.pdf","This work considers why-not questions in the context of top-k queries and score-based ranking functions. Following the popular linear scalarization approach for multi-objective optimization, we study rankings based on the weighted sum of multiple scores. A given weight choice may be controversial or perceived as unfair to certain individuals or organizations, triggering the question why some entity of interest has not yet shown up in the top-k. We introduce various notions of such why-not-yet queries and for- mally define them as satisfiability or optimization problems, whose goal is to propose alternative ranking functions that address the placement of the entities of interest. While some why-not-yet prob- lems have linear constraints, others require quantifiers, disjunction, and negation. We propose several optimizations, ranging from a monotonic-core construction that approximates the complex con- straints with a conjunction of linear ones, to various techniques that let the user control the tradeoff between running time and approximation quality. Experiments with real and synthetic data demonstrate the practicality and scalability of our technique, show- ing its superiority compared to the state of the art (SOA). PVLDB Reference Format: Zixuan Chen, Panagiotis Manolios, and Mirek Riedewald. Why Not Yet: Fixing a Top-k Ranking that Is Not Fair to Individuals. PVLDB, 16(9): 2377 - 2390, 2023. doi:10.14778/359858We propose the first general exact solution for problems SAT, BEST, and POINT. Adopting sampling approaches from related work can only provide approximate answers or results in infeasible running time for BEST. In general, sampling becomes ineffective when only a small fraction of the space of possible weight vectors ranks the expected tuples among the top-𝑘. For BOX, we propose the first known solution. To make it practical and scalable, we propose the notion of a monotonic core. Our clustering approach enables the user to improve running time for all problems as desired by controlling the number of clusters, with moderate loss in result quality even for large data. Interesting avenues for future work are computing a compact description of the entire set of weight vectors that rank the expected tuples among the top-𝑘and generalizing the approach to noisy and unreliable data."
Formal Methods,Panagiotis (Pete) Manolios,https://www.khoury.northeastern.edu/people/panagiotis-manolios/,"Pete Manolios is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He leads the computer-aided reasoning group.","Computer-aided modeling, verification, analysis, and synthesis of systems; Formal methods; Distributed computing; Programming languages; Software engineering; Aerospace","PhD in computer science, University of Texas at Austin; BS in computer science, Brooklyn College","January 2nd, 2014",An array-oriented language with static rank polymorphism,http://www.ccs.neu.edu/home/jrslepak/typed-j.pdf," ""An array-oriented language with static rank polymorphism"", Slepak, Justin and Shivers, Olin and Manolios, Panagiotis, Programming Languages and Systems, pages 27-46, 2014. Springer.","The array-computational model pioneered by Iverson’s lan- guages APL and J oﬀers a simple and expressive solution to the “von Neumann bottleneck.” It includes a form of rank, or dimensional, poly- morphism, which renders much of a program’s control structure im- plicit by lifting base operators to higher-dimensional array structures. We present the ﬁrst formal semantics for this model, along with the ﬁrst static type system that captures the full power of the core language. The formal dynamic semantics of our core language, Remora, illuminates several of the murkier corners of the model. This allows us to resolve some of the model’s ad hoc elements in more general, regular ways. Among these, we can generalise the model from SIMD to MIMD computations, by extending the semantics to permit functions to be lifted to higher- dimensional arrays in the same way as their arguments. Our static semantics, a dependent type system of carefully restricted power, is capable of describing array computations whose dimensions cannot be determined statically. The type-checking problem is decidable and the type system is accompanied by the usual soundness theorems. Our type system’s principal contribution is that it serves to extract the implicit control structure that provides so much of the language’s expres- sive power, making this structure explicitly apparent at compile time. 1 The Promise of Rank Polymorphism Behind every interesting programming language is an interesting model of com- putation. For example, the lambda calculus, the relational calculus, and ﬁnite- state automata are the computational models that, respectively, make Scheme, SQL and regular expressions interesting programming languages. Iverson’s lan- guage APL [7], and its successor J [10], are interesting for this very reason. That is, they provide a notational interface to an interesting model of computation: loop-free, recursion-free array processing, a model that is becoming increasingly relevant as we move into an era of parallel computation. APL and J’s array-computation model is important for several reasons. First, the model provides a solution to Backus’s “von Neumann bottleneck” [1]. Instead of using iteration or recursion, all operations are automatically aggregate oper- ations. This lifting is the fundamental control ﬂow mechanism. The iteration space associated with array processing is reiﬁed as the shape of the arrays being processed. Though the paradigm is not without implementation challenges of its own, it at least holds out the promise of eliminating the heroic measures required by modern compilers (e.g., the construction of program-dependency graphs and their diﬃcult associated decision procedures [20]) to extract parallelism through the serialised program’s obfuscatory encoding. Second, operator lifting provides a form of polymorphism based on operands’ rank, or dimensionality. An operation deﬁned for arguments of one rank is auto- matically deﬁned for arguments of any higher rank. They are thus parameterized over the ranks of their inputs. The operator for scalar addition is also used for adding a vector to a matrix, a scalar to a three-dimensional array, and so forth. Third, despite its great expressive power, the core computation model is sub- Turing. Lifting operations to work on aggregate structures means the control structure is embedded in the data structure. With a ﬁnite data structure rep- resenting the program’s control structure, all iteration is bounded. Thus APL’s computational model has the potential to occupy a “sweet spot” in language de- sign: increased analytic power without surrendering signiﬁcant expressiveness.We have given a formal reduction semantics for Iverson’s rank polymorphism which addresses several shortcomings of the model. Remora generalizes auto- matic operator lifting to include ﬁrst-class functions and MIMD computation. Embedding the core ideas of APL and J in a setting based on λ-calculus com- bines the expressive power of both models. Our type system rules out errors due to mismatching argument shapes and still gives the programmer enough freedom to write code whose result shape cannot be determined until run time."
Formal Methods,Ji-Yong Shin,https://www.khoury.northeastern.edu/people/ji-yong-shin/,"Ji-Yong Shin is an assistant professor in the Khoury College of Computer Sciences, based in Boston.",Distributed systems; Formal verification; Cloud storage systems; Operating systems,"PhD in Computer Science, Cornell University; MS in Computer Science, Korea Advanced Institute of Science and Technology — South Korea; BS in Computer Science and Industrial Engineering, Yonsei University — South Korea","March 5th, 2024",FusionFlow: Accelerating Data Preparation for Machine Learning with Hybrid CPU-GPU Processing,https://www.vldb.org/pvldb/vol17/p863-kim.pdf," Taeyoon Kim, Chanho Park, Mansur Mukimbekov, Heelim Hong, Minseok Kim, Ze Jin, Changdae Kim, Ji-Yong Shin, Myeongjae Jeon. (2023). FusionFlow: Accelerating Data Preparation for Machine Learning with Hybrid CPU-GPU Processing Proc. VLDB Endow., 17, 863-876. https://www.vldb.org/pvldb/vol17/p863-kim.pdf","Data augmentation enhances the accuracy of DL models by diversi- fying training samples through a sequence of data transformations. While recent advancements in data augmentation have demonstrated remarkable efficacy, they often rely on computationally expensive and dynamic algorithms. Unfortunately, current system optimiza- tions, primarily designed to leverage CPUs, cannot effectively sup- port these methods due to costs and limited resource availability. To address these issues, we introduce FusionFlow, a system that cooperatively utilizes both CPUs and GPUs to accelerate the data preprocessing stage of DL training that runs the data augmentation algorithm. FusionFlow orchestrates data preprocessing tasks across CPUs and GPUs while minimizing interference with GPU-based model training. In doing so, it effectively mitigates the risk of GPU memory overflow by managing memory allocations of the tasks within the GPU-wide free space. Furthermore, FusionFlow provides a dynamic scheduling strategy for tasks with varying computational demands and reallocates compute resources on the fly to enhance training throughput for both single and multi-GPU DL jobs. Our evaluations show that FusionFlow outperforms existing CPU-based methods by 16–285% in single-machine scenarios and, to achieve similar training speeds, requires 50–60% fewer CPUs compared to utilizing scalable compute resources from external servers. PVLDB Reference Format: Taeyoon Kim, ChanHo Park, Mansur Mukimbekov, Heelim Hong, Minseok Kim, Ze Jin, Changdae Kim, Ji-Yong Shin, and Myeongjae Jeon. FusionFlow: Accelerating Data Preprocessing for Machine Learning with CPU-GPU Cooperation. PVLDB, 17(4): 863 - 876, 2023. doi:10.14778/3636218.3636238 PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://github.com/omnia-unist/FusionFlow. This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 17, No. 4 ISSN 2150-8097. doi:10.14778/3636218.3636238 1We propose FusionFlow that speeds up the dynamic data augmen- tation algorithms on CPUs and GPUs. The key idea is exploiting intra-batch parallelism, which splits an input mini-batch into mul- tiple tiny-batches and augments the mini-batch in parallel on those compute resources. FusionFlow applies several optimizations to make GPU-offloading of tiny-batch tasks highly effective and per- forms CPU worker scaling to make CPU resource usage in data- parallel training more balanced. Experimental results confirm the effectiveness of FusionFlow."
Games,Seth Cooper,https://www.khoury.northeastern.edu/people/seth-cooper/,"Seth Cooper is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Tools for assisting game design and development; Interfaces for problem-solving; Scientific discovery games; Crowdsourcing games; Serious games; Citizen science,"PhD in Computer Science and Engineering, University of Washington; MS in Computer Science and Engineering, University of Washington; BS in Electrical Engineering and Computer Science, University of California, Berkeley","October 20th, 2020",Grammar Based Modular Level Generator for a Programming Puzzle Game,https://ceur-ws.org/Vol-2862/paper12.pdf," Chaima Jemmali, Carter Ithier, Seth Cooper, Magy Seif El-Nasr. (2020). Grammar Based Modular Level Generator for a Programming Puzzle Game AIIDE Workshops. https://ceur-ws.org/Vol-2862/paper12.pdf","Procedural Content Generation is widely used in games, how- ever, its use in educational puzzle games has been limited. These types of games present common challenges such as solvability and non triviality, but also the extra challenge of preserving intended learning goals. In this paper, we present a modular constructive approach to generate levels in a puzzle programming game. The approach uses a grammar to gen- erate game elements from code and works backwards from the solution to ensure solvability, controllability over the so- lution, and variation, allowing for alternative solutions that preserve the learning goals.In this paper, we presented a grammar based modular ap- proach to generate levels in a programming puzzle game. The approach works backwards from a solution code and uses both the solution map and the initial map to ensure that levels are solvable using the input code. The levels gener- ated allow variation in the solution space through alterna- tive codes while minimizing shorter, more trivial solutions. However, some of them still allow shorter codes. In the fu- ture, we want to improve on the approach, build a user- friendly interface and conduct a user-study with designers. Further, we would like to work on integrating procedurally generated levels within the game according to some player model that will inform us about the coding constructs that the player needs practice with."
Games,Seth Cooper,https://www.khoury.northeastern.edu/people/seth-cooper/,"Seth Cooper is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Tools for assisting game design and development; Interfaces for problem-solving; Scientific discovery games; Crowdsourcing games; Serious games; Citizen science,"PhD in Computer Science and Engineering, University of Washington; MS in Computer Science and Engineering, University of Washington; BS in Electrical Engineering and Computer Science, University of California, Berkeley","July 11th, 2017","On Variety, Complexity, and Engagement in Crowdsourced Disaster Response Tasks",http://www.ccs.neu.edu/home/scooper/index_files/pub/spatharioti2017variety.pdf," Sofia Eleni Spatharioti and Seth Cooper.  On Variety, Complexity, and Engagement in Crowdsourced Disaster Response Tasks. Proceedings of the 14th International Conference on Information Systems for Crisis Response and Management (2017).","Crowdsourcing is used to enlist workers as a resource for a variety of applications, including disaster response. However, simple tasks such as image labeling often feel monotonous and lead to worker disengagement. This provides a challenge for designing successful crowdsourcing systems. Existing research in the design of work indicates that task variety is a key factor in worker motivation. Therefore, we asked Amazon Mechanical Turk workers to complete a series of disaster response related subtasks, consisting of either image labeling or locating photographed areas on a map. We varied the frequency at which workers encountered the diﬀerent subtask types, and found that switching subtask type at diﬀerent frequencies impacted measures of worker engagement. This indicates that a certain amount of variety in subtasks may engage crowdsourcing workers better than uniform subtask types. Keywords crowdsourcing, Amazon Mechanical Turk, variety, complexity, engagementconclusion not found"
Games,Alexandra To,https://www.khoury.northeastern.edu/people/alexandra-to/,"Alexandra To is an assistant professor in the Khoury College of Computer Sciences and the College of Arts, Media and Design at Northeastern University, based in Boston.",,,"March 1st, 2021","“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability",https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2021/description-representations.pdf," Bennett, C. L., Gleason, C., Scheurman, M. K., Bigham, J. P., Guo, A., & To, A. (2021). “It’s Complicated”: Negotiating Accessibility and (Mis) Representation in Image Descriptions of Race, Gender, and Disability.",Failed to download
Human-Centered Computing,Stephen Intille,https://www.khoury.northeastern.edu/people/stephen-intille/,"Stephen Intille is a professor in the Khoury College of Computer Sciences and the Bouvé College of Health Sciences at Northeastern University, based in Boston.","Personal health informatics; Interactive, mobile sensing; Machine learning; Behavioral theory and measurement","PhD in Media Arts and Sciences, Massachusetts Institute of Technology; SM in Media Arts and Sciences, Massachusetts Institute of Technology; BSE in Computer Science and Engineering, University of Pennsylvania","January 1st, 2022",Techno-Spiritual Engagement: Mechanisms for Improving Uptake of mHealth Apps Designed for Church Members 130-138,http://ceur-ws.org/Vol-3124/paper13.pdf," Hye Sun Yun, Shou Zhou, Everlyne Kimani, Stefan Olafsson, Teresa K. O'Leary, Dhaval Parmar, Jessica A. Hoffman, Stephen S. Intille, Michael K. Paasche-Orlow, Timothy W. Bickmore. (2022). Techno-Spiritual Engagement: Mechanisms for Improving Uptake of mHealth Apps Designed for Church Members 130-138 IUI Workshops, 130-138. http://ceur-ws.org/Vol-3124/paper13.pdf","Keeping users engaged with mHealth applications is important but difficult to achieve. We describe the development of a smartphone-based application designed to promote health and wellness in church communities, along with mechanisms explicitly designed to maintain engagement. We evaluated religiously tailored techno-spiritual engagement mechanisms, including a prayer posting wall, pastor announcements, an embodied conversational agent for dialogue-based scriptural reflections and health coaching, and tailored push notifications. We conducted a four-week pilot study with 25 participants from two churches, measuring high levels of participant acceptance and satisfaction with all features of the application. Engagement with the app was higher for users considered to be more religious and correlated with the number of notifications received. Our findings demonstrate that our tailored mechanisms can increase engagement with an mHealth app. Keywords engagement, tailoring, field study, church communities, mHealth,s and Limitations Maintaining user retention with mobile interventions is essential for affecting longitudinal outcomes in health, education, and other application domains. Utilizing well- tailored engagement mechanisms that meet the interests and needs of a priority population may be useful in main- taining engagement with the app over time and more effective at providing help and interventions to users. We explored a range of religiously tailored engagement strategies that can be used to motivate church community members to interact with a smartphone-based mHealth app. We described and evaluated six religiously tailored engagement mechanisms in this pilot study: an ECA, Prayer Center, Pastor Announcements, Bible Story of the Day, Scriptural Meditation, and push notifications. We demonstrated that push notifications were effective at driving the use of the app, with the number of notifica- tions per day and the user’s private religious practices score being significant factors in predicting the number of user logins per day throughout the study. We found that user satisfaction with all elements of the app was high with several participants finding Clara to be relatable and having personal resonance with the religiosity, spir- ituality, and role she exhibits. Our study demonstrated that religiously tailored engagement mechanisms that fulfill techno-spiritual functions can help religious users engage with an mHealth application. We did not find that the use of the app led to signif- icant pre-post improvements in health attitudes or be- haviors, likely due to the short duration of the study and a small convenience sample. We plan to evaluate the effectiveness of our app in improving health behaviors through future studies and used this pilot study to mainly evaluate the engagement mechanisms. Also, due to the participants of our pilot study being financially compen- sated, we can expect some level of response bias where engagement and satisfaction with our app can partly be associated with being compensated rather than the en- gagement mechanisms. In addition, we did not explore the duration of ECA sessions or scrolling through the Prayer Wall as a measure of engagement. Measuring session duration in light of app suspension, exits, and interrupts is very error-prone, and we felt that these data were too noisy to warrant analysis. Finally, this pilot study lacked a control condition, and a series of studies to systematically evaluate each engagement mechanism relative to a control is ultimately needed."
Human-Centered Computing,Alexandra To,https://www.khoury.northeastern.edu/people/alexandra-to/,"Alexandra To is an assistant professor in the Khoury College of Computer Sciences and the College of Arts, Media and Design at Northeastern University, based in Boston.",,,"March 1st, 2021","“It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability",https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2021/description-representations.pdf," Bennett, C. L., Gleason, C., Scheurman, M. K., Bigham, J. P., Guo, A., & To, A. (2021). “It’s Complicated”: Negotiating Accessibility and (Mis) Representation in Image Descriptions of Race, Gender, and Disability.","s of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA ’20). Association for Computing Machinery, New York, NY, USA, 1–9. https://doIn this paper, we engaged 25 screen reader users who are also BIPOC, non-binary, and/or transgender on the topics of appearance, image descriptions, and AI’s generation of them. While assumed primarily visual, appearance is a sociomaterial phenomenon which blind people negotiate with nonvisual workarounds and understandings of visual representation. In parallel, image descriptions remain scant online, and AI may offer scalable solutions to proliferate their presence and therefore increase access to visual information by screen reader users. In our interviews, we learned that participants had extensive experiences developing their identity but were regularly misrepresented by others. They engaged numerous strategies for understanding appearance, which was often sought in certain contexts, yet they rarely described their appearance in practice, calling into question tensions of providing access and what may be safe or reasonable to ask people to disclose. Participants viewed descriptions of appearance and identity extremely different and thought it essential for image describers to understand the distinction; when possible, they wanted their preferred language used to describe themselves, but they strongly recommended less politicized and more concrete description of appearance when the photographee’s identity could not be confirmed. Participants were excited about the potential for AI to increase access to image descriptions, but they had significant concerns about its accurate and ethical deployment. From these lessons, we recommend increased commitments in HCI to destabilize assumptions that vision is a shared language in communications, image descriptions which directly engage appearance in appropriate contexts, and guidelines which continually probe image describers to evolve their practices with community p"
Machine Learning,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","February 20th, 2023",Improving Deep Policy Gradients with Value Function Search,https://openreview.net/pdf?id=6qZC7pfenQm," Enrico Marchesini, Christopher Amato. (2023). Improving Deep Policy Gradients with Value Function Search ICLR. https://openreview.net/pdf?id=6qZC7pfenQm","Deep Policy Gradient (PG) algorithms employ value networks to drive the learn- ing of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approxima- tion and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the su- pervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains. 1VFS introduces a two-scale perturbation operator voted to diversify a population of value networks to (i) explore local variations of current critics’ predictions and (ii) allow to explore diversified value functions to escape from local optima. The practical results of such components have been investigated with additional experiments that also motivate the improvement in sample efficiency and performance of VFS-based algorithms in a range of standard continuous control benchmarks. Our findings suggest that improving fundamental Deep PG primitives translates into higher-performing policies and better sample efficiency. 9 Published as a conference paper at ICLR 2023 7"
Machine Learning,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","July 5th, 2018",Decision-Making Under Uncertainty in Multi-Agent and Multi-Robot Systems: Planning and Learning,http://www.ccs.neu.edu/home/camato/publications/ijcai18.pdf," Christopher Amato. In the Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18), July 2018","Multi-agent planning and learning methods are be- coming increasingly important in today’s intercon- nected world. Methods for real-world domains, such as robotics, must consider uncertainty and limited communication in order to generate high- quality, robust solutions. This paper discusses our work on developing principled models to represent these problems and planning and learning methods that can scale to realistic multi-agent and multi- robot tasks. 1The methods that have been discussed have shown a lot of promise, but there are other methods that are also promising (e.g., [Dibangoye et al., 2016; Claes et al., 2017; Nguyen et al., 2017]) and many open questions yet to solve. It is worth noting that the methods in this paper focused on the ‘full’ Dec-POMDP problem, where there was uncertainty about outcomes, sensing and communication and agents are depen- dent on all others, but some problems do not have all of these characteristics. In these cases, ideas from Dec-POMDP mod- els and methods could still be used, but structure may be able to be exploited to allow higher-quality, more efﬁcient solution methods. Some such structure has been explored [Oliehoek and Amato, 2016], but efﬁciently exploiting structure in other cases could lead to scalable methods that also consider un- certainty (e.g., probabilistic solutions to common multi-robot problems). Traditionally, scalability of Dec-POMDPs solution meth- ods has been an issue, but more recent methods (such as those discussed in this paper) can scale to large domains. One approach that is used to overcome this lack of scal- ability is deep reinforcement learning. Developing deep RL approaches for Dec-POMDP-based models is becom- ing a very active ﬁeld (e.g., [Foerster et al., 2016; 2017; Mordatch and Abbeel, 2017; Omidshaﬁei et al., 2017c; Rashid et al., 2018]) and the resulting methods can handle large state and observation spaces (and potentially large ac- tion spaces). These methods (including ours) require a lot of data and typically use a simulator, so they may ﬁt better in situations akin to the ofﬂine sample-based planning scenarios described earlier. Therefore, there has been a great deal of success in scal- ing planning and ofﬂine learning methods to large domains, but efﬁcient online learning remains a challenge. As men- tioned above, online learning in a Dec-POMDP must be de- centralized (since fast and free communication is not avail- able to centralize decision-making), leading to nonstationar- ity, which must (continue to) be tackled. Besides more effec- tively conquering nonstationarity in learning, other open re- search topics include developing planning and learning meth- ods with additional efﬁciency and scalability (in terms of the number of agents as well as action and observation spaces) and how to properly represent and encode history informa- tion for planning and learning. Overall, there has been great progress in solving large, realistic Dec-POMDPs. It will be exciting to see what further developments and applications will arise in the future."
Machine Learning,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","May 5th, 2018",Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems,http://www.ccs.neu.edu/home/camato/publications/ICRA2018.pdf," Nghia Hoang, Yuchen Xiao, Kavinayan Sivakumar, Christopher Amato and Jonathan P. How. In the Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA-18), May 2018.","— A key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self- interested or even adversarial parties who actively try to prevent the agents from achieving their goals. The practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with ﬁxed, procedurally characterized strategies. In contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies. This problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly. Our approach ﬁrst optimizes a set of stratagems that represent these best responses. These optimized stratagems are then inte- grated into a uniﬁed policy that can detect and respond when the adversaries change their strategies. The near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware.This paper introduces a novel near-optimal adversarial pol- icy switching algorithm for decentralized, non-cooperative multi-agent systems. Unlike the existing works in literature which are mostly limited to simple decision-making sce- narios where a single agent plans its best response against an adversary whose strategy is speciﬁed a priori under reasonable assumptions, we investigate instead a class of multi-agent scenarios where multiple robots need to operate independently in collaboration with their teammates to act effectively against adversaries with changing strategies. To achieve this, we ﬁrst optimize a set of basic stratagems that each is tuned to respond optimally to a pre-identiﬁed basic tactic of the adversaries. The stratagems are then integrated into a uniﬁed policy which performs near-optimally against B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 (a) (b) (c) B3 B2 B1 R1 R2 B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 (d) (e) (f) Fig. 6: Image excerpts from a video demo showing (1) a team of 3 allied (blue) robots (B1,B2 and B3) that implement the optimized stratagem produced by our framework (Section III) to compete against (2) an opposing team of 2 opponent (red) robots (R1 and R2) which implement the hand-coded tactics DL and DR (see Section VI-A), respectively: (a) B1,B2 and B3 decide to invade the opposition territory; (b) B1 and B3 decide to attack the center while B2 decides to take the left ﬂank of the opposition; (c) B2 passes through R1’s defense while B1 takes an interesting position to block R2 so that B3 can pass through its defense; (d) B1 and B2 detect the ﬂag and mount a pincer attack; (e) R2 arrives to defend the ﬂag and B2 retreats to avoid getting tagged; and (f) without noticing B1 from behind, R2 continues its DR patrol, thus losing the ﬂag to B1. any high-level strategies of the adversaries that switches between their basic tactics. The near-optimality of our pro- posed framework can be established in both theoretical and empirical settings with interesting and consistent results. We believe this is a signiﬁcant step towards bridging the gap between theory and practice in multi-agent research."
Machine Learning,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","August 4th, 2017",COG-DICE: An Algorithm for Solving Continuous-Observation Dec-POMDPs,http://www.ccs.neu.edu/home/camato/publications/cogdice_ijcai.pdf," COG-DICE: An Algorithm for Solving Continuous-Observation Dec-POMDPs. Madison Clark-Turner and Christopher Amato. In the Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17), August 2017","The decentralized partially observable Markov de- cision process (Dec-POMDP) is a powerful model for representing multi-agent problems with de- centralized behavior. Unfortunately, current Dec- POMDP solution methods cannot solve problems with continuous observations, which are common in many real-world domains. To that end, we present a framework for representing and gener- ating Dec-POMDP policies that explicitly include continuous observations. We apply our algorithm to a novel tagging problem and an extended version of a common benchmark, where it generates poli- cies that meet or exceed the values of equivalent discretized domains without the need for ﬁnding an adequate discretization. 1This paper presented, for the ﬁrst time, an algorithm that gen- erates joint policies for Dec-POMDPs with continuous ob- servations. We presented both a discrete-observation ver- sion of the algorithm, which is applicable in domains with a large number of discrete observations, and a continuous- observation version. This method is broadly applicable as many real-world domains have large or continuous observa- tion spaces. COG-DICE has been successful in generating joint policies for both a novel and a preexisting problem and has highlighted the negative impacts that inappropriate dis- cretization can have on joint policy structure and value. For future work, we are interested in extending this work to high- dimensional observation spaces by exploring other (nonlin- ear) divisions and optimizing the algorithm parameters by ei- ther integrating these optimizations into the algorithm or pos- sibly building on previous work on Bayesian non-parametrics [Liu et al., 2015]."
Machine Learning,David Bau,https://www.khoury.northeastern.edu/people/david-bau/,"David Bau is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Machine learning; Computer vision; Artificial intelligence; Natural language processing; Human–computer interaction,"PhD in Computer Science, Massachusetts Institute of Technology; MS in Computer Science, Cornell University; AB in Mathematics, Harvard University","October 24th, 2022",Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,https://openreview.net/pdf?id=DeG07_TcZvT," Kenneth Li , Aspen K. Hopkins, David Bau, Fernanda B. Viégas, Hanspeter Pfister, Martin Wattenberg. (2023). Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task ICLR. https://openreview.net/pdf?id=DeG07_TcZvT","Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question in a synthetic setting by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network. By leveraging these intervention techniques, we produce “latent saliency maps” that help explain predictions. 1 1Our experiments provide evidence that Othello-GPT maintains a representation of game board states— that is, the Othello “world”—to produce sequences it was trained on. This representation appears to be nonlinear in an essential way. Further, we find that these representations can be causally linked to how the model makes its predictions. Understanding of the internal representations of a sequence model is interesting in its own right, but may also be helpful in deeper interpretations of the network. We have also described how interventional experiments may be used to create a “latent saliency map”, which gives a picture, in terms of the Othello board, of how the network has made a prediction. Applied to two versions of Othello-GPT that were trained on different data sets, the latent saliency maps highlight the dramatic differences between underlying representations of the Othello-GPT trained on synthetic dataset and its counterpart trained on championship dataset. There are several potential lines of future work. One natural extension would be to perform the same type of investigations with other, more complex games. It would also be interesting to compare the strategies learned by a sequence model trained on game transcripts with those of a model trained with a priori knowledge of Othello. One option is to compare latent saliency maps of Othello–GPT with standard saliency maps of an Othello-playing program which has the actual board state as input. More broadly, it would be interesting to study how our results generalize to models trained on natural language. One stepping stone might be to look at language models whose training data has included game transcripts. Will we see similar representation of board state? Grammar engineering tools (Weston et al., 2015; Hermann et al., 2017; Cˆot´e et al., 2018) could help define a synthetic data generation process that maps world representations onto natural language sentences, providing a similarly controllable setting like Othello while closing the distance to natural languages. For more complex natural language tasks, can we find meaningful world representations? Our hope is that the tools described in this paper—nonlinear probes, layerwise interventions, and latent saliency maps—may prove useful in natural language settings. 9 Published as a conference paper at ICLR 2023"
Machine Learning,David Bau,https://www.khoury.northeastern.edu/people/david-bau/,"David Bau is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Machine learning; Computer vision; Artificial intelligence; Natural language processing; Human–computer interaction,"PhD in Computer Science, Massachusetts Institute of Technology; MS in Computer Science, Cornell University; AB in Mathematics, Harvard University","October 13th, 2022",Mass-Editing Memory in a Transformer,https://openreview.net/pdf?id=MkbcAHIYgyS," Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, David Bau. (2023). Mass-Editing Memory in a Transformer ICLR. https://openreview.net/pdf?id=MkbcAHIYgyS","Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at memit.baulab.info. 1We have developed MEMIT, a method for editing factual memories in large language models by directly manipulating specific layer parameters. Our method scales to much larger sets of edits (100x) than other approaches while maintaining excellent specificity, generalization, and fluency. Our investigation also reveals some challenges: certain relations are more difficult to edit with robust specificity, yet even on challenging cases we find that MEMIT outperforms other methods by a clear margin. The knowledge representation we study is also limited in scope to working with directional (s, r, o) relations: it does not cover spatial or temporal reasoning, mathematical knowledge, linguistic knowledge, procedural knowledge, or even symmetric relations. For example, the association that “Tim Cook is CEO of Apple” must be processed separately from the opposite association that “The CEO of Apple is Tim Cook.” Despite these limitations, it is noteworthy that large-scale model updates can be constructed using an explicit analysis of internal computations. Our results raise a question: might interpretability-based methods become a commonplace alternative to traditional opaque fine-tuning approaches? Our positive experience brings us optimism that further improvements to our understanding of network internals will lead to more transparent and practical ways to edit, control, and audit models. 9 Published as a conference paper at ICLR 2023 7 ETHICAL CONSIDERATIONS Although we test a language model’s ability to serve as a knowledge base, we do not find these models to be a reliable source of knowledge, and we caution readers that a LLM should not be used as an authoritative source of facts. Our memory-editing methods shed light on the internal mechanisms of models and potentially reduce the cost and energy needed to fix errors in a model, but the same methods might also enable a malicious actor to insert false or damaging information into a model that was not originally present in the training data. 8"
Machine Learning,Divya Chaudhary,https://www.khoury.northeastern.edu/people/divya-chaudhary/,"Divya Chaudhary is an assistant teaching professor in the Khoury College of Computer Sciences at Northeastern University, based in Seattle.",Artificial intelligence; Data science; Machine learning; Natural language processing and information retrieval; Personal health informatics,"PhD in Computer Engineering, University of Delhi — India; MTech in Computer Science, Maharshi Dayanand University — India; BTech in Information Technology, Maharshi Dayanand University — India","October 22nd, 2023",Exploring the Indian Political YouTube Landscape: A Multimodal Multi-Task Approach,https://ceur-ws.org/Vol-3566/paper1.pdf," Adwita Arora, Naman Dhingra, Divya Chaudhary, Ian Gorton, Bijendra Kumar. (2023). Exploring the Indian Political YouTube Landscape: A Multimodal Multi-Task Approach MUWS@CIKM, 3-17. https://ceur-ws.org/Vol-3566/paper1.pdf","Social media profoundly influences all facets of our lives, including politics. Political parties, politicians, and media outlets have strategically cultivated their social media presence to engage with the public. However, with the advent of freely available Internet services in India, there has been a rising proliferation in the community of independent content creators on YouTube, with many getting millions of views per video. In this study, we present a novel multimodal dataset of videos, taken from 20 independent and influential content creators, annotated for five socially and politically relevant labels with a high inter-annotator score (0.820 - 0.956 Cohen’s Kappa Score) falling under the categories - Humour/Satire, Opposition/Criticism, Support/Advocacy, and Informational/Analysis. We consider three modalities in our dataset - textual (title and description of the video), visual (thumbnail) and audio (MFCC coefficients and additional spectral and temporal features) modalities. We also perform preliminary classification on our dataset using an early fusion multimodal model, combining audio, visual and textual modalities, which performs better than other unimodal and bimodal approaches, yielding a Macro-F1 score of 0.8742 and ROC-AUC score of 0.769. By introducing this novel dataset, we aim to stimulate further investigation within the domains of opinion dissemination across social networks and the analysis of multimodal content, especially within the Indian context. Keywords Multimodal Analysis, Political Analysis, Social Media Analysiss The consumption of content on social media sites, like YouTube, has grown manifold over the last decade. This has led to an exponential rise in the creation of short-form and long-form content on various topics, ranging from comedic videos to documentaries. In this study, we analyzed one such content creation topic, political videos uploaded by independent Indian content creators. We annotated around 400 videos collected from YouTube for different socially and politically relevant labels. We performed a content analysis on our annotated dataset using BERTopic for topic modelling and YAKE! for keyword extraction. We also applied an early fusion multimodal model on the features extracted using state-of-the-art backbone representations, namely MuRIL for text, ConvNeXT for images, and MCFF, ZCR, Spectral Bandwidth, Chroma STFT and Spectral Rolloff for audio. Our classification model yielded a Macro-F1 score of 0.8742. Compared to other unimodal and bimodal models, the early fusion model yielded significantly better results. 6. Future Work Future work that focuses on a number of important areas of development will raise the calibre and scope of this research. Here are some directions we want to go in: 1. Experimentation with other fusion models In this paper, we used an early fusion model, combining modalities before classification. However, there are alternative fusion techniques that warrant exploration, such as late fusion models, where each modality is processed independently before being integrated with other modalities, and attention- based fusion models where the importance of different modalities is assessed with respect to the task at hand, or ensemble models, which combines the strengths of multiple prediction models to improve results. 2. Audio feature extraction The features extracted for audio in this study are numerical metrics that regrettably fail to capture the nuances of speech, especially code-mixed Hindi-English speech, which is a predominant mode of communication in India. Experi- menting with other audio feature extraction methods, for example, using transcripts to capture semantic meaning, Mel-frequency spectrograms to capture phonetic variation or transformer-based models that are distinguished for their contextual understanding can offer more sophisticated results. 3. Extending the dataset We chose to annotate data collected for five tasks for the purposes of this study. However, the methods of classification and analysis can be extended to include even more relevant labels that cover more NLP and discourse analysis tasks. This includes the detection of hate speech towards marginalised communities veiled as opinions, misinformation and fake news detection or the spread and polarization of public opinions over time. 4. Multilingual and cross-regional support While our selection procedure primarily focused on YouTube channels that offered content in Hindi or English, it’s important to recognise that a more inclusive approach is necessary for a thorough representation of India’s political environment. To adequately capture the complex and varied political narratives that arise across the nation’s various linguistic and cultural realms, region- specific content must be included."
Machine Learning,Divya Chaudhary,https://www.khoury.northeastern.edu/people/divya-chaudhary/,"Divya Chaudhary is an assistant teaching professor in the Khoury College of Computer Sciences at Northeastern University, based in Seattle.",Artificial intelligence; Data science; Machine learning; Natural language processing and information retrieval; Personal health informatics,"PhD in Computer Engineering, University of Delhi — India; MTech in Computer Science, Maharshi Dayanand University — India; BTech in Information Technology, Maharshi Dayanand University — India","September 18th, 2023",Detection of Sexism on Social Media with Multiple Simple Transformers,https://ceur-ws.org/Vol-3497/paper-082.pdf," Chirayu Jhakal, Khushi Singal, Manan Suri, Divya Chaudhary, Bijendra Kumar, Ian Gorton. (2023). Detection of Sexism on Social Media with Multiple Simple Transformers CLEF (Working Notes), 959-966. https://ceur-ws.org/Vol-3497/paper-082.pdf","Social media platforms have become virtual communication channels, allowing users to voice their thoughts and opinions. However, this openness and features of anonymity have also given rise to the proliferation of harmful and offensive content, including sexism. This research aims at proposing a methodology and explores the use of different simple transformers. Monolingual Simple Transformers such as BERT, RoBERTa[1], BERTweet, DistilBERT, XLNet were evaluated on the EXIST2023 shared task challenge at the IberLEF2023 dataset. It was observed that RoBERTa has given the best results among all other transformers. The proposed approach has great scope for the efficient detection of sexist content on social media, aiding in the development of effective content moderation systems. Keywords Sexism Detection, Simple Transformer Models, Natural Language Processing,, our study highlights the effectiveness of pre-trained transformer models in detecting sexism in tweets, both in English and Spanish. The results demonstrate the importance of using language-specific models and models designed for social media data to achieve higher accuracy. This research contributes to the development of automated systems for identifying and addressing sexism in online communication, ultimately fostering a more inclusive and respectful digital environment. 6. Conclusion In this research, we investigated the detection of sexism in tweets using pre-trained transformer models for both English and Spanish languages. Our results demonstrate that these models can be effective in identifying instances of sexism in social media content. The BERTweet model performed well in capturing the nuances of English tweets, while the CamemBERT model showed promise for Spanish tweets. Additionally, the XLNet model exhibited superior performance among the English models, highlighting the effectiveness of permutation-based approaches. However, it is important to note that the accuracies achieved, especially for Spanish models, can still be improved. The findings of this study have implications for developing automated systems that can detect and mitigate sexism in online communication. By leveraging pre-trained transformer models, we can gain insights into the prevalence of sexism and take steps toward fostering a more inclusive and respectful digital environment. 7. Future Work While this research provides valuable insights into the detection of sexism in tweets, there are several avenues for future work that can enhance the accuracy and robustness of the models. Firstly, data augmentation techniques can be employed to improve model performance [3] [4] [5] [6]. By increasing the diversity and quantity of training data through techniques such as back-translation, word replacement, or text synthesis, we can potentially reduce the model’s bias and enhance its ability to detect subtle forms of sexism. Secondly, ensemble modeling can be explored to leverage the strengths of multiple models and improve overall performance. By combining predictions from different models, either by majority voting or weighted averaging, we can potentially achieve higher accuracy and mitigate the limitations of individual models. Furthermore, it is important to expand the evaluation of sexism detection models to different languages and cultural contexts. The linguistic characteristics and contextual nuances can significantly vary across languages, necessitating the development of language-specific models and datasets. Additionally, further research should focus on addressing the issue of bias in the models. It is crucial to identify and mitigate any biases encoded in the pre-trained models to ensure fair and equitable detection of sexism. Finally, it would be beneficial to conduct user studies and assess the real-world impact of automated systems in addressing sexism in online spaces. Understanding user perceptions, reactions, and potential ethical concerns will guide the development of more effective and responsible solutions. By pursuing these avenues, we can advance the field of sexism detection in social media and contribute to the development of robust and inclusive technologies."
Machine Learning,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","December 8th, 2019",Deep Supervised Summarization: Algorithm and Application to Learning Instructions,https://khoury.northeastern.edu/home/eelhami/publications/SupFL_NeurIPS19.pdf," Deep Supervised Summarization: Algorithm and Application to Learning Instructions C. Xu and E. Elhamifar, Neural Information Processing Systems (NeurIPS), 2019.","We address the problem of ﬁnding representative points of datasets by learning from multiple datasets and their ground-truth summaries. We develop a supervised subset selection framework, based on the facility location utility function, which learns to map datasets to their ground-truth representatives. To do so, we propose to learn representations of data so that the input of transformed data to the facility location recovers their ground-truth representatives. Given the NP-hardness of the utility function, we consider its convex relaxation based on sparse representation and investigate conditions under which the solution of the convex optimization recovers ground-truth representatives of each dataset. We design a loss function whose minimization over the parameters of the data representation network leads to satisfying the theoretical conditions, hence guaranteeing recovering ground- truth summaries. Given the non-convexity of the loss function, we develop an efﬁcient learning scheme that alternates between representation learning by mini- mizing our proposed loss given the current assignments of points to ground-truth representatives and updating assignments given the current data representation. By experiments on the problem of learning key-steps (subactivities) of instruc- tional videos, we show that our proposed framework improves the state-of-the-art supervised subset selection algorithms. 1s We addressed the problem of supervised subset selection by generalizing the facility location to learn from ground-truth summaries. We considered an efﬁcient sparse optimization of the uncapacitated facility location and investigated conditions under which it recovers ground-truth representatives and also becomes equivalent to the original NP-hard problem. We designed a loss function and an efﬁcient framework to learn representations of data so that the input of transformed data to the facility location satisﬁes the theoretical conditions, hence, recovers ground-truth summaries. We showed the effectiveness of our method for recovering key-steps of instructional videos. To the best of our knowledge, this is the ﬁrst work on supervised subset selection that derives conditions under which subset selection recovers ground-truth representatives and employs them to design a loss function for deep representation learning. We believe that this work took a major step towards a theoretically motivated supervised subset selection framework."
Machine Learning,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","October 27th, 2019",Unsupervised Procedure Learning via Joint Dynamic Summarization,http://www.ccs.neu.edu/home/eelhami/publications/ICCV19-ProceL-Ehsan.pdf," Unsupervised Procedure Learning via Joint Dynamic Summarization. E. Elhamifar and Z. Naing, International Conference on Computer Vision (ICCV), 2019.","We address the problem of unsupervised procedure learning from unconstrained instructional videos. Our goal is to produce a summary of the procedure key-steps and their ordering needed to perform a given task, as well as localization of the key-steps in videos. We develop a col- laborative sequential subset selection framework, where we build a dynamic model on videos by learning states and transitions between them, where states correspond to dif- ferent subactivities, including background and procedure steps. To extract procedure key-steps, we develop an opti- mization framework that ﬁnds a sequence of a small number of states that well represents all videos and is compatible with the state transition model. Given that our proposed optimization is non-convex and NP-hard, we develop a fast greedy algorithm whose complexity is linear in the length of the videos and the number of states of the dynamic model, hence, scales to large datasets. Under appropriate condi- tions on the transition model, our proposed formulation is approximately submodular, hence, comes with performance guarantees. We also present ProceL, a new multimodal dataset of 47.3 hours of videos and their transcripts from diverse tasks, for procedure learning evaluation. By exten- sive experiments, we show that our framework signiﬁcantly improves the state of the art performance.We developed a joint dynamic summarization method and a fast greedy algorithm for unsupervised procedure learning. Our method handles repeated key-steps, back- ground and missing or additional key-steps in videos. We presented ProceL, a new multimodal dataset for procedure learning. We showed our method signiﬁcantly improves the state of the art performance and showed the effectiveness of summarization tools, in general, for procedure learning."
Machine Learning,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","June 16th, 2019",Facility Location: Approximate Submodularity and Greedy Algorithm,http://www.ccs.neu.edu/home/eelhami/publications/SeqFL_ICML19.pdf," Facility Location: Approximate Submodularity and Greedy Algorithm, E. Elhamifar, International Conference on Machine Learning (ICML), 2019.","We develop and analyze a novel utility function and a fast optimization algorithm for subset se- lection in sequential data that incorporates the dynamic model of data. We propose a cardinality- constrained sequential facility location function that ﬁnds a ﬁxed number of representatives, where the sequence of representatives is compatible with the dynamic model and well encodes the data. As maximizing this new objective function is NP- hard, we develop a fast greedy algorithm based on submodular maximization. Unlike the con- ventional facility location, the computation of the marginal gain in our case cannot be done by oper- ations on each item independently. We exploit the sequential structure of the problem and develop an efﬁcient dynamic programming-based algorithm that computes the marginal gain exactly. We in- vestigate conditions on the dynamic model, under which our utility function is (ε-approximately) submodualr, hence, the greedy algorithm comes with performance guarantees. By experiments on synthetic data and the problem of procedure learning from instructional videos, we show that our framework signiﬁcantly improves the compu- tational time, achieves better objective function values and obtains more coherent summaries.s We proposed a utility function and a fast greedy algorithm for subset selection in sequential datasets, taking advantage of the dynamic model of data. We proved that under appro- priate conditions on transition dynamics, our utility function is ε-approximately submodular, hence, enjoys approximate guarantees via the greedy method. By experiments on syn- thetic and real data, we showed the effectiveness of our method in terms of running time and attained objective val- ues as well as addressing the procedure learning task. Greedy Sequential Facility Location"
Machine Learning,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","December 4th, 2017",Subset Selection and Summarization in Sequential Data,http://www.ccs.neu.edu/home/eelhami/publications/SeqSS-NIPS17-Ehsan.pdf," E. Elhamifar and M. C. De Paolis Kaluza; Neural Information Processing Systems (NIPS), 2017.","Subset selection, which is the task of ﬁnding a small subset of representative items from a large ground set, ﬁnds numerous applications in different areas. Sequential data, including time-series and ordered data, contain important structural relation- ships among items, imposed by underlying dynamic models of data, that should play a vital role in the selection of representatives. However, nearly all existing subset selection techniques ignore underlying dynamics of data and treat items independently, leading to incompatible sets of representatives. In this paper, we develop a new framework for sequential subset selection that ﬁnds a set of represen- tatives compatible with the dynamic models of data. To do so, we equip items with transition dynamic models and pose the problem as an integer binary optimization over assignments of sequential items to representatives, that leads to high encoding, diversity and transition potentials. Our formulation generalizes the well-known facility location objective to deal with sequential data, incorporating transition dynamics among facilities. As the proposed formulation is non-convex, we derive a max-sum message passing algorithm to solve the problem efﬁciently. Experiments on synthetic and real data, including instructional video summarization, show that our sequential subset selection framework not only achieves better encoding and diversity than the state of the art, but also successfully incorporates dynamics of data, leading to compatible representatives. 1s and Future Work We developed a new framework for sequential subset selection that takes advantage of the underlying dynamic models of data, promoting to select a set of representatives that are compatible according to the dynamic models of data. By experiments on synthetic and real data, we showed the effectiveness of our method for summarization of sequential data. Our ongoing research include development of fast greedy algorithms for our sequential subset selection formulation, investigation of the theoretical guarantees of our method, as well as development of more effective summarization-based feature extraction techniques and working with larger datasets for the task of instructional data summarization. 9"
Machine Learning,Ehsan Elhamifar,https://www.khoury.northeastern.edu/people/ehsan-elhamifar/,"Ehsan Elhamifar is an associate professor and director of the master’s in AI program in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is affiliated with the College of Engineering.",Artificial intelligence; Computer vision; Machine learning,"PhD in Electrical and Computer Engineering, Johns Hopkins University; MS in Engineering, Johns Hopkins University; MS in Electrical Engineering, Sharif University of Technology — Iran; BS in Biomedical Engineering, Amirkabir University of Technology — Iran","July 21st, 2017",Online Summarization via Submodular and Convex Optimization,http://www.ccs.neu.edu/home/eelhami/publications/onlineSS_CVPR17-Ehsan.pdf," E. Elhamifar and M. C. De Paolis Kaluza  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.","We consider the problem of subset selection in the online setting, where data arrive incrementally. Instead of stor- ing and running subset selection on the entire dataset, we propose an incremental subset selection framework that, at each time instant, uses the previously selected set of repre- sentatives and the new batch of data in order to update the set of representatives. We cast the problem as an integer bi- nary optimization minimizing the encoding cost of the data via representatives regularized by the number of selected items. As the proposed optimization is, in general, NP-hard and non-convex, we study a greedy approach based on un- constrained submodular optimization and also propose an efﬁcient convex relaxation. We show that, under appropri- ate conditions, the solution of our proposed convex algo- rithm achieves the global optimal solution of the non-convex problem. Our results also address the conventional problem of subset selection in the ofﬂine setting, as a special case. By extensive experiments on the problem of video summa- rization, we demonstrate that our proposed online subset selection algorithms perform well on real data, capturing diverse representative events in videos, while they obtain objective function values close to the ofﬂine setting.s We studied the problem of subset selection in the online setting, where data arrive incrementally. We proposed an incremental subset selection framework that, at each time instant, uses the previously selected set of representatives and the new batch of data in order to update the set of representatives. We cast the problem as an integer binary optimization minimizing the encoding cost of the data via representatives regularized by the number of selected items. We studied a randomized greedy approach based on uncon- strained submodular optimization and proposed a convex algorithm with theoretical performance guarantees. By ex- periments on real videos, we demonstrated the effectiveness of our methods for online video summarization."
Machine Learning,Huy Lê Nguyen,https://www.khoury.northeastern.edu/people/huy-le-nguyen/,"Huy Lê Nguyen is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Algorithmic techniques for massive data sets; Optimization; Machine learning,"PhD in Computer Science, Princeton University; MEng in Computer Science, Massachusetts Institute of Technology; BS in Computer Science and Mathematics, Massachusetts Institute of Technology","October 31st, 2022",Improved Learning-augmented Algorithms for k-means and k-medians Clustering,https://openreview.net/pdf?id=dCSFiAl_VO3," Thy Dinh Nguyen, Anamay Chaturvedi, Huy L. Nguyen. (2023). Improved Learning-augmented Algorithms for k-means and k-medians Clustering ICLR. https://openreview.net/pdf?id=dCSFiAl_VO3","We consider the problem of clustering in the learning-augmented setting. We are given a data set in d-dimensional Euclidean space, and a label for each data point given by a predictor indicating what subsets of points should be clustered together. This setting captures situations where we have access to some auxiliary informa- tion about the data set relevant for our clustering objective, for instance the labels output by a neural network. Following prior work, we assume that there are at most an α ∈(0, c) for some c < 1 fraction of false positives and false negatives in each predicted cluster, in the absence of which the labels would attain the optimal clustering cost OPT. For a dataset of size m, we propose a deterministic k-means algorithm that produces centers with an improved bound on the clustering cost compared to the previous randomized state-of-the-art algorithm while preserv- ing the O(dm log m) runtime. Furthermore, our algorithm works even when the predictions are not very accurate,conclusion not found"
Machine Learning,David Smith,https://www.khoury.northeastern.edu/people/david-smith/,"David A. Smith is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is a founding member of the NULab for Texts, Maps, and Networks, Northeastern University’s center for the digital humanities and computational social sciences.","Efficient inference for machine learning models with complex latent structure; Modeling natural language structures, such as morphology, syntax, and semantics; Modeling the mutations in texts as they propagate through social networks and in language across space and time; Interactive information retrieval and machine learning for expert users","PhD in Computer Science, Johns Hopkins University; BA in Classics, Harvard University","December 6th, 2023",Automatic Collation for Diversifying Corpora: Commonly Copied Texts as Distant Supervision for Handwritten Text Recognition,https://ceur-ws.org/Vol-3558/paper1708.pdf," David A. Smith, Jacob Murel, Jonathan Parkes Allen, Matthew Thomas Miller. (2023). Automatic Collation for Diversifying Corpora: Commonly Copied Texts as Distant Supervision for Handwritten Text Recognition CHR, 206-221. https://ceur-ws.org/Vol-3558/paper1708.pdf","Handwritten text recognition (HTR) has enabled many researchers to gather textual evidence from the human record. One common training paradigm for HTR is to identify an individual manuscript or coherent collection and to transcribe enough data to achieve acceptable performance on that collection. To build generalized models for Arabic-script manuscripts, perhaps one of the largest textual traditions in the pre-modern world, we need an approach that can improve its accuracy on unseen manuscripts and hands without linear growth in the amount of manually annotated data. We propose Automatic Collation for Diversifying Corpora (ACDC), taking advantage of the existence of multiple manuscripts of popular texts. Starting from an initial HTR model, ACDC automatically detects matching passages of popular texts in noisy HTR output and selects high-quality lines for retraining HTR without any manually annotated data. We demonstrate the e昀昀ectiveness of this approach to distant supervision by annotating a test set drawn from a diverse collection of 59 Arabic-script manuscripts and a training set of 81 manuscripts of popular texts embedded within a larger corpus. A昀琀er a few rounds of ACDC retraining, character accuracy rates on the test set increased by 19.6% absolute percentage, while a supervised model trained on manually annotated data from the same collection increased accuracy by 15.9%. We analyze the variation in ACDC’s performance across books and languages and discuss further applications to collating manuscript families. Keywords handwritten text recognition, collation, manuscriptss, or recommendations expressed do not necessarily re昀氀ect those of the NEH or Mellon."
Machine Learning,David Smith,https://www.khoury.northeastern.edu/people/david-smith/,"David A. Smith is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is a founding member of the NULab for Texts, Maps, and Networks, Northeastern University’s center for the digital humanities and computational social sciences.","Efficient inference for machine learning models with complex latent structure; Modeling natural language structures, such as morphology, syntax, and semantics; Modeling the mutations in texts as they propagate through social networks and in language across space and time; Interactive information retrieval and machine learning for expert users","PhD in Computer Science, Johns Hopkins University; BA in Classics, Harvard University","December 6th, 2023",Testing the Limits of Neural Sentence Alignment Models on Classical Greek and Latin Texts and Translations,https://ceur-ws.org/Vol-3558/paper6193.pdf," Caroline Craig, Kartik Goyal, Gregory R. Crane, Farnoosh Shamsian, David A. Smith. (2023). Testing the Limits of Neural Sentence Alignment Models on Classical Greek and Latin Texts and Translations CHR, 530-553. https://ceur-ws.org/Vol-3558/paper6193.pdf","The Greek and Latin classics, like many other ancient texts, have been widely translated into a variety of languages over the past two millennia. Although many digital editions and libraries contain one or two translations for a given text, about one hundred translations of the Iliad and twenty of Herodotus, for ex- ample, exist in English alone. Aligning the corpus of classical texts and translations at the sentence and word level would provide a valuable resource for studying translation theory, digital humanities, and natural language processing (NLP). Precise and faithful sentence alignment via computational methods, however, remains a challenging problem. Current alignment methods tend to have poor coverage and recall since their primary aim is to extract single sentence pairs for training machine translation systems. This paper evaluates and examines the limits of such state-of-the-art models for cross-language sentence embedding and alignment of ancient Greek and Latin texts with translations into English, French, Ger- man, and Persian. We release evaluation data for Plato’s Crito, manually annotated at the word and sentence level, and larger test datasets based on coarser structural metadata for Thucydides (Greek) and Lucretius (Latin). Testing LASER and LaBSE for sentence embedding and nearest-neighbor retrieval and Vecalign for sentence alignment, we found best results using LaBSE-Vecalign. LaBSE worked sur- prisingly well on ancient Greek, most probably because it had been merged with modern Greek data in its training. Both LASER-Vecalign and LaBSE-Vecalign did best when there were many ground-truth one-to-one alignments between source and target sentences, and when the order of sentences in the source was preserved in the translation. However, these conditions are o昀琀en not present in the kinds of literary and free translation we wish to study, nor in editions with multiple translations, extensive commentary, or other paratext. We perform book-level and chapter-level error analysis to inform the development of a so昀琀ware pipeline that can be deployed on the vast corpus of translations of ancient texts. Keywords sentence alignment, multilingual embedding, machine translation, Ancient Greek, Latin 530as a possible direction for future work. 3.2. Sentence Alignment For sentence alignment, we relied on Vecalign [25], which remains the state-of-the-art model though it was published in 2019. Vecalign computes a昀케nity scores between sentences or groups of sentences from a source and target text. The algorithm takes in sentence embed- dings as input and then uses these embeddings to assess the similarity of sentences. Then, it reduces the problem of alignment to enumeration over all possible pairs of groups of sentences across the source and target texts. This process yields highest scoring pairs, resulting in either one-to-one, one-to-many, many-to-one, or many-to-many aligned sentence pairs. This enu- meration is exponentially expensive in terms of the size of the bitext (pair of source and target texts). Therefore, a dynamic program is used to perform it e昀케ciently. Further approximations are made to reduce the runtime by incorporating inductive biases and modeling assumptions, such as the largely monotonic nature of sentence alignment across the bitext. Another source of drastic reduction in runtime is a coarse-to-昀椀ne approach employed for alignment, which 533 prunes the search space of sentence pairs aggressively by making severe contiguity and mono- tonicity assumptions. These approximations and assumptions reduce the runtime to being asymptotically linear in terms of the size of the corpus. However, as we show in our experi- ments, these assumptions might not necessarily hold for our task. The texts we are interested in exhibit signi昀椀cant non-monotonicity, discontiguousness, and noise in the form of extrane- ous material (paratext) that is interspersed throughout the content sentences that actually align across the bitext. Our 昀椀ndings point toward future research on better alignment algorithms which make fewer of these unrealistic assumptions for our task while remaining practical to execute. 4. Dataset All ancient Greek and Latin texts were extracted from The Perseus Project [23] (Perseus) and contain no paratext. The translations varied in terms of language, sources, formats, available annotations, and o昀琀en included paratext as detailed in Table 1. The texts are further described by annotation level, since this impacted their use in di昀昀erent experiments. We aimed to test our pipeline on a varied set of texts, including di昀昀erent styles (dialogue, poetry, prose) and level of noisiness (texts with and without paratext). Therefore, we used available annotations, leading some experiments to be evaluated at the chunk level (Table 2), sentence level (Table 3), or at coarser levels (chapter or book: Table 4). The one exception is the small test set (Table 3) that the authors of this paper manually annotated at the sentence level. We used this test set at the outset of our project for rapid testing in order to inform next steps. Details on the paratext present in the two noisy texts can be found in Table 4. 4.1. Preprocessing: Chunk-Level and Sentence Segmentation When we refer to “chunk-level,” we mean the most 昀椀ne-grained citation structure available on Perseus, such as chapters, sections, or Stephanus pages.4 By “sentence-level,” we mean the phrases obtained a昀琀er sentence segmentation, explained below. Any additional pre- and post-processing is listed in the appendix. Table 2 lists all texts which were previously annotated at the chunk level. Preprocessing on these texts was limited to concatenating the chunks into a continuous string. For the sentence- level experiments (Tables 3, 4), we applied standard preprocessing to all texts: concatenated the raw text into one string, then segmented into sentences. For languages supported by Stanza [18] (Latin, English, French), we 昀椀rst split the text into Stanza’s sentences, then split further on semi-colons and colons. For unsupported languages (Ancient Greek), we segmented ourselves by splitting on periods, semi-colons, and colons. 4Stephanus pagination refers to the page breaks used in modern editions and translations of the works of Plato. They were 昀椀rst established by a 1578 edition published by Henri Estienne, also known as Henricus Stephanus. 534 Table 1 Summary of Texts Used Work Translator Language Paratext Annotation Annotator Crito (Source) Greek no section Perseus Fowler English no section student Jowett English no section student Schleiermacher German no section student Mohammadi (student) Farsi no section student Thucydides (Source) Greek no section Perseus Crawley English no section Perseus Bétant French yes chapter OGL Lucretius (Source) Latin no Perseus card Perseus Watson, Good English yes book OGL (2 translations in 1 volume) Table 2 Datasets for Chunk-Level Evaluations: Chunks are sections of Stephanus pages (Crito) or sections (Thucydides). We report space-separated tokens for comparability across languages. Work Translator # Chunks Avg. Tok./Chunk Std. Dev. Crito (Source) 268 15.96 14.73 Fowler 267 21.29 19.99 Jowett 259 20.59 18.85 Schleiermacher 267 20.70 19.72 Mohammadi 267 21.42 20.57 Thucydides (Source) 3575 41.96 19.77 Crawley 3575 56.52 27.96 Table 3 Small Test Set: Dataset for Sentence-Level Evaluation Work Translator # Sentences Avg. Tok./Sent. Std. Dev. Annotator Crito (Source) 60 18.08 16.65 Paper authors Fowler 66 22.05 18.10 Paper authors Jowett 79 17.61 14.66 Paper authors 4.2. Annotations 4.2.1. Crito Students involved in a project at Lepizig University annotated thirteen translations of Crito (including two in English, one in German, and 昀椀ve in Persian), to match the chunk-level anno- tation of the Greek text in Perseus. In our experiments, we used both English translations (by Harold North Fowler [15] and Benjamin Jowett [16]), the one German translation (by Schleier- 535 Table 4 Noisy Data: Dataset for Sentence-Level Experiments with Coarse-Level Evaluation Work Translator # Sent. Avg.Tok./Sent. Std. Dev. Text Sent. Paratext Sent. Thucydides (Source) 6097 24.63 16.77 6097 0 Bétant 17203 14.23 10.51 10958 6245 Lucretius (Source) 2428 20.20 13.14 3575 0 Watson & Good 14648 13.83 11.61 8521 6127 Watson only 9815 15.82 13.40 3936 5879 Good only 4833 11.00 7.59 4584 248 macher [17]), and one of the Persian translations (by Mohammadi, published on Zenodo [22]). This Persian translation was done by one of the student annotators.5 4.2.2. The Open Greek and Latin Project (OGL) The two noisy translations in our dataset were annotated by The Open Greek and Latin Project. These texts are available to the public in XML 昀椀les that include varying levels of annotation.6 For the Thucydides French translation by Bétant [26], annotations include tags for paratext and book and chapter boundaries. For the Lucretius edition, which contains two English trans- lations by Rev. John Selby Watson and John Mason Good [13], annotations include tags for paratext and book boundaries. 4.3. Noisy Data Features Both the Thucydides (fr) and Lucretius translations include paratext. In Thucydides, paratext consists of a foreword; commentary and summary of contents preceding each of the eight books; 33 footnotes; and an index. In the Lucretius edition, paratext consists of a foreword, a commentary before the prose translation, 1426 notes including 955 footnotes interspersed through the text (70 in the foreword, 861 in the Watson translation, 24 in the Good translation), and an index. The Lucretius edition contains additional noise in the form of two translations included in one edition. 5. Experiments Our experiments are summarized in Table 5. To guide the direction of our research, we 昀椀rst ran our two candidate pipelines, LASER-Vecalign and LaBSE-Vecalign, on the Crito test set. We then validated these results with retrieval experiments on our full dataset, using available annotations. Lastly, we tested our best pipeline, LaBSE-Vecalign, on noisy data with a focus 5The student translators used treebanks, commentaries, lexicon entries, and English and German translations to aid their Persian translations. They also aligned their Persian translations at the word level to the Greek, using Ugarit (Mohammadi’s can be found here: https://ugarit.ialigner.com/userProfile.php?userid=52434&tgid=9362). 6https://github.com/OpenGreekAndLatin 536 Table 5 Summary of Experiments Goal Experiments Texts Used Annotation Evaluation Initial rapid LASER-Vecalign & Crito test set Sentence Scoring Functions testing LaBSE-Vecalign (see 5.3) LASER-Vecalign & Crito Chunk Recall LaBSE-Vecalign (Fowler, Jowett) Validate initial Chunk retrieval Crito (all), Chunk Recall results Thucydides (en) Sentence retrieval Crito test set Sentence Recall Sentence retrieval Thucydides (fr), Sentence Recall (coarse) Lucretius (en) Test best pipeline LaBSE-Vecalign Thucydides (fr), Sentence Recall (coarse) on noisy data Lucretius (en) on error analysis to understand how the pipeline would fare on the type of unannotated data that we would like to align using our pipeline. 5.1. Experimental Set-Up To run the retrieval experiments, we 昀椀rst passed pre-processed source and target texts seg- mented into chunks or sentences through an embedding model (LASER or LaBSE). Then we used cosine similarity to retrieve the most similar chunks or sentences across the bitext pairs. For chunk-level retrieval experiments, we le昀琀out from the source text any chunks with missing translations (hence the di昀昀erent number of chunks across translations in Table 7). For LASER-Vecalign and LaBSE-Vecalign, we passed the chunk- or sentence-level embed- dings into Vecalign, which outputs a set of predicted alignments. These may include one-to- one, one-to-many, many-to-one, or many-to-many alignments. 5.2. Evaluation For chunk-level retrieval, we report the percentage of source chunks with the correct result among the top 1 (correct result has the highest similarity score) and top 10 similarity scores. For sentence-level retrieval, we modify the numerator and denominator to account for one- to-many sentence alignments in the ground truth: for every Greek sentence, we compute the number of correct sentences retrieved divided by the number of target sentences in the true alignment. For LASER-Vecalign and LaBSE-Vecalign, we evaluated results based on available annota- tions. For the Crito test set, where we manually produced sentence-level ground truth, we used Vecalign’s scoring functions and two new functions we formulated, described below. For chunk alignment experiments, where the ground truth is a straightforward list of one-to-one alignments, we report the percentage of incorrect predictions. Finally, for LaBSE-Vecalign on 537 the two noisy texts, Thucydides (fr) and Lucretius (en), we evaluate the pipeline’s predictions at the coarse level of annotations available in the Open Greek and Latin Project’s database. These experiments were run on segmented sentences but we report accuracy relative to a predicted sentence belonging to the same chapter (Thucydides) or book (Lucretius) as the source text. 5.3. Scoring Functions For Vecalign Predictions Vecalign’s original scoring function reports strict and lax scores for Precision, Recall, and F1. The lax metric expands the de昀椀nition of true positives to include any correct sentence align- ment in a one-to-many or many-to-many prediction. A strict true positive requires exact matches between a ground-truth alignment and a prediction. When performing our initial rapid testing, we formulated two additional metrics. Both are based on post-processing Ve- calign’s results by merging predicted alignments to try to reconstitute Perseus sections. We used this annotation level as ground truth because Perseus sections are examples of the avail- able annotations applied to ancient texts by editorial convention. Therefore, we sought to determine how the pipelines would perform at this challenging level. If a昀琀er merging there’s a strict match, then this is a true positive under the “New Strict Scoring” function. In the “New Lax Scoring” function, we instead look for lax matches: for any sentence that appears on both sides of the reconstituted alignment that is in a Perseus section, true positives are increased by one. 6. Results 6.1. LaBSE Outperformed LASER LaBSE outperformed LASER in our initial testing of the candidate pipelines, LASER-Vecalign and LaBSE-Vecalign. This was repeated in our validation experiments (chunk-alignment, chunk-retrieval, and sentence-retrieval). When used in conjunction with Vecalign, LaBSE consistently outperformed LASER (Table 6). Unsurprisingly, both pipelines did best under the New Lax Scoring function. Since this metric gives credit for (counts as true positive) any correct sentence matching between source and target text, it most approximates the pipelines’ ability to correctly align sentences. Therefore, at the sentence-level, the results on clean data with the Crito were overall very promising. LaBSE’s and LASER’s worst results were with the Strict Scoring function, con昀椀rming our hy- pothesis that existing pipelines would struggle to return correct alignments at the more chal- lenging annotation level that we 昀椀nd in ancient texts and their translations. Results for both pipelines at the chunk level are in Table 12 in the appendix (LaBSE-Vecalign aligned all chunks correctly, while LASER-Vecalign aligned 98.88% of Greek - Fowler chunks and 94.98% of Greek - Jowett chunks correctly). In the retrieval experiments, LASER struggled most when retrieving chunks from the En- glish Thucydides translation, the longest text on which we tested LASER (Table 7). LASER’s performance improved when retrieving sentences from the Crito test set (Table 8), in other words when retrieving shorter spans of text from a shorter document. We do not see simi- lar di昀昀erence with LaBSE’s performance on shorter text spans and documents, with its scores 538 Table 6 LASER vs. LaBSE: Sentence alignment with Vecalign from the ancient Greek (G) of Plato’s Crito to the English translations of Fowler (F) and Jowett (J) and between English translations G–F G–F G–J G–J F–J F–J Embedding Model Used LabSE LASER LabSE LASER LabSE LASER Vecalign Strict Scoring Precision - Strict 0.737 0.655 0.600 0.414 0.536 0.472 Recall - Strict 0.778 0.704 0.623 0.453 0.577 0.481 F1 - Strict 0.757 0.679 0.611 0.432 0.556 0.476 New Strict Scoring Accuracy 0.870 0.796 0.685 0.500 0.712 0.577 New Lax Scoring Precision - Lax 0.984 0.944 0.950 0.791 0.972 0.966 Recall - Lax 0.827 0.778 0.805 0.647 0.727 0.714 F1 - Lax 0.899 0.853 0.871 0.712 0.832 0.821 Table 7 LASER vs. LaBSE: Chunk-Level retrieval from ancient Greek to translations of Crito (Cr.) and Thucy- dides (Thuc.) Fowler Jowett Schleiermacher Mohammadi Crawley (Cr., en) (Cr., en) (Cr., de) (Cr., fa) (Thuc., en) Num. Chunks 267 259 267 267 3575 LaBSE Top 1 75.66% 57.92% 78.65% 71.91% 79.61% Top 10 93.26% 83.40% 94.01% 87.27% 93.15% LASER Top 1 33.71% 14.67% 34.46% 23.60% 3.05% Top 10 67.42% 46.72% 68.16% 44.94% 8.98% at the sentence level slightly lower yet still comparable to those at the chunk level (Table 7, Table 8). 6.2. Performance Di昀昀erences on Clean Data LaBSE and LASER both exhibited performance di昀昀erences across translations of the Crito in experiments using the test set and the full text. With the test set, LASER-Vecalign and LaBSE- Vecalign did best aligning the Greek to Fowler’s more literal translation (Table 6), where there are the most one-to-one alignments in the ground truth (70% compared to 48% for Greek - Jowett and Fowler - Jowett). Likewise, both LaBSE and LASER had higher scores retrieving sentences between the Greek and Fowler than between the Greek and Jowett. Interestingly, both models perform better retrieving sentences between Fowler and Jowett (“F-J” in Table 8) than when used in conjunction with Vecalign (“F-J” in Table 6). At the chunk level, LaBSE 539 Table 8 LASER vs. LaBSE: Sentence-Level retrieval from the ancient Greek (G) of Plato’s Crito to the English translations of Fowler (F) and Jowett (J) and between English translations G–F G–J F–J LaBSE Top 1 76.67% 51.39% 73.58% Top 10 90.83% 82.54% 92.17% LASER Top 1 41.67% 30.83% 61.24% Top 10 67.50% 51.17% 78.34% Table 9 Sentence-Level retrieval Thucydides, Lucretius (noisy data) Thucydides (el - fr) Lucretius (la - en) (from same chapter) (from same book) LaBSE Top 1 36.49% 76.52% Top 10 60.70% 97.24% and LASER also did better on Fowler’s more literal translation in both chunk-level retrieval (Table 7) and chunk-level alignment (Table 12 in Appendix). 6.3. Challenges of Noisy Data Testing LaBSE and LaBSE-Vecalign on noisy data points to the challenges of the two noisy features present in our noisy dataset: the presence of paratext and multiple translations in the target document. LaBSE did worse retrieving sentences from the noisy Thucydides translation (60.70% in Top 10 in Table 9) than the English translation (93.15% in Top 10 in Table 7), even though the evaluation was done at a coarser level with the French. These experiments di昀昀er in three ways: the language of the target (French vs. English), the shorter text span in the sentence retrieval experiment, and the presence of paratext in the French. Given LaBSE’s comparable results with retrieval at the chunk and sentence level on the Crito, the presence of paratext is the likely reason for the performance di昀昀erence. At the coarser, book level we used to evaluate sentence retrieval with Lucretius, the presence of paratext no longer made a visible impact (Table 9). When used in conjunction with Vecalign, and also evaluated at the same coarse level, LaBSE- Vecalign did very well on Thucydides (fr), despite the presence of paratext. Thus 96.72% of paratext sentences correctly aligned to null and 94.04% of French text sentences were aligned to a Greek sentence from the same chapter (Table 10). However, on Lucretius 35.03% of paratext sentences were incorrectly aligned to null and 55.23% of English text sentences incorrectly aligned to null (Table 10). We also reported results using the number of Vecalign predictions as denominator, which can be found in Table 13 in the Appendix and show a similar pattern. 540 Table 10 LaBSE - Vecalign: By Number of Target Sentences, Thucydides (fr - el, chapter-level evaluation) and Lucretius (en - la, book-level evaluation) Type of Alignment Thucydides Lucretius (chapter-level) (book-level) As a percent of Paratext Sentences Number of paratext sentences 6245 6127 Paratext sents. to null (correct) 96.72% 64.97% Paratext sents. to source text sents. (incorrect) 3.28% 35.03% As a Percent of Target Text Sentences Number of text sentences 10958 8521 Text-to-text: to source sents. from same chapter or book 94.04% 44.24% Text-to-text: to at least 1 source sent. from same chapter or book 0.93% 0.49% Errors, text-to-text: to no source sents. from same chapter or book 4.93% 0.04% Errors, text-to-null 0.10% 55.23% Table 11 LaBSE - Vecalign: Lucretius (en - la) - No Paratext, First Translation Only (book-level evaluation) Type of Alignment Lucretius (book-level) As a Percent of Text Sents. from First Translation Only Number of Text Sentences 3648 Text-to-text: to source sents. from same book 97.97% Text-to-text: to at least one source sent. from same book 1.67% Errors, Text-to-text: to no source sent. from same book 0.00% Errors, Text-to-Null 0.36% In order to get results for Lucretius using LaBSE-Vecalign that are comparable to those on Thucydides, we had to suppress both paratext and the second translation (by Good) in the target edition. When we only suppressed paratext, 65.56% of Vecalign’s predictions were (incorrect) null-to-text alignments, and 55.21% of English text sentences were incorrectly aligned to null (Table 14 in the Appendix). When we only suppressed the second translation, 96.26% of text sentences were correctly aligned to Latin sentences from the same book, but 36.26% of paratext sentences were incorrectly aligned to Latin text sentences (Table 15 in the Appendix). Finally, when we suppressed paratext and only counted text sentences from the 昀椀rst translation (by Watson), 97.97% of English text sentences aligned to Latin sentences from the same book, and no English text sentences aligned to Latin sentences from a di昀昀erent book (Table 11). In other words, only a昀琀er preprocessing the English edition of Lucretius to obtain a clean dataset (no paratext, no second translation) was LaBSE - Vecalign able to achieve results comparable to those we saw with the French translation of Thucydides. Interestingly, when we compare against retrieval results in Table 9, LaBSE did better on its 541 own than it did with Vecalign on Lucretius, but worse on Thucydides. Thus using LaBSE em- beddings only, for 97.24% of the 2428 Latin sentences we retrieved English text sentences from the same book among the top 10 most similar sentences, while LaBSE-Vecalign aligned only 44.24% of the 8521 English text sentences to Latin sentences from the same book. On the other hand with Thucydides, we retrieved French text sentences from the same chapter among the top 10 most similar sentences for 60.70% of the 6097 Greek sentences using LaBSE embeddings only, while LaBSE-Vecalign aligned 94.04% the 10958 French text sentences to Greek sentences from the same chapter. These results indicate that Vecalign’s faulty assumption and inductive biases for approximate dynamic programming can override the signal from well-trained sen- tence representation methods and hurt the overall performance in the case of discontiguous noisy text. 7. Discussion Our experiments led us to identify LaBSE-Vecalign as the pipeline using existing models best suited to produce sentence-level alignments of Ancient Greek and Latin texts with their trans- lations. However, even with the clean data of the Crito, this pipeline evidenced di昀昀erences in performance across translations. This was even the case when aligning English to En- glish (Fowler-Jowett), with LaBSE-Vecalign showing better results aligning Greek-Fowler than Fowler-Jowett. The performance driver with Crito seems therefore to be the nature of the trans- lations themselves: the number of one-to-many and many-to-many alignments in the ground truth. The di昀昀erences in the two translations also emerge at the word level. We manually aligned the 昀椀rst half of the Crito’s words with Jowett’s translation on Ugarit [31] and com- pared to alignments with Fowler prepared by an author of Ugarit’s alignment guidelines [30]7. We were able to align 52% of Greek words with Jowett’s translation, compared to 80% with Fowler’s (Table 16 in the Appendix). Of these aligned Greek words, with Jowett 9% crossed the predicted sentence boundaries while none did with Fowler’s (Table 17 in the Appendix). With more realistic noisy data, LaBSE-Vecalign was not able to handle both the nature of the paratext in the Lucretius edition, and its inclusion of a second translation. The Lucretius edition not only has many footnotes, these are also lengthy. Thus the 昀椀rst two sentences of the translation in Lucretius (following 461 sentences of the foreword and 23 sentences of commentary preceding book 1) are followed by 44 sentences of footnotes before we 昀椀nd the third text sentence. In contrast, Thucydides had fewer footnotes (33), also interspersed in the translated text but all short (the longest spans 2 sentences). Figures 1 and 2 show details of the LaBSE-Vecalign results aligning Thucydides and Lucretius with their French and English translations, respectively. In the Thucydides detail, the Greek sentences on the le昀琀are much longer than the French sentences; the 昀椀昀琀h Greek sentence (sec- tion 1.2.2) is translated over 昀椀ve sentences in French (last row of Figure 1). The 昀椀rst three rows are errors; the 昀椀rst two contain no overlapping sentences and the third includes the correct French sentences for section 1.1.3 as well as the French translations of about half of section 1.1.1 and all of section 1.1.2. There is no evident pattern explaining these errors. However, 7Word alignments with Jowett: https://ugarit.ialigner.com/userProfile.php?userid=126388&tgid=12065 and Fowler: https://ugarit.ialigner.com/userProfile.php?userid=3&tgid=8609 542 Figure 1: Behavior of LaBSE - Vecalign aligning first 5 sentences of the Thucydides Greek to the French: rows shaded in green are correct alignments; remaining errors are shown through color-coded text with Lucretius, some errors can be explained (and excused), for example the 昀椀rst alignment in Figure 2: the footnotes that are also aligned to the 昀椀rst sentence of the Latin include part of the translated text (“O Bountiful Venus”) and Latin original (“Alma Venus”). As with Thucydides, the remaining errors occurred across sentences covering similar subjects. Figure 2 suggests that the errors with Lucretius can be attributed to Vecalign: to correctly align all blue English sentences to the 昀椀rst Latin sentence, the algorithm would have to skip sentences 485-527 and then recognize the fragment in sentence 528 as part of the phrase begun in sentence 484. Vecalign was not designed to handle this case; its approximation to run in linear time averages the embeddings of consecutive sentences using a relatively small window (10 sentences in the default settings that we used). The blue sentences are too far apart in the English edition for Vecalign to capture them in one alignment. This is reminiscent of an error we encountered in the LASER-Vecalign results aligning the Crito with Jowett’s English translation when a sentence in Jowett (in red) was out of order relative to the Greek (Figure 3). 8. Conclusion and Future Work The extensive corpus of translations of the Greek and Latin classics holds tremendous promise for the study of translation, natural language processing, and variation and change in cultural assumptions. By studying both close translations and those full of literary license, paraphrase, censorship, and misunderstanding, we hope to enable scholars and students to understand this corpus better and translation-studies and NLP researchers to perform empirical studies of variation in translation. Our experiments demonstrate that a pipeline composed of state-of- the-art NLP systems for performing automatic sentence alignment on literary text in ancient languages is useful but leaves a lot of room for improvement. Even our best-performing con- 543 Figure 2: Behavior of LaBSE - Vecalign aligning first 5 sentences of the Lucretius Latin to the English: rows shaded in green are correct alignments; remaining errors are shown through color-coded text Figure 3: Behavior of LASER - Vecalign when a sentence is out of order in Jowett’s translation relative to the Greek, Crito (section 52e-53a) 昀椀guration struggles when translations exhibit signi昀椀cant discontiguity and non-monotonicity with respect to the source text. We expect this to be the case for the majority of extant trans- lations that we would like to analyze computationally. The performance of our pipeline is further compromised by the presence of paratext—footnotes, commentaries, alternate transla- tions, quotes from the source, and other extraneous material. This kind of noise in the form of paratext is also a common feature of collections of translations like the one compiled by the Open Greek and Latin Project that we aim to process, align at multiple granularities, and ana- lyze computationally. Another example of noisy collections is the series of nineteenth-century Hachette editions of the classics, which alone contains 46 editions with more than one French translation each.8 Considering these factors, we perceive two directions for future work. The 昀椀rst is to improve Vecalign’s model to handle longer paratext and multiple translations by revisiting the assump- 8The Hachette series can be found on HathiTrust’s website: https://babel.hathitrust.org/cgi/mb?a=listis;c=1152044995 544 tions it makes in pruning its search space and redesigning the dynamic program for alignment. The second approach is to build classi昀椀cation systems 昀椀ne-tuned for the genre of classical translations. Since paratext annotations are not always available, this would allow us to auto- matically identify paratext and detect multiple translations before running books through our LaBSE-Vecalign pipeline. Fortunately, in the collection compiled by the Open Greek and Latin Project, some of this processing has been done, and the XML 昀椀les tag the paratext explicitly. It remains to build models to detect editions with multiple translations and facing source and target texts using page-level language identi昀椀cation to remove these violations of Vecalign’s continuity and monotonicity assumptions. We are currently preprocessing several translation collections of interest for running through our sentence alignment pipeline in order to release a large dataset linking the Greek and Latin classics and their translations at the sentence level."
Machine Learning,David Smith,https://www.khoury.northeastern.edu/people/david-smith/,"David A. Smith is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is a founding member of the NULab for Texts, Maps, and Networks, Northeastern University’s center for the digital humanities and computational social sciences.","Efficient inference for machine learning models with complex latent structure; Modeling natural language structures, such as morphology, syntax, and semantics; Modeling the mutations in texts as they propagate through social networks and in language across space and time; Interactive information retrieval and machine learning for expert users","PhD in Computer Science, Johns Hopkins University; BA in Classics, Harvard University","June 27th, 2014",Detecting and Evaluating Local Text Reuse in Social Networks,https://aclanthology.org/W14-2707.pdf," Shaobin Xu, David Smith, Abigail Mullen, and Ryan Cordell. Detecting and evaluating local text reuse in social networks. In ACL Joint Workshop on Social Dynamics and Personal Attributes in Social Media, 2014.","Texts propagate among participants in many social networks and provide evi- dence for network structure. We describe intrinsic and extrinsic evaluations for algo- rithms that detect clusters of reused pas- sages embedded within longer documents in large collections. We explore applica- tions of these approaches to two case stud- ies: the culture of free reprinting in the nineteenth-century United States and the use of similar language in the public state- ments of U.S. members of Congress. 1s We have presented techniques for detecting reused passages embedded within the larger discourses Figure 4: Reprints of John Brown’s 1859 speech at his sentencing. Counties are shaded with histor- ical population data, where available. Even taking population differences into account, few newspa- pers in the South printed the abolitionist’s state- ment. produced by actors in social networks. Some of this shared content is as brief as partisan talking points or lines of poetry; other reprints can en- compass extensive legislative boilerplate or chap- ters of novels. The longer passages are easier to detect, with prefect pseudo-recall without exhaus- tive scanning of the corpus. Precision-recall trade- offs will vary with the density of text reuse and the noise introduced by optical character recog- nition and other features of data collection. We then showed the feasibility of using network re- gression to measure the correlations between con- nections inferred from text reuse and networks de- rived from outside information."
Machine Learning,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","February 27th, 2023",Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,https://openreview.net/pdf?id=_2bDpAtr7PI," David Klee, Ondrej Biza, Robert Platt, Robin Walters. (2023). Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction ICLR. https://openreview.net/pdf?id=_2bDpAtr7PI","Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in SO(3). How- ever, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages SO(3) equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at https://dmklee.github.io/image2sphere. 1In this work, we present the first method to leverage SO(3)-equivariance for predicting distributions over 3D rotations from single images. Our method is better suited than regression methods at handling unknown object symmetries, generates more expressive distributions than methods using parametric families of multi-modal distributions while requiring fewer samples than an implicit modeling approach. We demonstrate state-of-the-art performance on the challenging PASCAL3D+ dataset composed of real images. One limitation of our work is that we use a high maximum frequency, L, in the spherical convolution operations to have higher resolution predictions. Because the number of operations in a spherical convolution is quadratic in L, it may be impractical for applications where more spherical convolutions are required. 9 Published as a conference paper at ICLR 2023"
Machine Learning,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","February 1st, 2023","Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow",https://openreview.net/pdf?id=9ZpciCOunFb," Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, Nima Dehmamy. (2023). Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow ICLR. https://openreview.net/pdf?id=9ZpciCOunFb","Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neu- ral networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness un- der certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating con- served quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability. 1conclusion not found"
Machine Learning,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","November 16th, 2022",The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,https://openreview.net/pdf?id=P4MUGRM4Acu," Dian Wang , Jung Yeon Park, Neel Sortur, Lawson L. S. Wong, Robin Walters, Robert Platt. (2023). The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry ICLR. https://openreview.net/pdf?id=P4MUGRM4Acu","Extensive work has demonstrated that equivariant neural networks can signifi- cantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the do- main symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the in- put. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surpris- ingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, impos- ing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. 1conclusion not found"
Machine Learning,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","June 8th, 2022",Integrating Symmetry into Differentiable Planning with Steerable Convolutions,https://openreview.net/pdf?id=n7CPzMPKQl," Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L. S. Wong. (2023). Integrating Symmetry into Differentiable Planning with Steerable Convolutions ICLR. https://openreview.net/pdf?id=n7CPzMPKQl","In this paper, we study a principled approach on incorporating group symme- try into end-to-end differentiable planning algorithms and explore the benefits of symmetry in planning. To achieve this, we draw inspiration from equivariant con- volution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant op- erator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navi- gation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2- DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN. 1, SymVIN and SymGPPN generalize better to different map sizes, com- pared to all non-equivariant baselines. Remark. In summary, our results show that the Sym- Plan models demonstrate end-to-end planning and learn- ing ability, potentially enabling further applications to other tasks as a differentiable component for planning. Additional results and ablation studies are in Appendix H. 7 DISCUSSION In this work, we study the symmetry in the 2D path-planning problem, and build a framework using the theory of steerable CNNs to prove that value iteration in path planning is actually a form of steer- able CNN (on 2D grids). Motivated by our theory, we proposed two symmetric planning algorithms that provided significant empirical improvements in several path-planning domains. Although our focus in this paper has been on Z2, our framework can potentially generalize to path planning on higher-dimensional or even continuous Euclidean spaces (Weiler et al., 2018; Brandstetter et al., 2021), by using equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We hope that our SymPlan frame- work, along with the design of practical symmetric planning algorithms, can provide a new pathway for integrating symmetry into differentiable planning. 9 Published as a conference paper at ICLR 2023 8 ACKNOWLEDGEMENT This work was supported by NSF Grants #2107256 and #2134178. R. Walters is supported by The Roux Institute and the Harold Alfond Foundation. We also thank the audience from previous poster and talk presentations for helpful discussions and anonymous reviewers for useful feedback. 9 REPRODUCIBILITY STATEMENT We provide additional details in the appendix. We also plan to open source the codebase. We briefly outline the appendix below. 1. Additional Discussion 2. Background: Technical background and concepts on steerable CNNs and group CNNs 3. Method: we provide full details on how to reproduce it 4. Theory/Framework: we provide the complete version of the theory statements 5. Proofs: this includes all proofs 6. Experiment / Environment / Implementation details: useful details for reproducibility 7. Additional results 10 Published as a conference paper at ICLR 2023"
Natural Language Processing,David Bau,https://www.khoury.northeastern.edu/people/david-bau/,"David Bau is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Machine learning; Computer vision; Artificial intelligence; Natural language processing; Human–computer interaction,"PhD in Computer Science, Massachusetts Institute of Technology; MS in Computer Science, Cornell University; AB in Mathematics, Harvard University","October 24th, 2022",Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,https://openreview.net/pdf?id=DeG07_TcZvT," Kenneth Li , Aspen K. Hopkins, David Bau, Fernanda B. Viégas, Hanspeter Pfister, Martin Wattenberg. (2023). Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task ICLR. https://openreview.net/pdf?id=DeG07_TcZvT","Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question in a synthetic setting by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network. By leveraging these intervention techniques, we produce “latent saliency maps” that help explain predictions. 1 1Our experiments provide evidence that Othello-GPT maintains a representation of game board states— that is, the Othello “world”—to produce sequences it was trained on. This representation appears to be nonlinear in an essential way. Further, we find that these representations can be causally linked to how the model makes its predictions. Understanding of the internal representations of a sequence model is interesting in its own right, but may also be helpful in deeper interpretations of the network. We have also described how interventional experiments may be used to create a “latent saliency map”, which gives a picture, in terms of the Othello board, of how the network has made a prediction. Applied to two versions of Othello-GPT that were trained on different data sets, the latent saliency maps highlight the dramatic differences between underlying representations of the Othello-GPT trained on synthetic dataset and its counterpart trained on championship dataset. There are several potential lines of future work. One natural extension would be to perform the same type of investigations with other, more complex games. It would also be interesting to compare the strategies learned by a sequence model trained on game transcripts with those of a model trained with a priori knowledge of Othello. One option is to compare latent saliency maps of Othello–GPT with standard saliency maps of an Othello-playing program which has the actual board state as input. More broadly, it would be interesting to study how our results generalize to models trained on natural language. One stepping stone might be to look at language models whose training data has included game transcripts. Will we see similar representation of board state? Grammar engineering tools (Weston et al., 2015; Hermann et al., 2017; Cˆot´e et al., 2018) could help define a synthetic data generation process that maps world representations onto natural language sentences, providing a similarly controllable setting like Othello while closing the distance to natural languages. For more complex natural language tasks, can we find meaningful world representations? Our hope is that the tools described in this paper—nonlinear probes, layerwise interventions, and latent saliency maps—may prove useful in natural language settings. 9 Published as a conference paper at ICLR 2023"
Natural Language Processing,David Bau,https://www.khoury.northeastern.edu/people/david-bau/,"David Bau is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Machine learning; Computer vision; Artificial intelligence; Natural language processing; Human–computer interaction,"PhD in Computer Science, Massachusetts Institute of Technology; MS in Computer Science, Cornell University; AB in Mathematics, Harvard University","October 13th, 2022",Mass-Editing Memory in a Transformer,https://openreview.net/pdf?id=MkbcAHIYgyS," Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, David Bau. (2023). Mass-Editing Memory in a Transformer ICLR. https://openreview.net/pdf?id=MkbcAHIYgyS","Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at memit.baulab.info. 1We have developed MEMIT, a method for editing factual memories in large language models by directly manipulating specific layer parameters. Our method scales to much larger sets of edits (100x) than other approaches while maintaining excellent specificity, generalization, and fluency. Our investigation also reveals some challenges: certain relations are more difficult to edit with robust specificity, yet even on challenging cases we find that MEMIT outperforms other methods by a clear margin. The knowledge representation we study is also limited in scope to working with directional (s, r, o) relations: it does not cover spatial or temporal reasoning, mathematical knowledge, linguistic knowledge, procedural knowledge, or even symmetric relations. For example, the association that “Tim Cook is CEO of Apple” must be processed separately from the opposite association that “The CEO of Apple is Tim Cook.” Despite these limitations, it is noteworthy that large-scale model updates can be constructed using an explicit analysis of internal computations. Our results raise a question: might interpretability-based methods become a commonplace alternative to traditional opaque fine-tuning approaches? Our positive experience brings us optimism that further improvements to our understanding of network internals will lead to more transparent and practical ways to edit, control, and audit models. 9 Published as a conference paper at ICLR 2023 7 ETHICAL CONSIDERATIONS Although we test a language model’s ability to serve as a knowledge base, we do not find these models to be a reliable source of knowledge, and we caution readers that a LLM should not be used as an authoritative source of facts. Our memory-editing methods shed light on the internal mechanisms of models and potentially reduce the cost and energy needed to fix errors in a model, but the same methods might also enable a malicious actor to insert false or damaging information into a model that was not originally present in the training data. 8"
Natural Language Processing,David Smith,https://www.khoury.northeastern.edu/people/david-smith/,"David A. Smith is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is a founding member of the NULab for Texts, Maps, and Networks, Northeastern University’s center for the digital humanities and computational social sciences.","Efficient inference for machine learning models with complex latent structure; Modeling natural language structures, such as morphology, syntax, and semantics; Modeling the mutations in texts as they propagate through social networks and in language across space and time; Interactive information retrieval and machine learning for expert users","PhD in Computer Science, Johns Hopkins University; BA in Classics, Harvard University","December 6th, 2023",Automatic Collation for Diversifying Corpora: Commonly Copied Texts as Distant Supervision for Handwritten Text Recognition,https://ceur-ws.org/Vol-3558/paper1708.pdf," David A. Smith, Jacob Murel, Jonathan Parkes Allen, Matthew Thomas Miller. (2023). Automatic Collation for Diversifying Corpora: Commonly Copied Texts as Distant Supervision for Handwritten Text Recognition CHR, 206-221. https://ceur-ws.org/Vol-3558/paper1708.pdf","Handwritten text recognition (HTR) has enabled many researchers to gather textual evidence from the human record. One common training paradigm for HTR is to identify an individual manuscript or coherent collection and to transcribe enough data to achieve acceptable performance on that collection. To build generalized models for Arabic-script manuscripts, perhaps one of the largest textual traditions in the pre-modern world, we need an approach that can improve its accuracy on unseen manuscripts and hands without linear growth in the amount of manually annotated data. We propose Automatic Collation for Diversifying Corpora (ACDC), taking advantage of the existence of multiple manuscripts of popular texts. Starting from an initial HTR model, ACDC automatically detects matching passages of popular texts in noisy HTR output and selects high-quality lines for retraining HTR without any manually annotated data. We demonstrate the e昀昀ectiveness of this approach to distant supervision by annotating a test set drawn from a diverse collection of 59 Arabic-script manuscripts and a training set of 81 manuscripts of popular texts embedded within a larger corpus. A昀琀er a few rounds of ACDC retraining, character accuracy rates on the test set increased by 19.6% absolute percentage, while a supervised model trained on manually annotated data from the same collection increased accuracy by 15.9%. We analyze the variation in ACDC’s performance across books and languages and discuss further applications to collating manuscript families. Keywords handwritten text recognition, collation, manuscriptss, or recommendations expressed do not necessarily re昀氀ect those of the NEH or Mellon."
Natural Language Processing,David Smith,https://www.khoury.northeastern.edu/people/david-smith/,"David A. Smith is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is a founding member of the NULab for Texts, Maps, and Networks, Northeastern University’s center for the digital humanities and computational social sciences.","Efficient inference for machine learning models with complex latent structure; Modeling natural language structures, such as morphology, syntax, and semantics; Modeling the mutations in texts as they propagate through social networks and in language across space and time; Interactive information retrieval and machine learning for expert users","PhD in Computer Science, Johns Hopkins University; BA in Classics, Harvard University","December 6th, 2023",Testing the Limits of Neural Sentence Alignment Models on Classical Greek and Latin Texts and Translations,https://ceur-ws.org/Vol-3558/paper6193.pdf," Caroline Craig, Kartik Goyal, Gregory R. Crane, Farnoosh Shamsian, David A. Smith. (2023). Testing the Limits of Neural Sentence Alignment Models on Classical Greek and Latin Texts and Translations CHR, 530-553. https://ceur-ws.org/Vol-3558/paper6193.pdf","The Greek and Latin classics, like many other ancient texts, have been widely translated into a variety of languages over the past two millennia. Although many digital editions and libraries contain one or two translations for a given text, about one hundred translations of the Iliad and twenty of Herodotus, for ex- ample, exist in English alone. Aligning the corpus of classical texts and translations at the sentence and word level would provide a valuable resource for studying translation theory, digital humanities, and natural language processing (NLP). Precise and faithful sentence alignment via computational methods, however, remains a challenging problem. Current alignment methods tend to have poor coverage and recall since their primary aim is to extract single sentence pairs for training machine translation systems. This paper evaluates and examines the limits of such state-of-the-art models for cross-language sentence embedding and alignment of ancient Greek and Latin texts with translations into English, French, Ger- man, and Persian. We release evaluation data for Plato’s Crito, manually annotated at the word and sentence level, and larger test datasets based on coarser structural metadata for Thucydides (Greek) and Lucretius (Latin). Testing LASER and LaBSE for sentence embedding and nearest-neighbor retrieval and Vecalign for sentence alignment, we found best results using LaBSE-Vecalign. LaBSE worked sur- prisingly well on ancient Greek, most probably because it had been merged with modern Greek data in its training. Both LASER-Vecalign and LaBSE-Vecalign did best when there were many ground-truth one-to-one alignments between source and target sentences, and when the order of sentences in the source was preserved in the translation. However, these conditions are o昀琀en not present in the kinds of literary and free translation we wish to study, nor in editions with multiple translations, extensive commentary, or other paratext. We perform book-level and chapter-level error analysis to inform the development of a so昀琀ware pipeline that can be deployed on the vast corpus of translations of ancient texts. Keywords sentence alignment, multilingual embedding, machine translation, Ancient Greek, Latin 530as a possible direction for future work. 3.2. Sentence Alignment For sentence alignment, we relied on Vecalign [25], which remains the state-of-the-art model though it was published in 2019. Vecalign computes a昀케nity scores between sentences or groups of sentences from a source and target text. The algorithm takes in sentence embed- dings as input and then uses these embeddings to assess the similarity of sentences. Then, it reduces the problem of alignment to enumeration over all possible pairs of groups of sentences across the source and target texts. This process yields highest scoring pairs, resulting in either one-to-one, one-to-many, many-to-one, or many-to-many aligned sentence pairs. This enu- meration is exponentially expensive in terms of the size of the bitext (pair of source and target texts). Therefore, a dynamic program is used to perform it e昀케ciently. Further approximations are made to reduce the runtime by incorporating inductive biases and modeling assumptions, such as the largely monotonic nature of sentence alignment across the bitext. Another source of drastic reduction in runtime is a coarse-to-昀椀ne approach employed for alignment, which 533 prunes the search space of sentence pairs aggressively by making severe contiguity and mono- tonicity assumptions. These approximations and assumptions reduce the runtime to being asymptotically linear in terms of the size of the corpus. However, as we show in our experi- ments, these assumptions might not necessarily hold for our task. The texts we are interested in exhibit signi昀椀cant non-monotonicity, discontiguousness, and noise in the form of extrane- ous material (paratext) that is interspersed throughout the content sentences that actually align across the bitext. Our 昀椀ndings point toward future research on better alignment algorithms which make fewer of these unrealistic assumptions for our task while remaining practical to execute. 4. Dataset All ancient Greek and Latin texts were extracted from The Perseus Project [23] (Perseus) and contain no paratext. The translations varied in terms of language, sources, formats, available annotations, and o昀琀en included paratext as detailed in Table 1. The texts are further described by annotation level, since this impacted their use in di昀昀erent experiments. We aimed to test our pipeline on a varied set of texts, including di昀昀erent styles (dialogue, poetry, prose) and level of noisiness (texts with and without paratext). Therefore, we used available annotations, leading some experiments to be evaluated at the chunk level (Table 2), sentence level (Table 3), or at coarser levels (chapter or book: Table 4). The one exception is the small test set (Table 3) that the authors of this paper manually annotated at the sentence level. We used this test set at the outset of our project for rapid testing in order to inform next steps. Details on the paratext present in the two noisy texts can be found in Table 4. 4.1. Preprocessing: Chunk-Level and Sentence Segmentation When we refer to “chunk-level,” we mean the most 昀椀ne-grained citation structure available on Perseus, such as chapters, sections, or Stephanus pages.4 By “sentence-level,” we mean the phrases obtained a昀琀er sentence segmentation, explained below. Any additional pre- and post-processing is listed in the appendix. Table 2 lists all texts which were previously annotated at the chunk level. Preprocessing on these texts was limited to concatenating the chunks into a continuous string. For the sentence- level experiments (Tables 3, 4), we applied standard preprocessing to all texts: concatenated the raw text into one string, then segmented into sentences. For languages supported by Stanza [18] (Latin, English, French), we 昀椀rst split the text into Stanza’s sentences, then split further on semi-colons and colons. For unsupported languages (Ancient Greek), we segmented ourselves by splitting on periods, semi-colons, and colons. 4Stephanus pagination refers to the page breaks used in modern editions and translations of the works of Plato. They were 昀椀rst established by a 1578 edition published by Henri Estienne, also known as Henricus Stephanus. 534 Table 1 Summary of Texts Used Work Translator Language Paratext Annotation Annotator Crito (Source) Greek no section Perseus Fowler English no section student Jowett English no section student Schleiermacher German no section student Mohammadi (student) Farsi no section student Thucydides (Source) Greek no section Perseus Crawley English no section Perseus Bétant French yes chapter OGL Lucretius (Source) Latin no Perseus card Perseus Watson, Good English yes book OGL (2 translations in 1 volume) Table 2 Datasets for Chunk-Level Evaluations: Chunks are sections of Stephanus pages (Crito) or sections (Thucydides). We report space-separated tokens for comparability across languages. Work Translator # Chunks Avg. Tok./Chunk Std. Dev. Crito (Source) 268 15.96 14.73 Fowler 267 21.29 19.99 Jowett 259 20.59 18.85 Schleiermacher 267 20.70 19.72 Mohammadi 267 21.42 20.57 Thucydides (Source) 3575 41.96 19.77 Crawley 3575 56.52 27.96 Table 3 Small Test Set: Dataset for Sentence-Level Evaluation Work Translator # Sentences Avg. Tok./Sent. Std. Dev. Annotator Crito (Source) 60 18.08 16.65 Paper authors Fowler 66 22.05 18.10 Paper authors Jowett 79 17.61 14.66 Paper authors 4.2. Annotations 4.2.1. Crito Students involved in a project at Lepizig University annotated thirteen translations of Crito (including two in English, one in German, and 昀椀ve in Persian), to match the chunk-level anno- tation of the Greek text in Perseus. In our experiments, we used both English translations (by Harold North Fowler [15] and Benjamin Jowett [16]), the one German translation (by Schleier- 535 Table 4 Noisy Data: Dataset for Sentence-Level Experiments with Coarse-Level Evaluation Work Translator # Sent. Avg.Tok./Sent. Std. Dev. Text Sent. Paratext Sent. Thucydides (Source) 6097 24.63 16.77 6097 0 Bétant 17203 14.23 10.51 10958 6245 Lucretius (Source) 2428 20.20 13.14 3575 0 Watson & Good 14648 13.83 11.61 8521 6127 Watson only 9815 15.82 13.40 3936 5879 Good only 4833 11.00 7.59 4584 248 macher [17]), and one of the Persian translations (by Mohammadi, published on Zenodo [22]). This Persian translation was done by one of the student annotators.5 4.2.2. The Open Greek and Latin Project (OGL) The two noisy translations in our dataset were annotated by The Open Greek and Latin Project. These texts are available to the public in XML 昀椀les that include varying levels of annotation.6 For the Thucydides French translation by Bétant [26], annotations include tags for paratext and book and chapter boundaries. For the Lucretius edition, which contains two English trans- lations by Rev. John Selby Watson and John Mason Good [13], annotations include tags for paratext and book boundaries. 4.3. Noisy Data Features Both the Thucydides (fr) and Lucretius translations include paratext. In Thucydides, paratext consists of a foreword; commentary and summary of contents preceding each of the eight books; 33 footnotes; and an index. In the Lucretius edition, paratext consists of a foreword, a commentary before the prose translation, 1426 notes including 955 footnotes interspersed through the text (70 in the foreword, 861 in the Watson translation, 24 in the Good translation), and an index. The Lucretius edition contains additional noise in the form of two translations included in one edition. 5. Experiments Our experiments are summarized in Table 5. To guide the direction of our research, we 昀椀rst ran our two candidate pipelines, LASER-Vecalign and LaBSE-Vecalign, on the Crito test set. We then validated these results with retrieval experiments on our full dataset, using available annotations. Lastly, we tested our best pipeline, LaBSE-Vecalign, on noisy data with a focus 5The student translators used treebanks, commentaries, lexicon entries, and English and German translations to aid their Persian translations. They also aligned their Persian translations at the word level to the Greek, using Ugarit (Mohammadi’s can be found here: https://ugarit.ialigner.com/userProfile.php?userid=52434&tgid=9362). 6https://github.com/OpenGreekAndLatin 536 Table 5 Summary of Experiments Goal Experiments Texts Used Annotation Evaluation Initial rapid LASER-Vecalign & Crito test set Sentence Scoring Functions testing LaBSE-Vecalign (see 5.3) LASER-Vecalign & Crito Chunk Recall LaBSE-Vecalign (Fowler, Jowett) Validate initial Chunk retrieval Crito (all), Chunk Recall results Thucydides (en) Sentence retrieval Crito test set Sentence Recall Sentence retrieval Thucydides (fr), Sentence Recall (coarse) Lucretius (en) Test best pipeline LaBSE-Vecalign Thucydides (fr), Sentence Recall (coarse) on noisy data Lucretius (en) on error analysis to understand how the pipeline would fare on the type of unannotated data that we would like to align using our pipeline. 5.1. Experimental Set-Up To run the retrieval experiments, we 昀椀rst passed pre-processed source and target texts seg- mented into chunks or sentences through an embedding model (LASER or LaBSE). Then we used cosine similarity to retrieve the most similar chunks or sentences across the bitext pairs. For chunk-level retrieval experiments, we le昀琀out from the source text any chunks with missing translations (hence the di昀昀erent number of chunks across translations in Table 7). For LASER-Vecalign and LaBSE-Vecalign, we passed the chunk- or sentence-level embed- dings into Vecalign, which outputs a set of predicted alignments. These may include one-to- one, one-to-many, many-to-one, or many-to-many alignments. 5.2. Evaluation For chunk-level retrieval, we report the percentage of source chunks with the correct result among the top 1 (correct result has the highest similarity score) and top 10 similarity scores. For sentence-level retrieval, we modify the numerator and denominator to account for one- to-many sentence alignments in the ground truth: for every Greek sentence, we compute the number of correct sentences retrieved divided by the number of target sentences in the true alignment. For LASER-Vecalign and LaBSE-Vecalign, we evaluated results based on available annota- tions. For the Crito test set, where we manually produced sentence-level ground truth, we used Vecalign’s scoring functions and two new functions we formulated, described below. For chunk alignment experiments, where the ground truth is a straightforward list of one-to-one alignments, we report the percentage of incorrect predictions. Finally, for LaBSE-Vecalign on 537 the two noisy texts, Thucydides (fr) and Lucretius (en), we evaluate the pipeline’s predictions at the coarse level of annotations available in the Open Greek and Latin Project’s database. These experiments were run on segmented sentences but we report accuracy relative to a predicted sentence belonging to the same chapter (Thucydides) or book (Lucretius) as the source text. 5.3. Scoring Functions For Vecalign Predictions Vecalign’s original scoring function reports strict and lax scores for Precision, Recall, and F1. The lax metric expands the de昀椀nition of true positives to include any correct sentence align- ment in a one-to-many or many-to-many prediction. A strict true positive requires exact matches between a ground-truth alignment and a prediction. When performing our initial rapid testing, we formulated two additional metrics. Both are based on post-processing Ve- calign’s results by merging predicted alignments to try to reconstitute Perseus sections. We used this annotation level as ground truth because Perseus sections are examples of the avail- able annotations applied to ancient texts by editorial convention. Therefore, we sought to determine how the pipelines would perform at this challenging level. If a昀琀er merging there’s a strict match, then this is a true positive under the “New Strict Scoring” function. In the “New Lax Scoring” function, we instead look for lax matches: for any sentence that appears on both sides of the reconstituted alignment that is in a Perseus section, true positives are increased by one. 6. Results 6.1. LaBSE Outperformed LASER LaBSE outperformed LASER in our initial testing of the candidate pipelines, LASER-Vecalign and LaBSE-Vecalign. This was repeated in our validation experiments (chunk-alignment, chunk-retrieval, and sentence-retrieval). When used in conjunction with Vecalign, LaBSE consistently outperformed LASER (Table 6). Unsurprisingly, both pipelines did best under the New Lax Scoring function. Since this metric gives credit for (counts as true positive) any correct sentence matching between source and target text, it most approximates the pipelines’ ability to correctly align sentences. Therefore, at the sentence-level, the results on clean data with the Crito were overall very promising. LaBSE’s and LASER’s worst results were with the Strict Scoring function, con昀椀rming our hy- pothesis that existing pipelines would struggle to return correct alignments at the more chal- lenging annotation level that we 昀椀nd in ancient texts and their translations. Results for both pipelines at the chunk level are in Table 12 in the appendix (LaBSE-Vecalign aligned all chunks correctly, while LASER-Vecalign aligned 98.88% of Greek - Fowler chunks and 94.98% of Greek - Jowett chunks correctly). In the retrieval experiments, LASER struggled most when retrieving chunks from the En- glish Thucydides translation, the longest text on which we tested LASER (Table 7). LASER’s performance improved when retrieving sentences from the Crito test set (Table 8), in other words when retrieving shorter spans of text from a shorter document. We do not see simi- lar di昀昀erence with LaBSE’s performance on shorter text spans and documents, with its scores 538 Table 6 LASER vs. LaBSE: Sentence alignment with Vecalign from the ancient Greek (G) of Plato’s Crito to the English translations of Fowler (F) and Jowett (J) and between English translations G–F G–F G–J G–J F–J F–J Embedding Model Used LabSE LASER LabSE LASER LabSE LASER Vecalign Strict Scoring Precision - Strict 0.737 0.655 0.600 0.414 0.536 0.472 Recall - Strict 0.778 0.704 0.623 0.453 0.577 0.481 F1 - Strict 0.757 0.679 0.611 0.432 0.556 0.476 New Strict Scoring Accuracy 0.870 0.796 0.685 0.500 0.712 0.577 New Lax Scoring Precision - Lax 0.984 0.944 0.950 0.791 0.972 0.966 Recall - Lax 0.827 0.778 0.805 0.647 0.727 0.714 F1 - Lax 0.899 0.853 0.871 0.712 0.832 0.821 Table 7 LASER vs. LaBSE: Chunk-Level retrieval from ancient Greek to translations of Crito (Cr.) and Thucy- dides (Thuc.) Fowler Jowett Schleiermacher Mohammadi Crawley (Cr., en) (Cr., en) (Cr., de) (Cr., fa) (Thuc., en) Num. Chunks 267 259 267 267 3575 LaBSE Top 1 75.66% 57.92% 78.65% 71.91% 79.61% Top 10 93.26% 83.40% 94.01% 87.27% 93.15% LASER Top 1 33.71% 14.67% 34.46% 23.60% 3.05% Top 10 67.42% 46.72% 68.16% 44.94% 8.98% at the sentence level slightly lower yet still comparable to those at the chunk level (Table 7, Table 8). 6.2. Performance Di昀昀erences on Clean Data LaBSE and LASER both exhibited performance di昀昀erences across translations of the Crito in experiments using the test set and the full text. With the test set, LASER-Vecalign and LaBSE- Vecalign did best aligning the Greek to Fowler’s more literal translation (Table 6), where there are the most one-to-one alignments in the ground truth (70% compared to 48% for Greek - Jowett and Fowler - Jowett). Likewise, both LaBSE and LASER had higher scores retrieving sentences between the Greek and Fowler than between the Greek and Jowett. Interestingly, both models perform better retrieving sentences between Fowler and Jowett (“F-J” in Table 8) than when used in conjunction with Vecalign (“F-J” in Table 6). At the chunk level, LaBSE 539 Table 8 LASER vs. LaBSE: Sentence-Level retrieval from the ancient Greek (G) of Plato’s Crito to the English translations of Fowler (F) and Jowett (J) and between English translations G–F G–J F–J LaBSE Top 1 76.67% 51.39% 73.58% Top 10 90.83% 82.54% 92.17% LASER Top 1 41.67% 30.83% 61.24% Top 10 67.50% 51.17% 78.34% Table 9 Sentence-Level retrieval Thucydides, Lucretius (noisy data) Thucydides (el - fr) Lucretius (la - en) (from same chapter) (from same book) LaBSE Top 1 36.49% 76.52% Top 10 60.70% 97.24% and LASER also did better on Fowler’s more literal translation in both chunk-level retrieval (Table 7) and chunk-level alignment (Table 12 in Appendix). 6.3. Challenges of Noisy Data Testing LaBSE and LaBSE-Vecalign on noisy data points to the challenges of the two noisy features present in our noisy dataset: the presence of paratext and multiple translations in the target document. LaBSE did worse retrieving sentences from the noisy Thucydides translation (60.70% in Top 10 in Table 9) than the English translation (93.15% in Top 10 in Table 7), even though the evaluation was done at a coarser level with the French. These experiments di昀昀er in three ways: the language of the target (French vs. English), the shorter text span in the sentence retrieval experiment, and the presence of paratext in the French. Given LaBSE’s comparable results with retrieval at the chunk and sentence level on the Crito, the presence of paratext is the likely reason for the performance di昀昀erence. At the coarser, book level we used to evaluate sentence retrieval with Lucretius, the presence of paratext no longer made a visible impact (Table 9). When used in conjunction with Vecalign, and also evaluated at the same coarse level, LaBSE- Vecalign did very well on Thucydides (fr), despite the presence of paratext. Thus 96.72% of paratext sentences correctly aligned to null and 94.04% of French text sentences were aligned to a Greek sentence from the same chapter (Table 10). However, on Lucretius 35.03% of paratext sentences were incorrectly aligned to null and 55.23% of English text sentences incorrectly aligned to null (Table 10). We also reported results using the number of Vecalign predictions as denominator, which can be found in Table 13 in the Appendix and show a similar pattern. 540 Table 10 LaBSE - Vecalign: By Number of Target Sentences, Thucydides (fr - el, chapter-level evaluation) and Lucretius (en - la, book-level evaluation) Type of Alignment Thucydides Lucretius (chapter-level) (book-level) As a percent of Paratext Sentences Number of paratext sentences 6245 6127 Paratext sents. to null (correct) 96.72% 64.97% Paratext sents. to source text sents. (incorrect) 3.28% 35.03% As a Percent of Target Text Sentences Number of text sentences 10958 8521 Text-to-text: to source sents. from same chapter or book 94.04% 44.24% Text-to-text: to at least 1 source sent. from same chapter or book 0.93% 0.49% Errors, text-to-text: to no source sents. from same chapter or book 4.93% 0.04% Errors, text-to-null 0.10% 55.23% Table 11 LaBSE - Vecalign: Lucretius (en - la) - No Paratext, First Translation Only (book-level evaluation) Type of Alignment Lucretius (book-level) As a Percent of Text Sents. from First Translation Only Number of Text Sentences 3648 Text-to-text: to source sents. from same book 97.97% Text-to-text: to at least one source sent. from same book 1.67% Errors, Text-to-text: to no source sent. from same book 0.00% Errors, Text-to-Null 0.36% In order to get results for Lucretius using LaBSE-Vecalign that are comparable to those on Thucydides, we had to suppress both paratext and the second translation (by Good) in the target edition. When we only suppressed paratext, 65.56% of Vecalign’s predictions were (incorrect) null-to-text alignments, and 55.21% of English text sentences were incorrectly aligned to null (Table 14 in the Appendix). When we only suppressed the second translation, 96.26% of text sentences were correctly aligned to Latin sentences from the same book, but 36.26% of paratext sentences were incorrectly aligned to Latin text sentences (Table 15 in the Appendix). Finally, when we suppressed paratext and only counted text sentences from the 昀椀rst translation (by Watson), 97.97% of English text sentences aligned to Latin sentences from the same book, and no English text sentences aligned to Latin sentences from a di昀昀erent book (Table 11). In other words, only a昀琀er preprocessing the English edition of Lucretius to obtain a clean dataset (no paratext, no second translation) was LaBSE - Vecalign able to achieve results comparable to those we saw with the French translation of Thucydides. Interestingly, when we compare against retrieval results in Table 9, LaBSE did better on its 541 own than it did with Vecalign on Lucretius, but worse on Thucydides. Thus using LaBSE em- beddings only, for 97.24% of the 2428 Latin sentences we retrieved English text sentences from the same book among the top 10 most similar sentences, while LaBSE-Vecalign aligned only 44.24% of the 8521 English text sentences to Latin sentences from the same book. On the other hand with Thucydides, we retrieved French text sentences from the same chapter among the top 10 most similar sentences for 60.70% of the 6097 Greek sentences using LaBSE embeddings only, while LaBSE-Vecalign aligned 94.04% the 10958 French text sentences to Greek sentences from the same chapter. These results indicate that Vecalign’s faulty assumption and inductive biases for approximate dynamic programming can override the signal from well-trained sen- tence representation methods and hurt the overall performance in the case of discontiguous noisy text. 7. Discussion Our experiments led us to identify LaBSE-Vecalign as the pipeline using existing models best suited to produce sentence-level alignments of Ancient Greek and Latin texts with their trans- lations. However, even with the clean data of the Crito, this pipeline evidenced di昀昀erences in performance across translations. This was even the case when aligning English to En- glish (Fowler-Jowett), with LaBSE-Vecalign showing better results aligning Greek-Fowler than Fowler-Jowett. The performance driver with Crito seems therefore to be the nature of the trans- lations themselves: the number of one-to-many and many-to-many alignments in the ground truth. The di昀昀erences in the two translations also emerge at the word level. We manually aligned the 昀椀rst half of the Crito’s words with Jowett’s translation on Ugarit [31] and com- pared to alignments with Fowler prepared by an author of Ugarit’s alignment guidelines [30]7. We were able to align 52% of Greek words with Jowett’s translation, compared to 80% with Fowler’s (Table 16 in the Appendix). Of these aligned Greek words, with Jowett 9% crossed the predicted sentence boundaries while none did with Fowler’s (Table 17 in the Appendix). With more realistic noisy data, LaBSE-Vecalign was not able to handle both the nature of the paratext in the Lucretius edition, and its inclusion of a second translation. The Lucretius edition not only has many footnotes, these are also lengthy. Thus the 昀椀rst two sentences of the translation in Lucretius (following 461 sentences of the foreword and 23 sentences of commentary preceding book 1) are followed by 44 sentences of footnotes before we 昀椀nd the third text sentence. In contrast, Thucydides had fewer footnotes (33), also interspersed in the translated text but all short (the longest spans 2 sentences). Figures 1 and 2 show details of the LaBSE-Vecalign results aligning Thucydides and Lucretius with their French and English translations, respectively. In the Thucydides detail, the Greek sentences on the le昀琀are much longer than the French sentences; the 昀椀昀琀h Greek sentence (sec- tion 1.2.2) is translated over 昀椀ve sentences in French (last row of Figure 1). The 昀椀rst three rows are errors; the 昀椀rst two contain no overlapping sentences and the third includes the correct French sentences for section 1.1.3 as well as the French translations of about half of section 1.1.1 and all of section 1.1.2. There is no evident pattern explaining these errors. However, 7Word alignments with Jowett: https://ugarit.ialigner.com/userProfile.php?userid=126388&tgid=12065 and Fowler: https://ugarit.ialigner.com/userProfile.php?userid=3&tgid=8609 542 Figure 1: Behavior of LaBSE - Vecalign aligning first 5 sentences of the Thucydides Greek to the French: rows shaded in green are correct alignments; remaining errors are shown through color-coded text with Lucretius, some errors can be explained (and excused), for example the 昀椀rst alignment in Figure 2: the footnotes that are also aligned to the 昀椀rst sentence of the Latin include part of the translated text (“O Bountiful Venus”) and Latin original (“Alma Venus”). As with Thucydides, the remaining errors occurred across sentences covering similar subjects. Figure 2 suggests that the errors with Lucretius can be attributed to Vecalign: to correctly align all blue English sentences to the 昀椀rst Latin sentence, the algorithm would have to skip sentences 485-527 and then recognize the fragment in sentence 528 as part of the phrase begun in sentence 484. Vecalign was not designed to handle this case; its approximation to run in linear time averages the embeddings of consecutive sentences using a relatively small window (10 sentences in the default settings that we used). The blue sentences are too far apart in the English edition for Vecalign to capture them in one alignment. This is reminiscent of an error we encountered in the LASER-Vecalign results aligning the Crito with Jowett’s English translation when a sentence in Jowett (in red) was out of order relative to the Greek (Figure 3). 8. Conclusion and Future Work The extensive corpus of translations of the Greek and Latin classics holds tremendous promise for the study of translation, natural language processing, and variation and change in cultural assumptions. By studying both close translations and those full of literary license, paraphrase, censorship, and misunderstanding, we hope to enable scholars and students to understand this corpus better and translation-studies and NLP researchers to perform empirical studies of variation in translation. Our experiments demonstrate that a pipeline composed of state-of- the-art NLP systems for performing automatic sentence alignment on literary text in ancient languages is useful but leaves a lot of room for improvement. Even our best-performing con- 543 Figure 2: Behavior of LaBSE - Vecalign aligning first 5 sentences of the Lucretius Latin to the English: rows shaded in green are correct alignments; remaining errors are shown through color-coded text Figure 3: Behavior of LASER - Vecalign when a sentence is out of order in Jowett’s translation relative to the Greek, Crito (section 52e-53a) 昀椀guration struggles when translations exhibit signi昀椀cant discontiguity and non-monotonicity with respect to the source text. We expect this to be the case for the majority of extant trans- lations that we would like to analyze computationally. The performance of our pipeline is further compromised by the presence of paratext—footnotes, commentaries, alternate transla- tions, quotes from the source, and other extraneous material. This kind of noise in the form of paratext is also a common feature of collections of translations like the one compiled by the Open Greek and Latin Project that we aim to process, align at multiple granularities, and ana- lyze computationally. Another example of noisy collections is the series of nineteenth-century Hachette editions of the classics, which alone contains 46 editions with more than one French translation each.8 Considering these factors, we perceive two directions for future work. The 昀椀rst is to improve Vecalign’s model to handle longer paratext and multiple translations by revisiting the assump- 8The Hachette series can be found on HathiTrust’s website: https://babel.hathitrust.org/cgi/mb?a=listis;c=1152044995 544 tions it makes in pruning its search space and redesigning the dynamic program for alignment. The second approach is to build classi昀椀cation systems 昀椀ne-tuned for the genre of classical translations. Since paratext annotations are not always available, this would allow us to auto- matically identify paratext and detect multiple translations before running books through our LaBSE-Vecalign pipeline. Fortunately, in the collection compiled by the Open Greek and Latin Project, some of this processing has been done, and the XML 昀椀les tag the paratext explicitly. It remains to build models to detect editions with multiple translations and facing source and target texts using page-level language identi昀椀cation to remove these violations of Vecalign’s continuity and monotonicity assumptions. We are currently preprocessing several translation collections of interest for running through our sentence alignment pipeline in order to release a large dataset linking the Greek and Latin classics and their translations at the sentence level."
Natural Language Processing,David Smith,https://www.khoury.northeastern.edu/people/david-smith/,"David A. Smith is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is a founding member of the NULab for Texts, Maps, and Networks, Northeastern University’s center for the digital humanities and computational social sciences.","Efficient inference for machine learning models with complex latent structure; Modeling natural language structures, such as morphology, syntax, and semantics; Modeling the mutations in texts as they propagate through social networks and in language across space and time; Interactive information retrieval and machine learning for expert users","PhD in Computer Science, Johns Hopkins University; BA in Classics, Harvard University","June 27th, 2014",Detecting and Evaluating Local Text Reuse in Social Networks,https://aclanthology.org/W14-2707.pdf," Shaobin Xu, David Smith, Abigail Mullen, and Ryan Cordell. Detecting and evaluating local text reuse in social networks. In ACL Joint Workshop on Social Dynamics and Personal Attributes in Social Media, 2014.","Texts propagate among participants in many social networks and provide evi- dence for network structure. We describe intrinsic and extrinsic evaluations for algo- rithms that detect clusters of reused pas- sages embedded within longer documents in large collections. We explore applica- tions of these approaches to two case stud- ies: the culture of free reprinting in the nineteenth-century United States and the use of similar language in the public state- ments of U.S. members of Congress. 1s We have presented techniques for detecting reused passages embedded within the larger discourses Figure 4: Reprints of John Brown’s 1859 speech at his sentencing. Counties are shaded with histor- ical population data, where available. Even taking population differences into account, few newspa- pers in the South printed the abolitionist’s state- ment. produced by actors in social networks. Some of this shared content is as brief as partisan talking points or lines of poetry; other reprints can en- compass extensive legislative boilerplate or chap- ters of novels. The longer passages are easier to detect, with prefect pseudo-recall without exhaus- tive scanning of the corpus. Precision-recall trade- offs will vary with the density of text reuse and the noise introduced by optical character recog- nition and other features of data collection. We then showed the feasibility of using network re- gression to measure the correlations between con- nections inferred from text reuse and networks de- rived from outside information."
Natural Language Processing,Byron Wallace,https://www.khoury.northeastern.edu/people/byron-wallace/,"Byron Wallace is an associate dean of graduate programs, director for the undergraduate data science program, and the Sy and Laurie Sternberg Interdisciplinary Associate Professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Data mining; Machine learning; Natural language processing,"PhD in Computer Science, Tufts University; BS in Computer Science, University of Massachusetts at Amherst","April 25th, 2017",Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization,https://arxiv.org/pdf/1702.02535v3.pdf," Ye Zhang, Matthew Lease, Byron C. Wallace","A fundamental advantage of neural mod- els for NLP is their ability to learn rep- resentations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., Word- Net or domain speciﬁc ontologies such as the Uniﬁed Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compres- sion. In contrast, we treat weight shar- ing as a ﬂexible mechanism for incorpo- rating prior knowledge into neural models. We show that this approach consistently yields improved performance on classiﬁ- cation tasks compared to baseline strate- gies that do not exploit weight sharing. 1We have proposed a novel method for incorporat- ing prior semantic knowledge into neural models via stochastic weight sharing. We have showed it generally improves text classiﬁcation performance vs. model variants which do not exploit external resources and vs. an approach based on retroﬁtting prior to training. In future work, we will inves- tigate generalizing our approach beyond classiﬁ- cation, and to inform weight sharing using other varieties and sources of linguistic knowledge."
Network Science,Albert-László Barabási,https://www.khoury.northeastern.edu/people/albert-laszlo-barabasi/,"Albert-László Barabási is the Robert Gray Dodge Professor of Network Science and a Distinguished University Professor at Northeastern University, based in Boston. He directs the Center for Complex Network Research and holds appointments in the College of Science and the Khoury College of Computer Sciences.",Network science; Properties of sub-cellular networks in understanding human disease; Control theory,"PhD in Physics, Boston University; MS in Theoretical Physics, Eötvös Loránd University — Hungary; BS in Physics and Engineering, University of Bucharest — Romania","September 8th, 2020",3D Topology Transformation with Generative Adversarial Networks,http://computationalcreativity.net/iccc20/papers/052-iccc20.pdf," Luca Stornaiuolo, Nima Dehmamy, Albert-László Barabási, Mauro Martino. (2020). 3D Topology Transformation with Generative Adversarial Networks ICCC, 461-468. http://computationalcreativity.net/iccc20/papers/052-iccc20.pdf","Generation and transformation of images and videos using artiﬁcial intelligence have ﬂourished over the past few years. Yet, there are only a few works aim- ing to produce creative 3D shapes, such as sculptures. Here we show a novel 3D-to-3D topology transfor- mation method using Generative Adversarial Networks (GAN). We use a modiﬁed pix2pix GAN, which we call Vox2Vox, to transform the volumetric style of a 3D ob- ject while retaining the original object shape. In par- ticular, we show how to transform 3D models into two new volumetric topologies - the 3D Network and the Ghirigoro. We describe how to use our approach to con- struct customized 3D representations. We believe that the generated 3D shapes are novel and inspirational. Fi- nally, we compare the results between our approach and a baseline algorithm that directly convert the 3D shapes, without using our GAN.and Future Direction In this paper, we presented a novel 3D-to-3D topology trans- fer paradigm based on transformations in 3D space. In particular, we built a 3D conditional GAN, Vox2Vox, that performs volumetric transformations to modify the internal structure of any 3D object, while maintaining its overall shape. We described our complete pipeline to apply our ap- proach to two different topologies: the 3D Network and the Ghirigoro. The results obtained by employing our method- ology are novel and inspirational. We compared the out- puts of the pipeline while using or not the 3D-cGAN and found that using the Vox2Vox output as a prior distribution results in much nicer outcomes where features are placed in strategic positions in the 3D shape preserving its struc- tural features. As a future direction, we plan to improve the 3D-to-3D topology transfer by given also the topology as a conditional input of the generative network. To do that, the machine learning algorithm has to learn itself the abstraction of the topology from a given 3D object."
Network Science,Albert-László Barabási,https://www.khoury.northeastern.edu/people/albert-laszlo-barabasi/,"Albert-László Barabási is the Robert Gray Dodge Professor of Network Science and a Distinguished University Professor at Northeastern University, based in Boston. He directs the Center for Complex Network Research and holds appointments in the College of Science and the Khoury College of Computer Sciences.",Network science; Properties of sub-cellular networks in understanding human disease; Control theory,"PhD in Physics, Boston University; MS in Theoretical Physics, Eötvös Loránd University — Hungary; BS in Physics and Engineering, University of Bucharest — Romania","April 18th, 2020","Give more data, awareness and control to individual citizens, and they will help COVID-19 containment",http://www.tdp.cat/issues16/tdp.a389a20.pdf," Mirco Nanni, Gennady L. Andrienko, Albert-László Barabási, Chiara Boldrini, Francesco Bonchi, Ciro Cattuto, Francesca Chiaromonte, Giovanni Comandé, Marco Conti, Mark Coté, Frank Dignum, Virginia Dignum, Josep Domingo-Ferrer, Paolo Ferragina, Fosca Giannotti, Riccardo Guidotti, Dirk Helbing, Kimmo Kaski, János Kertész, Sune Lehmann, Bruno Lepri, Paul Lukowicz, Stan Matwin, David Megías, Anna Monreale, Katharina Morik, Nuria Oliver, Andrea Passarella, Andrea Passerini, Dino Pedreschi, Alex Pentland, Fabio Pianesi, Francesca Pratesi, Salvatore Rinzivillo, Salvatore Ruggieri, Arno Siebes, Vicenç Torra, Roberto Trasarti, Jeroen van den Hoven, Alessandro Vespignani. (2020). Give more data, awareness and control to individual citizens, and they will help COVID-19 containment Trans. Data Priv., 13, 61-66. http://www.tdp.cat/issues16/tdp.a389a20.pdf","The rapid dynamics of COVID-19 calls for quick and effective tracking of virus transmis- sion chains and early detection of outbreaks, especially in the “phase 2” of the pandemic, when lockdown and other restriction measures are progressively withdrawn, in order to avoid or mini- mize contagion resurgence. For this purpose, contact-tracing apps are being proposed for large scale adoption by many countries. A centralized approach, where data sensed by the app are all sent to a nation-wide server, raises concerns about citizens’ privacy and needlessly strong digital surveillance, thus alerting us to the need to minimize personal data collection and avoiding location tracking. We advocate the conceptual advantage of a decentralized approach, where both contact and location data are collected exclusively in individual citizens’ “personal data stores”, to be shared separately and selectively (e.g., with a backend system, but possibly also with other citizens), voluntarily, only when the citizen has tested positive for COVID-19, and with a privacy preserving level of granularity. This 61 62 Mirco Nanni et al. approach better protects the personal sphere of citizens and affords multiple beneﬁts: it allows for detailed information gathering for infected people in a privacy-preserving fashion; and, in turn this enables both contact tracing, and, the early detection of outbreak hotspots on more ﬁnely-granulated geographic scale. The decentralized approach is also scalable to large populations, in that only the data of positive patients need be handled at a central level. Our recommendation is two-fold. First to extend existing decentralized architectures with a light touch, in order to manage the collection of lo- cation data locally on the device, and allow the user to share spatio-temporal aggregates – if and when they want and for speciﬁc aims – with health authorities, for instance. Second, we favour a longer- term pursuit of realizing a Personal Data Store vision, giving users the opportunity to contribute to collective good in the measure they want, enhancing self-awareness, and cultivating collective efforts for rebuilding society. Keywords. COVID-19, Personal Data Store, mobility data analysis, contact tracing. 1conclusion not found"
Personal Health Informatics,Stephen Intille,https://www.khoury.northeastern.edu/people/stephen-intille/,"Stephen Intille is a professor in the Khoury College of Computer Sciences and the Bouvé College of Health Sciences at Northeastern University, based in Boston.","Personal health informatics; Interactive, mobile sensing; Machine learning; Behavioral theory and measurement","PhD in Media Arts and Sciences, Massachusetts Institute of Technology; SM in Media Arts and Sciences, Massachusetts Institute of Technology; BSE in Computer Science and Engineering, University of Pennsylvania","January 1st, 2022",Techno-Spiritual Engagement: Mechanisms for Improving Uptake of mHealth Apps Designed for Church Members 130-138,http://ceur-ws.org/Vol-3124/paper13.pdf," Hye Sun Yun, Shou Zhou, Everlyne Kimani, Stefan Olafsson, Teresa K. O'Leary, Dhaval Parmar, Jessica A. Hoffman, Stephen S. Intille, Michael K. Paasche-Orlow, Timothy W. Bickmore. (2022). Techno-Spiritual Engagement: Mechanisms for Improving Uptake of mHealth Apps Designed for Church Members 130-138 IUI Workshops, 130-138. http://ceur-ws.org/Vol-3124/paper13.pdf","Keeping users engaged with mHealth applications is important but difficult to achieve. We describe the development of a smartphone-based application designed to promote health and wellness in church communities, along with mechanisms explicitly designed to maintain engagement. We evaluated religiously tailored techno-spiritual engagement mechanisms, including a prayer posting wall, pastor announcements, an embodied conversational agent for dialogue-based scriptural reflections and health coaching, and tailored push notifications. We conducted a four-week pilot study with 25 participants from two churches, measuring high levels of participant acceptance and satisfaction with all features of the application. Engagement with the app was higher for users considered to be more religious and correlated with the number of notifications received. Our findings demonstrate that our tailored mechanisms can increase engagement with an mHealth app. Keywords engagement, tailoring, field study, church communities, mHealth,s and Limitations Maintaining user retention with mobile interventions is essential for affecting longitudinal outcomes in health, education, and other application domains. Utilizing well- tailored engagement mechanisms that meet the interests and needs of a priority population may be useful in main- taining engagement with the app over time and more effective at providing help and interventions to users. We explored a range of religiously tailored engagement strategies that can be used to motivate church community members to interact with a smartphone-based mHealth app. We described and evaluated six religiously tailored engagement mechanisms in this pilot study: an ECA, Prayer Center, Pastor Announcements, Bible Story of the Day, Scriptural Meditation, and push notifications. We demonstrated that push notifications were effective at driving the use of the app, with the number of notifica- tions per day and the user’s private religious practices score being significant factors in predicting the number of user logins per day throughout the study. We found that user satisfaction with all elements of the app was high with several participants finding Clara to be relatable and having personal resonance with the religiosity, spir- ituality, and role she exhibits. Our study demonstrated that religiously tailored engagement mechanisms that fulfill techno-spiritual functions can help religious users engage with an mHealth application. We did not find that the use of the app led to signif- icant pre-post improvements in health attitudes or be- haviors, likely due to the short duration of the study and a small convenience sample. We plan to evaluate the effectiveness of our app in improving health behaviors through future studies and used this pilot study to mainly evaluate the engagement mechanisms. Also, due to the participants of our pilot study being financially compen- sated, we can expect some level of response bias where engagement and satisfaction with our app can partly be associated with being compensated rather than the en- gagement mechanisms. In addition, we did not explore the duration of ECA sessions or scrolling through the Prayer Wall as a measure of engagement. Measuring session duration in light of app suspension, exits, and interrupts is very error-prone, and we felt that these data were too noisy to warrant analysis. Finally, this pilot study lacked a control condition, and a series of studies to systematically evaluate each engagement mechanism relative to a control is ultimately needed."
Robotics,Robert Platt,https://www.khoury.northeastern.edu/people/robert-platt/,"Robert Platt is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is also affiliated with the College of Engineering.","Perception, planning, and control for robotic manipulation","PhD in Computer Science, University of Massachusetts Amherst","February 27th, 2023",Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,https://openreview.net/pdf?id=_2bDpAtr7PI," David Klee, Ondrej Biza, Robert Platt, Robin Walters. (2023). Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction ICLR. https://openreview.net/pdf?id=_2bDpAtr7PI","Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in SO(3). How- ever, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages SO(3) equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at https://dmklee.github.io/image2sphere. 1In this work, we present the first method to leverage SO(3)-equivariance for predicting distributions over 3D rotations from single images. Our method is better suited than regression methods at handling unknown object symmetries, generates more expressive distributions than methods using parametric families of multi-modal distributions while requiring fewer samples than an implicit modeling approach. We demonstrate state-of-the-art performance on the challenging PASCAL3D+ dataset composed of real images. One limitation of our work is that we use a high maximum frequency, L, in the spherical convolution operations to have higher resolution predictions. Because the number of operations in a spherical convolution is quadratic in L, it may be impractical for applications where more spherical convolutions are required. 9 Published as a conference paper at ICLR 2023"
Robotics,Robert Platt,https://www.khoury.northeastern.edu/people/robert-platt/,"Robert Platt is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is also affiliated with the College of Engineering.","Perception, planning, and control for robotic manipulation","PhD in Computer Science, University of Massachusetts Amherst","November 16th, 2022",The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,https://openreview.net/pdf?id=P4MUGRM4Acu," Dian Wang , Jung Yeon Park, Neel Sortur, Lawson L. S. Wong, Robin Walters, Robert Platt. (2023). The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry ICLR. https://openreview.net/pdf?id=P4MUGRM4Acu","Extensive work has demonstrated that equivariant neural networks can signifi- cantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the do- main symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the in- put. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surpris- ingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, impos- ing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. 1conclusion not found"
Robotics,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","February 20th, 2023",Improving Deep Policy Gradients with Value Function Search,https://openreview.net/pdf?id=6qZC7pfenQm," Enrico Marchesini, Christopher Amato. (2023). Improving Deep Policy Gradients with Value Function Search ICLR. https://openreview.net/pdf?id=6qZC7pfenQm","Deep Policy Gradient (PG) algorithms employ value networks to drive the learn- ing of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approxima- tion and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the su- pervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains. 1VFS introduces a two-scale perturbation operator voted to diversify a population of value networks to (i) explore local variations of current critics’ predictions and (ii) allow to explore diversified value functions to escape from local optima. The practical results of such components have been investigated with additional experiments that also motivate the improvement in sample efficiency and performance of VFS-based algorithms in a range of standard continuous control benchmarks. Our findings suggest that improving fundamental Deep PG primitives translates into higher-performing policies and better sample efficiency. 9 Published as a conference paper at ICLR 2023 7"
Robotics,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","July 5th, 2018",Decision-Making Under Uncertainty in Multi-Agent and Multi-Robot Systems: Planning and Learning,http://www.ccs.neu.edu/home/camato/publications/ijcai18.pdf," Christopher Amato. In the Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18), July 2018","Multi-agent planning and learning methods are be- coming increasingly important in today’s intercon- nected world. Methods for real-world domains, such as robotics, must consider uncertainty and limited communication in order to generate high- quality, robust solutions. This paper discusses our work on developing principled models to represent these problems and planning and learning methods that can scale to realistic multi-agent and multi- robot tasks. 1The methods that have been discussed have shown a lot of promise, but there are other methods that are also promising (e.g., [Dibangoye et al., 2016; Claes et al., 2017; Nguyen et al., 2017]) and many open questions yet to solve. It is worth noting that the methods in this paper focused on the ‘full’ Dec-POMDP problem, where there was uncertainty about outcomes, sensing and communication and agents are depen- dent on all others, but some problems do not have all of these characteristics. In these cases, ideas from Dec-POMDP mod- els and methods could still be used, but structure may be able to be exploited to allow higher-quality, more efﬁcient solution methods. Some such structure has been explored [Oliehoek and Amato, 2016], but efﬁciently exploiting structure in other cases could lead to scalable methods that also consider un- certainty (e.g., probabilistic solutions to common multi-robot problems). Traditionally, scalability of Dec-POMDPs solution meth- ods has been an issue, but more recent methods (such as those discussed in this paper) can scale to large domains. One approach that is used to overcome this lack of scal- ability is deep reinforcement learning. Developing deep RL approaches for Dec-POMDP-based models is becom- ing a very active ﬁeld (e.g., [Foerster et al., 2016; 2017; Mordatch and Abbeel, 2017; Omidshaﬁei et al., 2017c; Rashid et al., 2018]) and the resulting methods can handle large state and observation spaces (and potentially large ac- tion spaces). These methods (including ours) require a lot of data and typically use a simulator, so they may ﬁt better in situations akin to the ofﬂine sample-based planning scenarios described earlier. Therefore, there has been a great deal of success in scal- ing planning and ofﬂine learning methods to large domains, but efﬁcient online learning remains a challenge. As men- tioned above, online learning in a Dec-POMDP must be de- centralized (since fast and free communication is not avail- able to centralize decision-making), leading to nonstationar- ity, which must (continue to) be tackled. Besides more effec- tively conquering nonstationarity in learning, other open re- search topics include developing planning and learning meth- ods with additional efﬁciency and scalability (in terms of the number of agents as well as action and observation spaces) and how to properly represent and encode history informa- tion for planning and learning. Overall, there has been great progress in solving large, realistic Dec-POMDPs. It will be exciting to see what further developments and applications will arise in the future."
Robotics,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","May 5th, 2018",Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems,http://www.ccs.neu.edu/home/camato/publications/ICRA2018.pdf," Nghia Hoang, Yuchen Xiao, Kavinayan Sivakumar, Christopher Amato and Jonathan P. How. In the Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA-18), May 2018.","— A key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self- interested or even adversarial parties who actively try to prevent the agents from achieving their goals. The practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with ﬁxed, procedurally characterized strategies. In contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies. This problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly. Our approach ﬁrst optimizes a set of stratagems that represent these best responses. These optimized stratagems are then inte- grated into a uniﬁed policy that can detect and respond when the adversaries change their strategies. The near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware.This paper introduces a novel near-optimal adversarial pol- icy switching algorithm for decentralized, non-cooperative multi-agent systems. Unlike the existing works in literature which are mostly limited to simple decision-making sce- narios where a single agent plans its best response against an adversary whose strategy is speciﬁed a priori under reasonable assumptions, we investigate instead a class of multi-agent scenarios where multiple robots need to operate independently in collaboration with their teammates to act effectively against adversaries with changing strategies. To achieve this, we ﬁrst optimize a set of basic stratagems that each is tuned to respond optimally to a pre-identiﬁed basic tactic of the adversaries. The stratagems are then integrated into a uniﬁed policy which performs near-optimally against B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 (a) (b) (c) B3 B2 B1 R1 R2 B1 B3 B2 R1 R2 B1 B3 B2 R1 R2 (d) (e) (f) Fig. 6: Image excerpts from a video demo showing (1) a team of 3 allied (blue) robots (B1,B2 and B3) that implement the optimized stratagem produced by our framework (Section III) to compete against (2) an opposing team of 2 opponent (red) robots (R1 and R2) which implement the hand-coded tactics DL and DR (see Section VI-A), respectively: (a) B1,B2 and B3 decide to invade the opposition territory; (b) B1 and B3 decide to attack the center while B2 decides to take the left ﬂank of the opposition; (c) B2 passes through R1’s defense while B1 takes an interesting position to block R2 so that B3 can pass through its defense; (d) B1 and B2 detect the ﬂag and mount a pincer attack; (e) R2 arrives to defend the ﬂag and B2 retreats to avoid getting tagged; and (f) without noticing B1 from behind, R2 continues its DR patrol, thus losing the ﬂag to B1. any high-level strategies of the adversaries that switches between their basic tactics. The near-optimality of our pro- posed framework can be established in both theoretical and empirical settings with interesting and consistent results. We believe this is a signiﬁcant step towards bridging the gap between theory and practice in multi-agent research."
Robotics,Christopher Amato,https://www.khoury.northeastern.edu/people/chris-amato/,"Christopher Amato is an associate professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Artificial intelligence; Machine learning; Robotics,"PhD in Computer Science, University of Massachusetts, Amherst; MS in Computer Science, University of Massachusetts, Amherst; BA in Clinical Psychology and Philosophy, Tufts University","August 4th, 2017",COG-DICE: An Algorithm for Solving Continuous-Observation Dec-POMDPs,http://www.ccs.neu.edu/home/camato/publications/cogdice_ijcai.pdf," COG-DICE: An Algorithm for Solving Continuous-Observation Dec-POMDPs. Madison Clark-Turner and Christopher Amato. In the Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17), August 2017","The decentralized partially observable Markov de- cision process (Dec-POMDP) is a powerful model for representing multi-agent problems with de- centralized behavior. Unfortunately, current Dec- POMDP solution methods cannot solve problems with continuous observations, which are common in many real-world domains. To that end, we present a framework for representing and gener- ating Dec-POMDP policies that explicitly include continuous observations. We apply our algorithm to a novel tagging problem and an extended version of a common benchmark, where it generates poli- cies that meet or exceed the values of equivalent discretized domains without the need for ﬁnding an adequate discretization. 1This paper presented, for the ﬁrst time, an algorithm that gen- erates joint policies for Dec-POMDPs with continuous ob- servations. We presented both a discrete-observation ver- sion of the algorithm, which is applicable in domains with a large number of discrete observations, and a continuous- observation version. This method is broadly applicable as many real-world domains have large or continuous observa- tion spaces. COG-DICE has been successful in generating joint policies for both a novel and a preexisting problem and has highlighted the negative impacts that inappropriate dis- cretization can have on joint policy structure and value. For future work, we are interested in extending this work to high- dimensional observation spaces by exploring other (nonlin- ear) divisions and optimizing the algorithm parameters by ei- ther integrating these optimizations into the algorithm or pos- sibly building on previous work on Bayesian non-parametrics [Liu et al., 2015]."
Robotics,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","February 27th, 2023",Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction,https://openreview.net/pdf?id=_2bDpAtr7PI," David Klee, Ondrej Biza, Robert Platt, Robin Walters. (2023). Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction ICLR. https://openreview.net/pdf?id=_2bDpAtr7PI","Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in SO(3). How- ever, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages SO(3) equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at https://dmklee.github.io/image2sphere. 1In this work, we present the first method to leverage SO(3)-equivariance for predicting distributions over 3D rotations from single images. Our method is better suited than regression methods at handling unknown object symmetries, generates more expressive distributions than methods using parametric families of multi-modal distributions while requiring fewer samples than an implicit modeling approach. We demonstrate state-of-the-art performance on the challenging PASCAL3D+ dataset composed of real images. One limitation of our work is that we use a high maximum frequency, L, in the spherical convolution operations to have higher resolution predictions. Because the number of operations in a spherical convolution is quadratic in L, it may be impractical for applications where more spherical convolutions are required. 9 Published as a conference paper at ICLR 2023"
Robotics,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","February 1st, 2023","Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow",https://openreview.net/pdf?id=9ZpciCOunFb," Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, Nima Dehmamy. (2023). Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow ICLR. https://openreview.net/pdf?id=9ZpciCOunFb","Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neu- ral networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness un- der certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating con- served quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability. 1conclusion not found"
Robotics,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","November 16th, 2022",The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,https://openreview.net/pdf?id=P4MUGRM4Acu," Dian Wang , Jung Yeon Park, Neel Sortur, Lawson L. S. Wong, Robin Walters, Robert Platt. (2023). The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry ICLR. https://openreview.net/pdf?id=P4MUGRM4Acu","Extensive work has demonstrated that equivariant neural networks can signifi- cantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the do- main symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the in- put. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surpris- ingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, impos- ing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. 1conclusion not found"
Robotics,Robin Walters,https://www.khoury.northeastern.edu/people/robin-walters/,"Robin Walters is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Deep learning; Equivariant neural networks; Representation theory; Algebraic geometry,"PhD in Mathematics, University of Chicago; AB in Mathematics, Harvard University","June 8th, 2022",Integrating Symmetry into Differentiable Planning with Steerable Convolutions,https://openreview.net/pdf?id=n7CPzMPKQl," Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L. S. Wong. (2023). Integrating Symmetry into Differentiable Planning with Steerable Convolutions ICLR. https://openreview.net/pdf?id=n7CPzMPKQl","In this paper, we study a principled approach on incorporating group symme- try into end-to-end differentiable planning algorithms and explore the benefits of symmetry in planning. To achieve this, we draw inspiration from equivariant con- volution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant op- erator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navi- gation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2- DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN. 1, SymVIN and SymGPPN generalize better to different map sizes, com- pared to all non-equivariant baselines. Remark. In summary, our results show that the Sym- Plan models demonstrate end-to-end planning and learn- ing ability, potentially enabling further applications to other tasks as a differentiable component for planning. Additional results and ablation studies are in Appendix H. 7 DISCUSSION In this work, we study the symmetry in the 2D path-planning problem, and build a framework using the theory of steerable CNNs to prove that value iteration in path planning is actually a form of steer- able CNN (on 2D grids). Motivated by our theory, we proposed two symmetric planning algorithms that provided significant empirical improvements in several path-planning domains. Although our focus in this paper has been on Z2, our framework can potentially generalize to path planning on higher-dimensional or even continuous Euclidean spaces (Weiler et al., 2018; Brandstetter et al., 2021), by using equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We hope that our SymPlan frame- work, along with the design of practical symmetric planning algorithms, can provide a new pathway for integrating symmetry into differentiable planning. 9 Published as a conference paper at ICLR 2023 8 ACKNOWLEDGEMENT This work was supported by NSF Grants #2107256 and #2134178. R. Walters is supported by The Roux Institute and the Harold Alfond Foundation. We also thank the audience from previous poster and talk presentations for helpful discussions and anonymous reviewers for useful feedback. 9 REPRODUCIBILITY STATEMENT We provide additional details in the appendix. We also plan to open source the codebase. We briefly outline the appendix below. 1. Additional Discussion 2. Background: Technical background and concepts on steerable CNNs and group CNNs 3. Method: we provide full details on how to reproduce it 4. Theory/Framework: we provide the complete version of the theory statements 5. Proofs: this includes all proofs 6. Experiment / Environment / Implementation details: useful details for reproducibility 7. Additional results 10 Published as a conference paper at ICLR 2023"
Robotics,Lawson Wong,https://www.khoury.northeastern.edu/people/lawson-wong/,"Lawson L.S. Wong is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"PhD, Massachusetts Institute of Technology","November 16th, 2022",The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry,https://openreview.net/pdf?id=P4MUGRM4Acu," Dian Wang , Jung Yeon Park, Neel Sortur, Lawson L. S. Wong, Robin Walters, Robert Platt. (2023). The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry ICLR. https://openreview.net/pdf?id=P4MUGRM4Acu","Extensive work has demonstrated that equivariant neural networks can signifi- cantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the do- main symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the in- put. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surpris- ingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, impos- ing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems. 1conclusion not found"
Robotics,Lawson Wong,https://www.khoury.northeastern.edu/people/lawson-wong/,"Lawson L.S. Wong is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"PhD, Massachusetts Institute of Technology","October 24th, 2022",Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation,https://openreview.net/pdf?id=PYbe4MoHf32," Linfeng Zhao, Huazhe Xu, Lawson L. S. Wong. (2023). Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation ICLR. https://openreview.net/pdf?id=PYbe4MoHf32","Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which cou- ples forward computation and backpropagation and needs to balance forward plan- ner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to de- couple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scal- ability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace. 1already mentioned in the above section: Beyond an intermediate iteration number (around 30-50), IDPs are more favorable because of scalability and computational cost. We present other analyses here. We start from ConvGPPN and ID-ConvGPPN, which perform the best in ADPs and IDPs class, re- spectively. They also have the most number of parameters and use greatest time because of the gates in ConvGRU units. As shown in Figure 2, this also caused two issues of ConvGPPN: scalability to larger maps/iterations (out of memory for 27 × 27 80 iterations and 49 × 49 50 and 80 iterations), and also convergence stability (e.g. 27 × 27 50 iterations). For SymVIN and ID-SymVIN, they replace Conv2D with SteerableConv, with computational cost slightly higher than VIN and much lower than ConvGPPN. Thus, they can successfully run on all tasks and iteration numbers. However, we find that explicit SymVIN may diverge due to bad ini- tialization, and this is more severe if the network is deeper (more iterations), as in Figure 2’s 50 and 7 Published as a conference paper at ICLR 2023 30 50 80 #Layers Klayer 0 100 200 Forward Time Task = 15 × 15 30 50 80 #Layers Klayer Task = 27 × 27 30 50 80 #Layers Klayer Task = 49 × 49 30 50 80 Forward Kfwd 0 100 200 Forward Time Task = 15 × 15 30 50 80 Forward Kfwd Task = 27 × 27 30 50 80 Forward Kfwd Task = 49 × 49 30 50 80 #Layers Klayer 0 50 100 Backward Time Task = 15 × 15 30 50 80 #Layers Klayer Task = 27 × 27 30 50 80 #Layers Klayer Task = 49 × 49 VIN SymVIN ConvGPPN 30 50 80 Forward Kfwd 0 50 100 Backward Time Task = 15 × 15 30 50 80 Forward Kfwd Task = 27 × 27 30 50 80 Forward Kfwd Task = 49 × 49 ID-VIN ID-SymVIN ID-ConvGPPN Figure 6: The runtime (in seconds) on 2D navigation tasks with size 15 × 15, 27 × 27, and 49 × 49, averaged over 5 seeds. The ADPs are on the left six figures and the IDPs on the right. Missing dots are due to out of memory caused by algorithmic differentiation. The upper row is for forward pass runtime, and the lower row is for backward runtime. The horizontal axes mean differently: (1) ADPs: the number of layers Klayer, also number of iterations, and (2) IDPs: the forward pass iterations Kfwd. 0 10 20 30 40 50 60 Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Successful Rate 0 10 20 30 40 50 60 Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Successful Rate VIN ID-VIN SymVIN ID-SymVIN ConvGPPN ID-ConvGPPN Figure 7: (Left) Training curves on 2D maze navigation 15 × 15 maps, with 80 layers for ADPs and 80 iterations for IDPs. (Right) Training curves on 49 × 49 maps, with 30 layers for ADPs (due to scalability issue) and 80 iterations for IDPs. 80 iterations. Nevertheless, ID-SymVIN alleviates this issue, since implicit differentiable planning decouples forward and backward pass, so the gradient computation is not affected by forward pass. Furthermore, VIN and ID-VIN are surprisingly less affected by the number of iterations and problem scale. We find their forward passes tends to converge faster to reasonable equilibria, thus further increasing iteration does not help, nor break convergence as long as memory is sufficient. Forward and backward runtime. We visualize the runtime of IDPs and ADPs in Figure 6. For IDPs, we use the forward-iteration solver for the forward pass and Anderson solver for the backward pass. Note that in the bottom left, we intentionally plot backward runtime vs. forward pass iterations. This emphasizes that IDPs decouple forward and backward passes because the backward runtime does not rely on forward pass iterations. Instead, for ADPs, value iteration is done by network layers, thus the backward pass is coupled: the runtime increases with depth and some runs failed due to limited memory (11GB, see missing dots). Therefore, this set of figures shows better scalability of IDPs (no missing dots – out of memory – and constant backward time). In terms of absolute time, the forward runtime of IDPs when using the forward solver is comparable with successful ADPs. 5.3 TRAINING PERFORMANCE Setup. Beyond evaluating generalization to novel maps, we compare their training efficiency with learning curves. Each learning curve is aggregated over 5 seeds, which are from the models in the above section. The learning curves are for all planners on 15 × 15 maps (Figure 7 left) and 49 × 49 maps (Figure 7 right, 30 layers for ADPs – due to scalability issue – and 80 iterations for IDPs). Results. On 15×15 maps, we show Klayer = 80 layers for ADPs and Kfwd = 80 iterations for IDPs. ID-ConvGPPN performs the best and is much more stable than its ADP counterpart ConvGPPN. ID- SymVIN learns reliably, while SymVIN fails to converge due to instability from 80 layers. ID-VIN and VIN are comparable throughout training. On 49 × 49 maps, we visualize Klayer = 30 layers 8 Published as a conference paper at ICLR 2023 for ADPs (due to their limited scalability) and Kfwd = 80 iterations for IDPs. ConvGPPN cannot run at all even for only 30 layers, while ID-ConvGPPN still reaches a near-perfect success rate. ID- SymVIN learns slightly better than SymVIN and reaches higher asymptotic performance. ID-VIN has a similar trend to VIN, but performs worse overall due to the complexity of the task. 5.4 PERFORMANCE ON MORE TASKS Table 1: Averaged test success rate (%) over 5 seeds for using 10K/2K/2K dataset on the rest of 3 types of tasks. We highlight entry with italic for runs with at least one diverged trial (any success rate < 20%). Type Methods 18 × 18 Mani. 36 × 36 Mani. Workspace Mani. Visual Nav. Explicit VIN 89.65±7.97 74.75±8.18 80.98±3.84 66.11±8.91 SymVIN 55.15±49.54 65.72±47.11 82.17±24.72 96.04±4.24 ConvGPPN 79.71±20.71 70.55±36.13 70.23±19.44 81.76±31.04 Implicit (ours) ID-VIN 80.53±6.98 56.27±20.92 77.17±7.24 62.53±15.93 ID-SymVIN 99.63±0.08 98.53±1.42 87.60±24.11 86.41±30.34 ID-ConvGPPN 97.28±0.74 93.60±1.68 92.60±1.83 98.91±0.34 Setup. We run all planners on the other three challenging tasks. For visual navigation, we randomly generate 10K/2K/2K maps using the same strategy as 2D navigation and then render four egocentric panoramic views for each location from produced 3D environments with Gym- MiniWorld (Chevalier-Boisvert, 2018). For configuration-space manipulation and workspace manipulation, we randomly generate 10K/2K/2K tasks with 0 to 5 obstacles in workspace. In configuration-space manipulation, we manually convert each task into a 18 × 18 or 36 × 36 map (20◦or 10◦per bin). The workspace task additionally needs a mapper network to convert the 96×96 workspace (image of obstacles) to an 18 × 18 2-DOF configuration space (2D occupancy grid). We provide additional details in the Section D. Results. In Table 1, due to space limitations, we average over Klayer = 30, 50, 80 for ADPs and Kfwd = 30, 50, 80 for IDPs. For each task, we present the mean and standard deviation over 5 seeds times three hyperparameters and provide the separated results to Section E. We italicize entries for runs with at least one diverged trial (any success rate < 20%). Generally, IDPs perform much more stably. On 18×18 or 36×36 configuration-space manipulation, ID-SymVIN and ID-ConvGPPN reach almost perfect results, while ID-VIN has diverged runs on 36×36 (marked in italic). SymVIN and ConvGPPN are more unstable, while VIN even outperforms them and is also better than ID-VIN. On 18×18 workspace manipulation, because of the difficulty of jointly learning maps and potentially planning on inaccurate maps, most numbers are worse than in configuration-space. ID-ConvGPPN still performs the best, and other methods are comparable. For 15×15 visual navigation, it needs to learn a mapper from panoramic images and is more challenging. ID-ConvGPPN is still the best. ID-SymVIN exhibits some failed runs and gets underperformed by SymVIN in these seeds, and ID-VIN is comparable with VIN. Across all tasks, the results confirm the superiority of scalability and convergence stability of IDPs and demonstrate the ability of jointly training mappers (with algorithmic differentiation for this layer) even when using implicit differentiation for planners. 6 CONCLUSION This work studies how VIN-based differentiable planners can be improved from an implicit-function perspective: using implicit differentiation to solve the equilibrium imposed by the Bellman equation. We develop a practical pipeline for implicit differentiable planning and propose implicit versions of VIN, SymVIN, and ConvGPPN, which is comparable to or outperforms their explicit counterparts. We find that implicit differentiable planners (IDPs) can scale up to longer planning-horizon tasks and larger iterations. In summary, IDPs are favorable for these cases to ADPs for several reasons: (1) better performance mainly due to stability, (2) can scale up while some ADPs fail due to memory limit, (3) less computation time. On the contrary, if using too few iterations, the equilibrium may be poorly solved, and ADPs should be used instead. While we focus on value iteration, the idea of implicit differentiation is general and applicable beyond path planning, such as in continuous control where Neural ODEs can be deployed to solving ODEs or PDEs. 9 Published as a conference paper at ICLR 2023 7 ACKNOWLEDGEMENT This work was supported by NSF Grant #2107256. We also thank Clement Gehring and Lingzhi Kong for helpful discussions and anonymous reviewers for useful feedback. 8 REPRODUCIBILITY STATEMENT We provide an appendix with extended details of implementation in Section D, experiment details in Section D, and additional results in Section E. We plan to open-source our code next. 10 Published as a conference paper at ICLR 2023"
Robotics,Lawson Wong,https://www.khoury.northeastern.edu/people/lawson-wong/,"Lawson L.S. Wong is an assistant professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",,"PhD, Massachusetts Institute of Technology","June 8th, 2022",Integrating Symmetry into Differentiable Planning with Steerable Convolutions,https://openreview.net/pdf?id=n7CPzMPKQl," Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L. S. Wong. (2023). Integrating Symmetry into Differentiable Planning with Steerable Convolutions ICLR. https://openreview.net/pdf?id=n7CPzMPKQl","In this paper, we study a principled approach on incorporating group symme- try into end-to-end differentiable planning algorithms and explore the benefits of symmetry in planning. To achieve this, we draw inspiration from equivariant con- volution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant op- erator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navi- gation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2- DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN. 1, SymVIN and SymGPPN generalize better to different map sizes, com- pared to all non-equivariant baselines. Remark. In summary, our results show that the Sym- Plan models demonstrate end-to-end planning and learn- ing ability, potentially enabling further applications to other tasks as a differentiable component for planning. Additional results and ablation studies are in Appendix H. 7 DISCUSSION In this work, we study the symmetry in the 2D path-planning problem, and build a framework using the theory of steerable CNNs to prove that value iteration in path planning is actually a form of steer- able CNN (on 2D grids). Motivated by our theory, we proposed two symmetric planning algorithms that provided significant empirical improvements in several path-planning domains. Although our focus in this paper has been on Z2, our framework can potentially generalize to path planning on higher-dimensional or even continuous Euclidean spaces (Weiler et al., 2018; Brandstetter et al., 2021), by using equivariant operations on steerable feature fields (such as steerable convolutions, pooling, and point-wise non-linearities) from steerable CNNs. We hope that our SymPlan frame- work, along with the design of practical symmetric planning algorithms, can provide a new pathway for integrating symmetry into differentiable planning. 9 Published as a conference paper at ICLR 2023 8 ACKNOWLEDGEMENT This work was supported by NSF Grants #2107256 and #2134178. R. Walters is supported by The Roux Institute and the Harold Alfond Foundation. We also thank the audience from previous poster and talk presentations for helpful discussions and anonymous reviewers for useful feedback. 9 REPRODUCIBILITY STATEMENT We provide additional details in the appendix. We also plan to open source the codebase. We briefly outline the appendix below. 1. Additional Discussion 2. Background: Technical background and concepts on steerable CNNs and group CNNs 3. Method: we provide full details on how to reproduce it 4. Theory/Framework: we provide the complete version of the theory statements 5. Proofs: this includes all proofs 6. Experiment / Environment / Implementation details: useful details for reproducibility 7. Additional results 10 Published as a conference paper at ICLR 2023"
Software Engineering,Karl Lieberherr,https://www.khoury.northeastern.edu/people/karl-lieberherr/,"Karl Lieberherr is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Modularity in software development; Test-driven development for relational queries using neural networks; Socio-technical systems for problem solving in STEM areas involving competition and collaboration,"PhD, ETH Zurich — Switzerland; MS in mathematics, ETH Zurich — Switzerland","February 11th, 2016",Design and Secure Evaluation of Side-Choosing Games,http://www.ccs.neu.edu/home/lieber/papers/side-choosing-games/wit-ec16/design-secure-evaluation-SCG-WIT-EC16.pdf," Ahmed Abdelmeged, Ruiyang Xu and Karl Lieberherr. Design and Secure Evaluation of Side-Choosing Games. AAAI 2016 Workshop on Incentive and Trust in E-Communities (WIT-EC'16), 4 pages.","We present an important, general class of new games, called side-choosing games (SCGs), for “gamifying” problem solving in formal sciences. Applications of SCGs include (1) peer-grading in teaching to (2) study- ing the evolution of knowledge in formal sciences to (3) organizing algorithm competitions. We view SCGs as a new and general model for formulating formal problems that need to be solved using human compu- tation. Our interest in this paper is on how to evalu- ate an SCG tournament fairly and effectively. We ob- serve that a speciﬁc kind of collusion, where players lie about their strength and sacriﬁce themselves, could bias the evaluation of SCG tournaments dramatically. Fol- lowing the idea of Social Choice Theory in the sense of Arrow, we take an axiomatic approach to guaran- tee that a speciﬁc kind of collusion is impossible. We prove the Collusion-Resistance Theorem as a general principle for designing collusion-resistant evaluations for SCG tournaments. The Collusion-Resistance The- orem is surprising: it tells us to be indifferent to wins but to count certain kinds of losses for scoring players and ranking them. If collusion is not an issue, we of- fer a family of useful ranking functions which are not collusion-resistant. 1We propose the concept of side-choosing Game (SCG) as a model for plausibility checking of claims using a general- ization of extensive form games. SCGs are useful for orga- nizing techno-social systems for creating knowledge in For- mal Sciences. Considering that a speciﬁc kind of collusion might compromise the truth, we modeled the ranking of par- ticipants functionally via three axioms or postulates: NNEW (Non-Negative Effect for Winning), NPEL (Non-Positive Effect for Losing) and the crucial axiom CR (Collusion- resistance, which says that games where one is not in con- trol cannot affect ones ranking, hence preventing gaming the game). We prove the Collusion-Resistance Theorem which states that ranking has to be based on fault counting. What comes next? Our plan is to deploy SCG-based ap- plications on the web and gather the beneﬁts of collective in- telligence. So far, we have already applied SCG-based ideas and tools in designing courses at Northeastern University from algorithm and software development courses to basic courses on spreadsheets and databases. And we were plan- ning to build a tool that can be used in MOOCs or algorithm competitions. An implementation of a domain-speciﬁc lan- guage for human computation in formal sciences is a chal- lenge that requires several algorithms to be developed. Why not develop those algorithms with SCG-based human com- putation effectively bootstrapping the system based on user feedback. We view SCG as the programming language for human computation to solve complex problems. Another important area that needs further work is where participants can propose new claims. A modular approach to solving claims is needed. For example, a complex claim C1might be reducible to a simpler claim C2 so that a solu- tion for C2 implies a solution for C1. We propose a formal study of claim relations which can themselves be captured as claims and approached with side-choosing games. Collusion is linked to trust in a tournament to ﬁnd the best players. Collusion-resistance eliminates some collusion but there is still other collusion possible. We will report on this at the workshop."
Software Engineering,Jan Vitek,https://www.khoury.northeastern.edu/people/jan-vitek/,"Jan Vitek is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Programming languages; Compilers; Program analysis; Memory management; Virtual machine; Type systems,"PhD in information systems, University of Geneva — Switzerland; MS in computer science, University of Victoria — Canada","November 15th, 2017",Correctness of Speculative Optimizations with Dynamic Deoptimization,https://arxiv.org/pdf/1711.03050.pdf," Olivier Fluckiger, Gabriel Scherer, Ming-Ho Yee, Aviral Goel, Amal Ahmed and Jan Vitek","machine state. The relation C Aτ −→C′ specifies that executing the next instruction may result in the configuration C′. The action Aτ indicates whether this reduction is observable: it is either the silent action, written τ, an I/O action read lit or print lit, or stop. We write C T −→∗C′ when there are zero or more steps from C to C′. The trace T is a list of non-silent actions in the order in which they appeared. Actions are defined in Figure 16, and the full reduction relation is given in Figure 17. Most rules get the current instruction, I(L), perform an operation, and advance to the next label, referred to by the shorthand (L +1). The read lit and print lit actions represent observable I/O operations. They are emitted by Read and Print in Figure 17. The action read lit on the read x transition may be any literal value. This is the only reduction rule that is non-deterministic. Note that the relation C −→∗C′, containing only sequences of silent reductions, is deterministic. The Proceedings of the ACM on Programming Languages, Vol. 2, No. POPL, Article 49. Publication date: January 2018. Correctness of Speculative Optimizations 49:15 A ::= I/O action | print lit | read lit | stop Aτ ::= | A | τ silent label T ::= action trace | (empty trace) | A | Aτ | T A | T Aτ [Refl] C −→∗C [SilentCons] C T −→∗C′ C′ τ −→C′′ C T −→∗C′′ [ActionCons] C T −→∗C′ C′ A −→C′′ C T A −→∗C′′ Fig. 16. Actions and traces. stop reduction emits the stop transition, and also produces a configuration with no instructions, ∅. This is a technical device to ensure that the resulting configuration is stuck. A program with a silent loop has a different trace from a program that halts. Given a program P, let start(P) be its starting configuration, and reachable(P) be the set of configurations reachable from it; they are all the states that may be encountered during a valid run of P. [StartConf] I def= P(main, active) L def= start(I) start(P) def= ⟨P I L ∅∅∅⟩ reachable(P) def= {C | ∃T, start(P) T −→∗C} 5.3 Equivalence of Configurations: Bisimulation We use weak bisimulation to prove equivalence between configurations. The idea is to define, for each program transformation, a correspondence relation R between configurations over the source and transformed programs. We show that related configurations have the same observable behavior, and reducing them results in configurations that are themselves related. Two programs are equivalent if their starting configurations are related. Definition 5.1 (Weak Bisimulation). Given programs P1 and P2 and relation R between the config- urations of P1 and P2, R is a weak simulation if for any related states (C1, C2) ∈R and any reduction C1 Aτ −→C′ 1 over P1, there exists a reduction C2 Aτ −→∗C′ 2 over P2 such that (C′ 1, C′ 2) are themselves related by R. Reduction over P2 is allowed to take zero or more steps, but not to change the trace. In other words, the diagram on the left below can always be completed into the diagram on the right. C1 C′ 1 C2 R Aτ C1 C′ 1 C2 C′ 2 R Aτ R ∗ Aτ R is a weak bisimulation if it is a weak simulation and the symmetric relation R−1 also is—a reduction from C2 can be matched by CS Speculative optimizations are key to just-in-time optimization of dynamic languages. As these optimizations depend on predicates about the program state, the language implementation must monitor the validity of predicates and be ready to deoptimize the program if a predicate is invalidated. While, many modern compiler rely on this approach, the interplay between optimization and deoptimization often remains opaque. Our contribution is to show that when the predicates and the deoptimization metadata are reified in the program representation, it becomes quite easy to define correct program transformations that are deoptimization aware. In this work we extend the intermediate representation with one new instruction, assume, which plays the double role of checking for the validity of predicates and specifying the actions required to deoptimize the program. Program transformations can inspect both the predicates that are being monitored and the deoptimization metadata and transform them when needed. The formalization presented here is for one particular intermediate language that we hope to be representative of a typical dynamic language. We present a bisimulation proof between multiple versions of the same function, optimized under different assumptions. We formalize deoptimization invariants between versions and show that they enable very simple proofs for standard compiler optimizations, constant folding, unreachable code elimination, and function inlining. We also prove correct three optimizations that are specifically dealing with deoptimizations, namely unrestricted deoptimization, predicate hoisting, and assume composition. There are multiple avenues of future investigation. The optimizations presented here rely on intraprocedural analysis and the granularity of deoptimization is a whole function. If we were to extend this work to interprocedural analysis, it would become much trickier to determine what functions are to be invalidated as a speculation in one function may allow optimizations in many other functions. The current representation forces to check predicates before each use, but some predicates are cheaper to check by monitoring operations that could invalidate them. To do this would require changes to our model as the assume instruction would need to be split between a monitor and a deoptimization point. Lastly, the expressive power of predicates is an interesting question as there is a clear trade-off — richer predicates may allow more optimizations but are likely to be costlier to monitor."
Systems and Networking,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","June 1st, 2019",MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing,http://www.ccs.neu.edu/home/gene/papers/hpdc19.pdf," ""MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing"", Rohan Garg, Gregory Price, and Gene Cooperman, Proc. of 28th Int. Symp. on High Performance Parallel and Distributed Computing, Phoenix, AZ, USA, ACM, pp. 49--60, June, 2019","Transparently checkpointing MPI for fault tolerance and load bal- ancing is a long-standing problem in HPC. The problem has been complicated by the need to provide checkpoint-restart services for all combinations of an MPI implementation over all network interconnects. This work presents MANA (MPI-Agnostic Network- Agnostic transparent checkpointing), a single code base which sup- ports all MPI implementation and interconnect combinations. The agnostic properties imply that one can checkpoint an MPI appli- cation under one MPI implementation and perhaps over TCP, and then restart under a second MPI implementation over InfiniBand on a cluster with a different number of CPU cores per node. This tech- nique is based on a novel split-process approach, which enables two separate programs to co-exist within a single process with a single address space. This work overcomes the limitations of the two most widely adopted transparent checkpointing solutions, BLCR and DMTCP/InfiniBand, which require separate modifications to each MPI implementation and/or underlying network AP2 MANA: DESIGN AND IMPLEMENTATION Multiple aspects of the design of MANA are covered in this sec- tion. Section 2.1 discusses the design for supporting a split-process. Section 2.2 discusses the need to save and restore persistent MPI opaque objects, such as communicators, groups and topologies. Section 2.3 briefly discusses the commonly used algorithm to drain point-to-point MPI messages in transit prior to intiaiting a check- point. Sections 2.4 and 2.5 present a new two-phase algorithm (Algorithm 2), which enables checkpointing in-progress MPI collec- tive communication calls in a fully agnostic environment. Finally, Sections 2.6 and 2.7 present details of the overall implementation of MANA. 2.1 Upper and Lower Half: Checkpointing with an Ephemeral MPI Library In this section, we define the lower half of a split-process as the memory associated with the MPI library and dependencies, includ- ing network libraries. The upper half is the remaining Linux process memory associated with the MPI application’s code, data, stack, and other regions (e.g., environment variables). The terms lower half and upper half are in analogy with the upper and lower half of a device driver in an operating system kernel. This separation into lower and upper half does not involve additional threads or processes. Instead, it serves primarily to tag memory so that only upper half memory will be saved or restored during checkpoint and restart. Section 2.6 describes an additional “helper thread”, but that thread is active only during checkpoint and restart. Libc and other system libraries may appear in both the lower half as a dependency of the MPI libraries, and the upper half as an independent dependency of the MPI application. This split-process approach allows MANA to balance two con- flicting objectives: a shared address space; and isolation of upper and lower halves. The isolation allows MANA to omit the lower half memory (an “ephemeral” MPI library) when it creates a checkpoint image file. The shared address space allows the flow of control to pass efficiently from the upper-half MPI application to the lower- half MPI library through standard C/Fortran calling conventions, including call by reference. As previously noted, Remote Produce Calls (RPC) are not employed. Isolation is needed so that at checkpoint time, the lower half can be omitted from the checkpoint image, and at the time of restart, replaced with a small “bootstrap” MPI program with new MPI li- braries. The bootstrap program calls MPI_Init() and each MPI process discovers its MPI rank via a call to MPI_Rank(). The mem- ory present at this time becomes the lower half. The MPI process then restores the upper-half memory from a checkpoint image file corresponding to the MPI rank id. Control is then transferred back to the upper-half MPI application, and the stack in the lower half is never used again. Shared address space is needed for efficiency. A dual-process proxy approach was explored in [16, Section IV.B] and in [35, Sec- tion IV.A]. The former work reported a 6% runtime overhead for real-world CUDA applications, and the latter work reported run- time overheads in excess of 20% for some OpenCL examples from the NVIDIA SDK 3.0. In contrast, Section 3 reports runtime over- heads less than 2% for MANA under older Linux kernels, and less than 1% runtime overhead for recent Linux kernels. Discarding the lower half greatly simplifies the task of check- pointing. By discarding the lower half, the MPI application in the upper half appears as an isolated process with no inter-process communication. Therefore, a single-process checkpointing package can create a checkpoint image. A minor inconvenience of this split-process approach is that calls to sbrk() will cause the kernel to extend the process heap in the data segment. Calls to sbrk() can be caused by invocations of malloc(). Since the kernel has no concept of a split-process, the kernel may choose, for example, to extend the lower half data segment after restart since that corresponds to the original program seen by the kernel before the upper-half memory is restored. MANA resolves this by interposing on calls to sbrk() in the upper-half libc, and then inserts calls to mmap() to extend the heap of the upper-half. Finally, MANA employs coordinated checkpointing, and a check- point coordinator sends messages to each MPI rank at the time of checkpoint (see Sections 2.3, 2.4 and 2.5). MPI opaque objects (communicators, groups, topologies) are detected on creation and restored on restart (see Section 2.2). This is part of a broader strat- egy by which MPI calls with persistent effects (such as creation of these opaque objects) are recorded during runtime and replayed on restart. 2.2 Checkpointing MPI Communicators, Groups, and Topologies An MPI application can create communication subgroups and topolo- gies to group processes for ease of programmability and efficient communication. MPI implementations provide opaque handles to the application as a reference to a communicator object or group. MANA interposes on all calls that refer to these opaque identi- fiers, and virtualizes the identifiers. At runtime, MANA records any MPI calls that can modify the MPI communication state, such as MPI_Comm_create, MPI_Group_incl, etc. On restart, MANA recre- ates the MPI communicator state by replaying the MPI calls using a new MPI library. The runtime virtualization of identifiers allows the application to continue running with consistent handles across checkpoint-restart. A similar checkpointing strategy also works for other opaque identifiers, such as, MPI derived datatypes, etc. 2.3 Checkpointing MPI Point-to-Point Communication Capturing the state of MPI processes requires quiescing the process threads, and preserving the process memory to a file on the disk. However, this alone is not sufficient to capture a consistent state of the computation. Any MPI messages sent but not yet received at the time of quiescing processes must also be saved as part of the checkpoint image. MANA employs a variation of an all-to-all bookmark exchange algorithm to reach this consistent state. LAM/MPI [31] demon- strated the efficacy of a such a Chandy/Lamport [11] algorithm for checkpointing MPI applications. Hursey et al. [22] lifted this mechanism out of interconnect drivers and into the MPI library. MANA further lifts this mechanism outside the MPI library, and into a virtualized MPI API. An early prototype of MANA demonstrated a naïve application of this bookmark exchange algorithm was sufficient for handling pre-checkpoint draining for point-to-point communication; how- ever, collective-communication calls may have MPI implementation effects that can determine when it is “safe” to begin a checkpoint. For this reason, a naïve application to the entire API was insufficient to ensure correctness. This is discussed in Section 2.4. 2.4 Checkpointing MPI Collectives: Overview The MPI collective communications primitive involves communi- cation amongst all or a program-defined subset of MPI ranks (as specified by the MPI communicator argument to the function). The internal behavior of collectives are specific to each MPI implemen- tation, and so it is not possible to make guarantees about their behavior, such as when and how messages are exchanged when ranks are waiting for one or more ranks to enter the collective. In prior work [22, 31], internal knowledge of the MPI library state was required to ensure that checkpointing would occur at a “safe” state. In particular, Hursey et al. [22] required interconnect drivers be classified as “checkpoint-friendly” or “checkpoint-unfriendly”, changing behavior based on this classification. As MANA lives outside the MPI library, a naive application of the Hursey et al. algorithm can have effects that cross the upper and lower half boundaries of an MPI rank (for example, when shared memory is being used for MPI communication). This problem occurs because of the truly network-agnostic trait of MANA. As MANA has no concept of transport level constructs, it cannot determine what “safe” means in context of collectives. To correct this, MANA’s support for collective communication requires it to maintain the following invariant: No checkpoint must take place while a rank is inside a collective communication call. There exists one exception to this rule: a trivial barrier. A trivial barrier is a simple call to MPI_Barrier(). This call produces no side effects on an MPI rank, and so it can be safely interrupted during checkpoint, and then re-issued when restarting the MPI application. This is possible due to the split-process architecture of MANA, as trivial barrier calls occur exclusively in the lower half, which is discarded and replaced across checkpoint and restart. MANA leverages this exception to build a solution for all other collective calls. As we discuss MANA’s algorithm for checkpointing collective calls, we take into consideration three subtle, but important, con- cerns. Challenge I (consistency): In the case of a single MPI collec- tive communication call, there is a danger that rank A will see a request to checkpoint before entering the collective call, while rank B will see the same request after entering the collective call, in violation of MANA’s invariant. Both ranks might report that they are ready to checkpoint, and the resulting inconsistent snapshot will create problems during restart. This situation could arise, for example, if the mes- sage from the checkpoint coordinator to rank B is excessively delayed in the network. To resolve this, MANA introduces a two-pass protocol in which the coordinator makes a re- quest (sends an intend-to-checkpoint message), each MPI rank acknowledges with its current state, and finally the coordinator posts a checkpoint request (possibly preceded by extra iterations). Challenge II (progress and latency): Given the aforementioned solution for consistency, long delays may occur before a checkpoint request can be serviced. It may be that rank A has entered the barrier, and rank B will require several hours to finish a task before entering the barrier. Hence, the two-pass protocol may create unacceptable delays before a checkpoint can be taken. Algorithm 2 addresses this by introducing a trivial barrier prior to the collective communication call. We refer to this as a two-phase algorithm since each collective call is now replaced by a wrapper function that invokes a trivial barrier call (phase 1) followed by the original collective call (phase 2). Challenge III (multiple collective calls): Until now, it was assumed that at most one MPI collective communication call was in progress at the time of checkpoint. It may happen that there are multiple ongoing collective calls. During the time that some MPI ranks exit from a collective call, it may happen that MPI ranks associated with an independent col- lective call have left the MPI trivial barrier (phase 1) and have now entered the real collective call (phase 2). As a result, servicing a checkpoint may be excessive delayed. To solve this, we introduce an intend-to-checkpoint message, such that no ranks will be allowed to enter phase 2, and extra itera- tions will be inserted into the request-acknowledge protocol between coordinator and MPI rank. 2.5 Checkpointing MPI Collectives: Detailed Algorithm Here we present a single algorithm (Algorithm 2) for checkpointing MPI collectives which contains the elements described in Section 2.4: a multi-iteration protocol; and a two-phase algorithm incorporating a trivial barrier before any collective communication call. From the viewpoint of an MPI application, any call to an MPI collective communication function is interposed on by a wrapper function, as shown in Algorithm 1. Algorithm 1 Two-Phase collective communication wrapper. (This wrapper function interposes on all MPI collective com- munication functions invoked by an MPI application) 1: function Collective Communication Wrapper 2: # Begin Phase 1 3: Call MPI_Barrier() # trivial barrier 4: # Begin Phase 2 5: Call original MPI collective communication function 6: end function Recall that a trivial barrier is an extra call to MPI_Barrier() prior to a collective call. A collective MPI call can intuitively be divided into two parts: the participating MPI ranks “register” themselves as ready for the collective communication; and then the “work” of communication is carried out. Where the time for the collective communication calls of an MPI program is significant, it is typically due to significant “work” in the second part of the calls. Adding a trivial barrier requires the MPI ranks to register themselves once for the trvial barrier (but no work is involved), and then register themselves again for the actual MPI collective communication. The overhead due to registering twice is tiny in practice. Evidence for this can be seen in the experiments in Section 3.2.3, which show small overhead. The purpose of Algorithm 1 is to enforce the following extension of the invariant presented in Section 2.4: No checkpoint must take place while a rank is inside the collective communication call (Phase 2) of a wrapper function for collective communication (Algorithm 1). We formalize this with the following theorem, which guarantees Algorithm 2 satisfies this invariant. Theorem 1. Under Algorithm 2, an MPI rank is never inside a collective communication call when a checkpoint message is received from the checkpoint coordinator. The proof of this theorem is deferred until the end of this sub- section. We begin the path to this proof by stating an axiom that serves to define the concept of a barrier. Axiom 1. For a given invocation of an MPI barrier, it never happens that a rank A exits from the barrier before another rank B enters the barrier under the “happens-before” relation. Next, we present the following two lemmas. Checkpoint Coordinator Rank A Rank B Barrier (1) (3) (2) (4) Figure 1: Fundamental “happens-before” relation in commu- nication between the checkpoint coordinator and the MPI ranks involved in an MPI barrier. Lemma 1. For a given MPI barrier, if the checkpoint coordinator sends a message to each MPI rank participating in the barrier, and if at least one of the reply messages from the participating ranks reports that its rank has exited the barrier, then the MPI coordinator can send a second message to each participating rank, and each MPI rank will reply that it has entered the barrier (and perhaps also exited the barrier). Proof. We prove the lemma by contradiction. Suppose that the lemma does not hold. Figure 1 shows the general case in which this happens. At event 4, the checkpoint coordinator will conclude that event 1 (rank A has exited the MPI barrier) happened before event 2 (the first reply by each rank), which happened before event 3 (in which rank B has not yet entered the barrier). But this contradicts Axiom 1. Therefore, our assumption is false, and the lemma does indeed hold. □ Lemma 2. Recall that an MPI collective communication wrapper makes a call to a trivial barrier and then makes an MPI collective communication call. For a given invocation of an MPI collective com- munication wrapper, we know that one of four cases must hold: (a) an MPI rank is in the collective communication call, and all other ranks are either in the call, or have exited; (b) an MPI rank is in the collective communication call, and no rank has exited, and every other rank has at least entered the trivial barrier (and possibly proceeded further); (c) an MPI rank is in the trivial barrier and no other rank has exited (but some may not yet have entered the trivial barrier); (d) either no MPI rank has entered the trivial barrier, or all MPI ranks have exited the MPI collective communication call. Proof. The proof is by repeated application of Lemma 1. For case a, if an MPI rank is in the collective communication call and another rank has exited the collective call, then Lemma 1 says that there cannot be any rank that has not yet entered the collective call. For case b, note that if an MPI rank is in the collective communi- cation call, then that rank has exited the trivial barrier. Therefore, by Lemma 1, all other ranks have at least entered the trivial barrier. Further, we can assume that no ranks that have exited the collec- tive call, since we would otherwise be in case a, which is already accounted for. For case c, note that if an MPI rank is in the trivial barrier and no rank has exited the trivial barrier, then Lemma 1 says that there cannot be any rank that has not yet entered the trivial barrier. Finally, if we are not in case a, b, or c, then the only remaining possibility is case d: all ranks have not yet entered the trivial barrier or all ranks have exited the collective call. □ Algorithm 2 Two-Phase algorithm for deadlock-free check- pointing of MPI collectives 1: Messages: {intend-to-checkpoint, extra-iteration, do-ckpt} 2: MPI states: {ready, in-phase-1, exit-phase-2} 3: Process Checkpoint Coordinator do 4: function Begin Checkpoint 5: send intend-to-ckpt msg to all ranks 6: receive responses from each rank 7: while some rank in state exit-phase-2 do 8: send extra-iteration msg to all ranks 9: receive responses from each rank 10: end while 11: send do-ckpt msg to all ranks 12: end function 13: Process MPI Rank do 14: upon event intend-to-ckpt msg or extra-iteration msg do 15: if not inCollectiveWrapper then 16: reply to ckpt coord: state ←ready 17: end if 18: if inCollectiveWrapper and in Phase 1 then 19: reply to ckpt coord: state ←in-phase-1 20: end if 21: if inCollectiveWrapper and in Phase 2 then 22: # guaranteed ckpt coord won’t request ckpt here 23: finish executing coll. comm. call 24: reply to ckpt coord: state ←exit-phase-2 25: # ckpt coord can request ckpt after this 26: set state ←ready 27: end if 28: continue, but wait before next coll. comm. call 29: upon event do-ckpt msg do 30: # guaranteed now that no rank is in phase 2 during ckpt 31: do local checkpoint for this rank 32: # all ranks may now continue executing 33: if this rank is waiting before coll. comm. call then 34: unblock this rank and continue executing 35: end if We now continue with the proof of the main theorem (Theo- rem 1), which was deferred earlier. Proof. (Proof of Theorem 1 for Algorithm 2.) Lemma 2 states that one of four cases must hold in a call by MANA to an MPI collective communication wrapper. We wish to exclude the possibility that an MPI rank is in the collective communication call (case a or b of the lemma) when the checkpoint coordinator invokes a checkpoint. In Algorithm 2, assume that the checkpoint coordinator has sent an intend-to-ckpt message, and has not yet sent a do-ckpt message. An MPI rank will either reply with state ready or in-phase-1 (show- ing that it is not in the collective communication call and that it will stop before entering the collective communication call), or else it must be in Phase 2 of the wrapper (potentially within the collective communication call), and it will not reply to the coordinator until exiting the collective call. □ Theorem 2. Under Algorithm 2, deadlock will never occur. Further, the delay between the time when all ranks have received the intend- to-checkpoint message and the time when the do-ckpt message has been sent is bounded by the maximum time for any individual MPI rank to enter and exit the collective communication call, plus network message latency. Proof. The algorithm will never deadlock, since each rank must either make progress based on the normal MPI operation or else it stops before the collective communication call. If any rank replies with the state exit-phase-2, then the checkpoint coordinator will send an additional extra-iteration message. So, at the time of check- point, all ranks will have state ready or in-phase-1. Next, the delay between the time when all ranks have received the intend-to-checkpoint message and the time when the do-ckpt message has been sent is clearly bounded by the maximum time for an individual MPI rank to enter and exit the collective commu- nication call, plus the usual network message latency. This is the case since once the intend-to-checkpoint message is received, no MPI rank may enter the collective communication call. So, upon re- ceiving the intend-to-checkpoint message, either the rank is already in Phase 2 or else it will remain in Phase 1. □ Implementation of Algorithm 2: At the time of process launch for an MPI rank, a separate checkpoint helper thread is also in- jected into each rank. This thread is responsible for listening to checkpoint-related messages from a separate coordinator process and then responding. This allows the MPI rank to asynchronously process events based on messages received from the checkpoint coordinator. Furthermore at the time of checkpoint, the existing threads of the MPI rank process are quiesced (paused) by the helper thread, and the helper thread carries out the checkpointing require- ments, such as copying the upper-half memory regions to stable storage. The coordinator process does not participate in the check- pointing directly. In the implementation, a DMTCP coordinator and DMTCP checkpoint thread [1] are modified to serve as checkpoint coordinator and helper thread, respectively. 2.6 Verification with TLA+/PlusCal To gain further confidence in our implementation for handling collective communication (Section 2.5), we developed a model for the protocol in TLA+ [25] and then used the PlusCal model checker of TLA+ based on TLC [38] to verify Algorithm 2. Specifically, PlusCal was used to verify the algorithm invariants of deadlock- free execution and consistent state when multiple concurrent MPI processes are executing. The PlusCal model checker did not report any deadlocks or broken invariants for our implementation. 2.7 Checkpoint/Restart Package Any single-process checkpointing package could be utilized for the basis of implementing MANA. This work presents a prototype implemented by extending DMTCP [1] and by developing a DMTCP plugin [2]. Cao et al. [9] demonstrated that DMTCP can checkpoint MPI-based HPCG over 32,752 CPU cores (38 TB) in 11 minutes, and MPI-based NAMD over 16,368 cores (10 TB) in 2.6 minutes. DMTCP uses a helper thread inside each application process, and a coordinated checkpointing protocol by using a centralized coor- dinator daemon. Since this was close to the design requirements of MANA, we leveraged this infrastructure and extended the DMTCP coordinator to implement the two-phase algorithm. The same approach could be extended to base MANA on top of a different underlying transparent checkpointing package. For example, one could equally well have modified an existing MPI co- ordinator process to communicate with a custom helper thread in each MPI rank that then invokes the BLCR checkpointing package when it is required to execute the checkpoint. In particular, all sock- ets and other network communication objects are inside the lower half, and so even a single-process or single-host checkpointing package such as BLCR would suffice for this work. 3 EXPERIMENTAL EVALUATION This section seeks to answer the following questions: Q1: What is the runtime overhead of running MPI applications under MANA? Q2: What are the checkpoint and restart overheads of transparent checkpointing of MPI applications under MANA? Q3: Can MANA allow transparent switching of MPI implementa- tions across checkpoint-restart for the purpose of load balancing? 3.1 Setup We first describe the hardware and software setup for MANA’s evaluation. 3.1.1 Hardware. The experiments were run on the Cori supercom- puter [13] at the National Energy Research Scientific Computing Center (NERSC). As of this writing, Cori is the #12 supercomputer in the Top-500 list [36]. All experiments used the Intel Haswell nodes (dual socket with a 16-core Xeon E5-2698 v3 each) connected via Cray’s Aries interconnect network. Checkpoints were saved to the backend Lustre filesystem. 3.1.2 Software. Cori provides modules for two implementations of MPI: Intel MPI and Cray MPICH. The Cray compiler (based on an Intel compiler) and Cray MPICH are the recommended way to use MPI, presumably for reasons of performance. Cray MPICH version 3.0 was used for the experiments. 3.1.3 Application Benchmarks. MANA was tested with five real- world HPC applications from different computational science do- mains: (1) GROMACS [4]: Versatile package for molecular dynamics, often used for biochemical molecules. (2) CLAMR [12, 29]: Mini-application for CelL-based Adaptive Mesh Refinement. 90 95 100 Normalized Performance (%) GROMACS miniFE HPCG CLAMR LULESH 1 2 4 8 16 32 0 1 2 4 8 16 32 1 2 4 8 16 32 # MPI Rank(s) (Single node) 1 2 4 8 16 32 1 9 27 Figure 2: Single Node: Runtime overhead under MANA for different real-world HPC benchmarks with an unpatched Linux kernel. (Higher is better.) 90 95 100 Normalized Performance (%) GROMACS miniFE HPCG CLAMR LULESH 2 4 8 16 32 64 0 2 4 8 16 32 64 2 4 8 16 32 64 # Compute Nodes (32 ranks/node, except LULESH) 2 4 8 16 32 64 2 4 8 16 32 64 Figure 3: Multiple Nodes: Runtime overhead under MANA for different real-world HPC benchmarks with an un- patched Linux kernel. In all cases, except LULESH, 32 MPI ranks were executed on each compute node. (Higher is bet- ter.) (3) miniFE [20]: Proxy application for unstructured implicit fi- nite element codes. (4) LULESH [24]: Unstructured Lagrangian Explicit Shock Hy- drodynamics (5) HPCG [14] (High Performance Conjugate Gradient): Uses a variety of linear algebra operations to match a broad set of important HPC applications, and used for ranking HPC systems. 3.2 Runtime Overhead 3.2.1 Real-world HPC Applications. Next, we evaluate the perfor- mance of MANA for real-world HPC applications. It will be shown that the runtime overhead is close to 0 % for miniFE and HPCG, and as much as 2 % for the other three applications. The higher overhead has been tracked down to an inefficiency in the Linux ker- nel [27] in the case of many point-to-point MPI calls (send/receive) with messages of small size. This worst case is analyzed further in Section 3.3, where tests with an optimized Linux kernel show a worst case runtime overhead of 0.6 %. The optimized Linux kernel is based on a patch under review for a future Linux version. Single Node: Since the tests were performed within a larger clus- ter where the network use of other jobs could create congestion, we first eliminate any network-related overhead by running the benchmarks on a single node with multiple MPI ranks, both under 0 1000000 2000000 3000000 4000000 Size (Bytes) 0 5000 10000 15000 20000 Bandwidth (MB/s) Without MANA With MANA (native kernel) With MANA (patched kernel) Figure 4: Point-to-Point Bandwidth under MANA with patched and unpatched Linux kernel. (Higher is better.) MANA and natively (without MANA). This experiment isolates the single-node runtime overhead of MANA by ensuring that all communication among ranks is intra-node. Figure 2 shows the results for the five different real-world HPC applications running on a single node under MANA. Each run was repeated 5 times (including the native runs), and the figure shows the mean of the 5 runs. The absolute runtimes varied from 4.5 min to 15 min, depending on the configuration. The worst case overhead incurred by MANA is 2.1 % in the case of GROMACS (with 16 MPI ranks). In most cases, the mean overhead is less than 2 %. Multiple Nodes: Next, the scaling of MANA across the network is examined for up to 64 compute nodes and with 32 ranks per node (except for LULESH, whose configuration restricts the number of ranks/node based on the number of nodes). Hence, the number of MPI ranks ranges from 64 to 2048. Figure 3 shows the results of five different real-world HPC ap- plications running on multiple nodes under MANA. Each run was repeated 5 times, and the mean of 5 runs is reported. We observe a trend similar to the single node case. MANA imposes an overhead of typically less than 2 %. The highest overhead observed is 4.5 % in the case of GROMACS (512 ranks running over 16 nodes). However, see Section 3.3 where we demonstrate a reduced overhead of 0.6 % with GROMACS. 3.2.2 Memory Overhead. The upper-half libraries were built with mpicc, and hence include additional copies of the MPI library that are not used. However, the upper-half MPI library is never ini- tialized, and so no network library is ever loaded into the upper half. Since a significant portion of the lower half is comprised only of the MPI library and its dependencies, the additional copy of the libraries (with one copy residing in the upper half) imposes a constant memory overhead. This text segment (code region) was 26 MB in all of our experiments on Cori with the Cray MPI library. In addition to the code, the libraries (for example, the networking driver library) in the lower half also allocate additional memory regions (shared memory regions, pinned memory regions, memory- mapped driver regions). We observed that the shared memory re- gions mapped by the network driver library grow in proportion with the number of nodes (up to 64 nodes): from 2 MB (for 2 nodes) to 40 MB for (64 nodes). We expect MANA to have a reduced check- point time compared to DMTCP/InfiniBand [10], as MANA discards these regions during checkpointing, reducing the amount of data that’s written out to the disk. 0 1000000 2000000 3000000 4000000 Size (Bytes) 0 50 100 150 200 250 300 Latency (µs) Without MANA With MANA (a) Point-to-Point Latency 0 200000 400000 600000 800000 1000000 Size (Bytes) 0 50 100 150 200 250 300 Latency (µs) Without MANA With MANA (b) Collective MPI_Gather 0 200000 400000 600000 800000 1000000 Size (Bytes) 0 100 200 300 400 500 Latency (µs) Without MANA With MANA (c) Collective MPI_Allreduce Figure 5: OSU Micro-benchmarks under MANA. (Results are for two MPI ranks on a single node.) 3.2.3 Microbenchmarks. To dig deeper into the sources for the run- time overhead, we tested MANA with the OSU micro-benchmarks. The benchmarks stress and evaluate the bandwidth and latency of different specific MPI subsystems. Our choice of the specific micro-benchmarks was motivated by the MPI calls commonly used by our real-world MPI applications. Figure 5 shows the results with three benchmarks from the OSU micro-benchmark suite. These benchmarks correspond with the most frequently used MPI subsystems in the set of real-world HPC applications. The benchmarks were run with 2 MPI ranks running on a single compute node. The results show that latency does not suffer under MANA, for both point-to-point and collective communication. (The latency curves for application running under MANA closely follow the curves when the application is run natively.) 3.3 Source of Overhead and Improved Overhead for Patched Linux Kernel All experiments in this section were performed on a single node of our local cluster, where it was possible to directly install a patched Linux kernel in the bare machine. Further investigation revealed two sources of runtime overhead. The larger source of overhead is due to the use of the “FS” register during transfer of flow of control between the upper and lower half and back during a call to the MPI library in the lower half. The “FS” register of the x86-64 CPU is used by most compilers to refer to the thread-local variables declared in the source code. The upper and lower half programs each have their own thread-local storage region. Hence, when switching between the upper and lower half programs, the value of the “FS” register must be changed to point to the correct thread-local region. Most Linux kernels today require a kernel call to invoke a privileged assembly instruction to get or set the “FS” register. In 2011, Intel Ivy Bridge CPUs introduced a new, unprivileged FSGSBASE assembly instruction for modifying the “FS” register, and a patch to the Linux kernel [27] is under review to allow other Linux programs to use this more efficient mechanism for managing the “FS” register. (Other architectures, such as ARM, use unprivileged addressing modes for thread-local variables that do not depend on special constructs, such as the x86 segments.) A second (albeit smaller) source of overhead is the virtualization of MPI communicators and datatypes, and recording of metadata for MPI sends and receives. Virtualization requires a hash table lookup and locks for thread safety. The first and larger source of overhead is then eliminated by using the patched Linux kernel, as discussed above. Point-to-point bandwidth benchmarks were run both with and without the patched Linux kernel (Figure 4). A degradation in runtime performance is seen for MANA for small message sizes (less than 1 MB) in the case of a native kernel. However, the figure shows that the patched kernel yields much reduced runtime overhead for MANA. Note that the Linux kernel community is actively reviewing this patch (currently in its third version), and it is likely to be incorporated in future Linux releases. Finally, we return to GROMACS, since it exhibited a higher runtime overhead (e.g., 2.1 % in the case of 16 ranks) in many cases. We did a similar experiment, running GROMACS with 16 MPI ranks on a single node with the patched kernel. With the patched kernel, the performance degradation was reduced to 0.6 %. 3.4 Checkpoint-restart Overhead In this section, we evaluate MANA’s performance when checkpoint- ing and restarting HPC applications. Figure 6 shows the checkpoint- ing overhead for five different real-world HPC applications running on multiple nodes under MANA. Each run was repeated 5 times, and the mean of five runs is reported. For each run, we use the fsync system call to ensure the data is flushed to the Lustre backend storage. The total checkpointing data written at each checkpoint varies from 5.9 GB (in the case of 64 ranks of GROMACS running over 2 nodes) to 4 TB (in the case of 2048 ranks of HPCG running over 64 nodes). Note that the checkpointing overhead is proportional to the total amount of memory used by the benchmark. This is also reflected in the size of the checkpoint image per MPI rank. While Figure 6 reports the overall checkpoint time, note that there is significant variation in the write times for each MPI rank during a given run. (The time for one rank to write its checkpoint data can be up to 4 times more than that for 90 % of the other ranks.) This phenomenon of stragglers during a parallel write has also been noted by other researchers [2, 37]. Thus, the overall checkpoint time is bottlenecked by the checkpoint time of the slowest rank. Next, we ask what are the sources of the checkpointing overhead? Does the draining of MPI messages and the two-phase algorithm impose a significant overhead at checkpoint time? 2 4 8 16 32 64 1 5 10 15 20 25 30 35 40 Checkpoint Time (s) (93 MB) (93 MB) (92 MB) (92 MB) (94 MB) (92 MB) GROMACS 2 4 8 16 32 64 (2.0 GB) (1.3 GB) (806 MB) (1.3 GB) (902 MB) (1.3 GB) miniFE 2 4 8 16 32 64 # Compute Nodes (32 ranks/node, except LULESH) (2.0 GB) (2.0 GB) (2.0 GB) (2.0 GB) (2.0 GB) (2.0 GB) HPCG 2 4 8 16 32 64 (656 MB) (594 MB) (552 MB) (501 MB) (594 MB) (552 MB) CLAMR 2 4 8 16 32 64 (276 MB) (164 MB) (114 MB) (91 MB) (85 MB) (88 MB) LULESH Figure 6: Checkpointing overhead and checkpoint image sizes under MANA for different real-world HPC bench- marks running on multiple nodes. In all cases, except LULESH, 32 MPI ranks were executed on each compute node. For LULESH, the total number of ranks was either 64 (for 2, 4, and 8 nodes), or 512 (for 16, 32, and 64 nodes). Hence, the maximum number of ranks (for 64 nodes) was 2048. The numbers above the bars (in parentheses) indicate the check- point image size for each MPI rank. 2 4 8 16 32 64 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Restart Time (s) GROMACS 2 4 8 16 32 64 miniFE 2 4 8 16 32 64 # Compute Nodes (32 ranks/node, except LULESH) HPCG 2 4 8 16 32 64 CLAMR 2 4 8 16 32 64 LULESH Figure 7: Restart overhead under MANA for different real- world HPC benchmarks running on multiple nodes. In all cases, except LULESH, 32 MPI ranks were executed on each compute node. Ranks/node is as in Figure 6. Figure 8 shows the contribution of different components to the checkpointing overhead for the case of 64 nodes for the five different benchmarks. In all cases, the communication overhead for handling MPI collectives in the two-phase algorithm of Section 2.5 is found to be less than 1.6 s. In all cases, the time to drain in-flight MPI messages was less than 0.7 s. The total checkpoint time was dominated by the time to write to the storage system. The next big source of checkpointing GROMACS miniFE HPCG CLAMR LULESH Benchmark (64 nodes; 32 ranks/node, except LULESH) 0 20 40 60 80 100 Contribution to Checkpoint Time (%s) Write Time Drain Time Comm. overhead Figure 8: Contribution of different factors to the checkpoint- ing overhead under MANA for different real-world HPC benchmarks running on 64 nodes. Ranks/node is as in Fig- ure 6. The “drain time” is the delay in starting a checkpoint while MPI message in transit are completed. The communi- cation overhead is the time required in the protocol for net- work communication between the checkpoint coordinator and each rank. overhead was the communication overhead. The current imple- mentation of the checkpointing protocol in DMTCP uses TCP/IP sockets for communication between the MPI ranks and the central- ized DMTCP coordinator. The communication overhead associated with the TCP layer is found to increase with the number of ranks, especially due to metadata in the case of small messages that are exchanged between MPI ranks and the coordinator. Finally, Figure 7 shows the restart overhead under MANA for the different MPI benchmarks. The restart time varies from less than 10 s to 68 s (for 2048 ranks of HPCG running over 64 nodes). The restart times increase in proportion to the total amount of checkpointing data that is read from the storage. In all the cases, the restart overhead is dominated by the time to read the data from the disk. The time to recreate the MPI opaque identifiers (see Section 2.2) is less than 10 % of the total restart time. 3.5 Transparent Switching of MPI libraries across Checkpoint-restart This section demonstrates that MANA can transparently switch between different MPI implementations across checkpoint-restart. This is useful for debugging programs (even the MPI library) as it allows a program to switch from a production version of an MPI library to a debug version of the MPI library. The GROMACS application is launched using the production version of CRAY MPI, and a checkpoint is taken 55 s into the run. The computation is then restarted on top of a custom-compiled debug version of MPICH (for MPICH version 3.3). MPICH was chosen because it is a reference implementation whose simplicity makes it easy to instrument for debugging. 3.6 Transparent Migration across Clusters Next, we consider cross-cluster migration for purposes of wide- area load balancing either among clusters at a single HPC site or even among multiple HPC sites. This is rarely done, since the two common vehicles for transparent checkpoint (BLCR as the base of 190 200 210 220 Runtime (s) Native Restarted (migrated from Cori) Open MPI/IB (2x4) MPICH/TCP (2x4) MPICH (8x1) Restart Configuration 0 Figure 9: Performance degradation of GROMACS after cross- cluster migration under three different restart configura- tions. The application was restarted after being check- pointed at the half-way mark on Cori. (Lower is better.) an MPI-specific checkpoint-restart service; or DMTCP/InfiniBand) both save the MPI library within the checkpoint image and continue to use that same MPI library on the remote cluster after migration. At each site and for each cluster, administrators typically configure and tune a locally recommended MPI implementation for perfor- mance. Migrating an MPI application along with its underlying MPI library destroys the benefit of this local performance tuning. This experiment showcases the benefits of MPI-agnostic, network- agnostic support for transparent checkpointing. GROMACS is run under MANA, initially running on Cori with a statically linked Cray MPI library running over the Cray Aries network. GROMACS on Cori is configured to run with 8 ranks over 4 nodes (2 ranks per node). Each GROMACS rank is single-threaded. A checkpoint was then taken exactly half way into the run. The checkpoints were then migrated to a local cluster that uses Open MPI over the InfiniBand network. The restarted GROMACS under MANA was compared with three other configurations: GROMACS using the local Open MPI, con- figured to use the local InfiniBand network (8 ranks over 2 nodes); GROMACS/MPICH, configured to use TCP (8 ranks over 2 nodes); and GROMACS/MPICH, running on a single node (8 ranks over 1 node). The network-agnostic nature of MANA allowed the Cori version of GROMACS to be restarted on the local cluster with any of three network options. We wished to isolate the effects due to MANA from the effects due to different compilers on Cori and the local cluster. In order to accomplish this, the native GROMACS on the local cluster was com- piled specially. The Cray compiler of Cori (using Intel’s C compiler) was used to generate object files (.o files) on Cori. Those object files were copied to the local cluster. The native GROMACS was then built using the local mpicc, but with the (.o files) as input instead of the (.c files). The local mpicc linked these files with the local MPI implementation, and the native application was then launched in the traditional way. Figure 9 shows that GROMACS’s performance degrades by less than 1.8% post restart on the local cluster for the three different restart configurations (compared to the corresponding native runs). Also, note that the performance of GROMACS under MANA post restart closely tracks the performance of the native configuration. 4 DISCUSSION AND FUTURE WORK Next, we discuss both the limitations and some future implications of this work concerning dynamic load balancing. 4.1 Limitations While the split-process approach for checkpointing and process migration is quite flexible, it does include some limitations inherited by any approach based on transparent checkpointing. Naturally, when restarting on a different architecture, the CPU instruction set must be compatible. In particular, on the x86 architecture, the MPI application code must be compiled to the oldest x86 sub-architecture among those remote clusters where one might consider restarting a checkpoint image. (However, the MPI libraries themselves may be fully optimized for the local architecture, since restarting on a remote cluster implies using a new lower half.) Similarly, while MPI implies a standard API, any local extensions to MPI must be avoided. The application binary interface (ABI) used by the compiled MPI application must either be compatible or else a “shim” layer of code must be inserted in the wrapper functions for calling from the upper half to the lower half. And of course, the use of a checkpoint coordinator implies coor- dinated checkpointing. If a single MPI rank crashes, MANA must restore the entire MPI computation from an earlier checkpoint. 4.2 Future Work MPI version 3 has added nonblocking collective communication calls (e.g., MPI_Igather). In future work, we propose to extend the two-phase algorithm for collective communication of Section 2.5 to the nonblocking case. The approach to be explored would be to employ a first phase that uses a nonblocking trivial barrier (MPI_Ibarrier), and to then convert the actual asynchronous col- lective call to a synchronous collective call (e.g., MPI_Gather to MPI_Igather) for the second phase. Nonblocking variations of col- lective communication calls are typically used as performance op- timizations in an MPI application. If an MPI rank reaches the col- lective communication early, then instead of blocking, it can con- tinue with an alternate compute task while occasionally testing (via MPI_Test/MPI_Wait) to see if the other ranks have all reached the barrier. In the two-phase analog, a wrapper around the nonblocking collective communication causes MPI_Ibarrier to be invoked. When the ranks have all reached the nonblocking trivial barrier and the MPI_Test/MPI_Wait calls of the original MPI application reports completion of the MPI_Ibarrier call of phase 1, then this implies that the ranks are all ready to enter the actual collective call of phase 2. A wrapper around MPI_Test/MPI_Wait can then invoke the actual collective call of phase 2. The split-process approach of MANA opens up some impor- tant new features in managing long-running MPI applications. An immediately obvious feature is the possibility of switching in the middle of a long run to a customized MPI implementation. Hence, one can dynamically substitute a customized MPI for performance analysis (e.g., using PMPI for profiling or tracing); or using a spe- cially compiled “debug” version of MPI to analyze a particular but occurring in the MPI library in the middle of a long run. This work also helps support many tools and proposals for opti- mizing MPI applications. For example, a trace analyzer is sometimes used to discover communication hotspots and opportunities for bet- ter load balancing. Such results are then fed back by re-configuring the binding of MPI ranks to specific hosts in order to better fit the underlying interconnect topology. MANA can enable new approaches to dynamically load balance across clusters and also to re-bind MPI ranks in the middle of a long run to create new configurations of rank-to-host bindings (new topology mappings). Currently, such bindings are chosen statically and used for the entire lifetime of the MPI application run. This added flexibility allows system managers to burst current long- running applications into the Cloud during periods of heavy usage or when the the MPI application enters a new phase for which a different rank-to-host binding is optimal. Finally, MANA can enable a new class of very long-running MPI applications — ones which may outlive the lifespan of the original MPI Implementation, cluster, or even the network interconnect. Such temporally complex computations might be discarded as in- feasible today without the ability to migrate MPI implementations or clusters. 5 RELATED WORK Hursey et al. [22] developed a semi-network-agnostic checkpoint service for Open-MPI. It applied an “MPI Message” abstraction to a Chandy/Lamport algorithm [11], greatly reducing the complexity to support checkpoint/restart for many multiple network intercon- nects. However, it also highlighted the weakness of implementing transparent checkpointing within the MPI library, since porting to an additional MPI implementation would likely require as much software development as for the first MPI implementation. Addi- tionally, its dependence on BLCR imposed a large overhead cost, as it lacks support for SysV shared memory. Separate proxy processes for high- and low-level operations have been proposed both by CRUM (for CUDA) and McKernel (for the Linux kernel). CRUM [16] showed that by running a non-reentrant library in a separate process, one can work around the problem of a library “polluting” the address space of the application process — i.e., creating and leaving side-effects in the application process’s address space. This decomposition of a single application process into two processes, however, forces the transfer of data between two processes via RPC, which can cause a large overhead. McKernel [17] runs a “lightweight” kernel along with a full- fledged Linux kernel. The HPC application runs on the lightweight kernel, which implements time-critical system calls. The rest of the functionality is offloaded to a proxy process running on the Linux kernel. The proxy process is mapped in the address space of the main application, similar to MANA’s concept of a lower half, to min- imize the overhead of “call forwarding” (argument marshalling/un- marshalling). In general, a proxy process approach is problematic for MPI, since it can lead to additional jitter as the operating system tries to schedule the extra proxy process alongside the application pro- cess. The jitter harms performance since the MPI computation is constrained to complete no faster than its slowest MPI rank. Process-in-process [21] has in common with MANA that both approaches load multiple programs into a single address space. However, the goal of process-in-process was intra-node communi- cation optimization, and not checkpoint-restart. Process-in-process loads all MPI ranks co-located on the same node as separate threads within a single process, but in different logical “namespaces”, in the sense of the dlmopen namespaces in Linux. It would be diffi- cult to adapt process-in-process for use in checkpoint-restart since that approach implies a single “ld.so” run-time linker library that managed all of the MPI ranks. In particular, difficulties occur when restarting with fresh MPI libraries while “ld.so” retains pointers to destructor functions in the pre-checkpoint MPI libraries. In the special regime of application-specific checkpointing for bulk synchronous MPI applications, Sultana et al. [33] supported checkpointing by separately saving and restoring MPI state (MPI identifiers such as communicators, and so on). This is combined with application-specific code to save the application state. Thus, when a live process fails, it is restored using these two components, without the need restart the overall MPI job. SCR [32], and FTI [3] are other application-specific checkpoint- ing techniques. An application developer declares memory regions they’d like to checkpoint and checkpointing can only be done at specific points in the program determined by the application devel- oper. Combining these techniques with transparent checkpointing is outside the scope of this work, though it is an interesting avenue for further inquiry. In general, application-specific and transparent checkpointing each have their merits. Both application-specific and transparent checkpointing are used in practice. At the high end of HPC, application-specific checkpointing is preferred since the labor for supporting this is small compared to the labor already invested in supporting an extreme HPC application. At the low and medium end of HPC, developers prefer trans- parent checkpointing because the development effort for the soft- ware is more moderate, and the labor overhead of a specialized application-specific checkpointing solution would then be signif- icant. System architectures based on burst buffers (e.g., Cray’s DataWarp [19]) can be used to reduce the checkpointing overhead for both application-specific and transparent checkpointing. 6 CONCLUSION This work presents an MPI-Agnostic, Network-Agnostic transpar- ent checkpointing methodology for MPI (MANA), based on a split- process mechanism. The runtime overhead is typically less than 2%, even in spite of the overhead incurred by the current Linux ker- nel when the “FS” register is modified each time control passes between upper and lower half. Further, Section 3.3 shows that a commit (patch) to fix this by the Linux kernel developers is un- der review and that this commit reduces the runtime overhead of GROMACS from 2.1% to 0.6% using the patched kernel. Similar reductions to about 0.6% runtime overhead are expected in the general case. An additional major novelty is the demonstration of practical, efficient migration between clusters at different sites using differ- ent networks and different configurations of CPU cores per node. This was considered impractical in the past because a checkpoint image from one cluster will not be tuned for optimal performance on the second cluster. Section 3.6 demonstrates that this is now feasible, and that the migration of a GROMACS job with 8 MPI ranks experiences an average runtime overhead of less than 1.8% as compared to the native GROMACS application (without MANA) on the remote cluster. As before, even this overhead of 1.8% is likely to be reduced to about 0.6% in the future, based on the results of Section 3.3 with a patched Linux kernel."
Systems and Networking,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","September 10th, 2018",CRUM: Checkpoint-Restart Support for CUDA’s Unified Memory,http://www.ccs.neu.edu/home/gene/papers/cluster18.pdf," ""CRUM: Checkpoint-Restart Support for CUDA's Unified Memory"", Rohan Garg, Apoorve Mohan, Michael Sullivan, Gene Cooperman, Proc. of IEEE Int. Conf. on Cluster Computing</em> (Cluster'18), pp. 302--313, 2018","—Uniﬁed Virtual Memory (UVM) was recently intro- duced on recent NVIDIA GPUs. Through software and hardware support, UVM provides a coherent shared memory across the en- tire heterogeneous node, migrating data as appropriate. The older CUDA programming style is akin to older large-memory UNIX applications which used to directly load and unload memory segments. Newer CUDA programs have started taking advantage of UVM for the same reasons of superior programmability that UNIX applications long ago switched to assuming the presence of virtual memory. Therefore, checkpointing of UVM will become increasingly important, especially as NVIDIA CUDA continues to gain wider popularity: 87 of the top 500 supercomputers in the latest listings are GPU-accelerated, with a current trend of ten additional GPU-based supercomputers each year. A new scalable checkpointing mechanism, CRUM (Checkpoint-Restart for Uniﬁed Memory), is demonstrated for hybrid CUDA/MPI computations across multiple computer nodes. CRUM supports a fast, forked checkpointing, which mostly overlaps the CUDA computation with storage of the checkpoint image in stable storage. The runtime overhead of using CRUM is 6% on average, and the time for forked checkpointing is seen to be a factor of up to 40 times less than traditional, synchronous checkpointing. Index Terms—CUDA, GPU, UVM, checkpoint-restart, DMTCPII. BACKGROUND AND MOTIVATION A. History and Motivation for Uniﬁed Virtual Memory (UVM) Uniﬁed Virtual Memory (UVM) and its predecessor, Uniﬁed Virtual Addressing (UVA), are major CUDA features that are incompatible with prior CUDA checkpointing approaches. Yet, UVM is an important innovation for future CUDA applica- tions. Through software and hardware support, UVM provides a coherent shared memory across the entire heterogeneous node [14], [15]. The use of UVM-managed memory greatly simpliﬁes data sharing and movement among multiple GPUs. This is especially useful given that the most energy-efﬁcient supercomputers place multiple compute accelerators per node—for instance, TSUBAME3.0 [16], Coral Summit [17], and the NVIDIA SATURNV [18] supercomputer use 4, 6, and 8 GPUs per node, respectively. The features and progression of UVM are brieﬂy described below. UVA Cuda 4 Fermi GPUs UVM-Lite Cuda 6 Kepler GPUs UVM-Full Cuda 8 Pascal GPUs 2011 2013 2016 Fig. 2: The technology advancement of CUDA uniﬁed virtual memory. Historically, in CUDA 4 (2011), Fermi-class GPUs added support for Uniﬁed Virtual Addressing (UVA) with zero-copy memory. UVA allows transparent zero-copy accesses to mem- ory across a heterogeneous node using a partitioned address space. UVA never migrates data, and so non-local memory accesses suffer from less bandwidth and longer latency. To reduce the performance penalty of non-local zero-copy memory accesses, ﬁrst-generation Uniﬁed Virtual Memory (UVM-Lite) was introduced in CUDA 6 (2013) for Kepler- class GPUs [19]. UVM-Lite shares a single memory space across a heterogeneous node, and it transparently migrates all memory pages that are attached to the CUDA streams associ- ated with each kernel. This simpliﬁes deep copies with pointer- based structures and it allows GPUs to transparently migrate UVM-managed memory to the device, nearly achieving the performance of CUDA programs using explicit memory man- agement. Due to hardware restrictions, however, UVM-Lite does not allow concurrent access to the same memory from both CPU and GPU—host-side access is only allowed once all GPU-side accesses to a CUDA stream have completed. Concurrent access to UVM-managed memory from different GPUs is allowed, but data are never migrated between devices and non-local memory is accessed in a zero-copy fashion. Second-generation UVM (UVM-Full) was introduced in CUDA 8 (2016) for Pascal-class GPUs [20]. It eliminates the concurrent-access constraints of the prior UVM generation and adds support for system-wide atomic memory operations, providing an unrestricted coherent shared memory across the heterogeneous node. On-demand data migration is supported by UVM-Full across all CPUs and GPUs in a node, with the placement of any piece of data being determined by a variety of heuristics [15]. Pascal-era UVM also adds support for memory over- subscription, meaning that UVM-managed regions that are larger than the GPU device memory can be accessed without explicit data movement. This is important for applications with large data. In particular, it greatly simpliﬁes the programming of large-memory jobs, and avoids the need to explicitly marshal data to and from the GPU [21]. For instance, GPU- capacity-exceeding deep neural network training has been accomplished in the past through explicit data movement [22], but it can also be performed with less programmer effort by UVM over-subscription [23]. B. GPUs for Exascale: DUEs and GPU Reliability The advantages of using GPUs for high-performance com- puting have been realized and a steep rise in their use in large- scale HPC systems has been observed (see Figure 1). Eighty- seven (87) systems in the Top500 list were reported to be powered by NVIDIA GPUs in November 2017, as compared to one (1) in November 2009 [1]. Thus, it is important that both hardware and the software stack (pertaining to the use of GPUs) should be highly available and reliable to maximize large-scale HPC systems productivity. While this makes GPUs attractive for exascale computing, the high GPU detectable-uncorrectable error rate (as compared to CPUs) remains an issue. Checkpointing plays an important role in mediating this issue. Various studies have been con- ducted for understanding the reliability aspects of using GPU’s in large-scale HPC systems. The studies suggest that the newer generation GPU’s are more reliable, as are the large-scale HPC systems using them (i.e., the observed MTBF of systems using newer GPU’s is much longer than their estimated MTBF) [2]– [7]. However, one factor that motivates efﬁcient checkpoint- restart on GPU accelerated systems is that GPU memory currently tends to have more DUEs (Detected Unrecoverable Errors) per GB than CPU memory. Memory in CPU nodes is composed of narrow 4-bit or 8-bit wide DRAM devices that are grouped together into DIMMs, meaning certain ECC codes (often called chipkill ECC) can correct the data that comes from an entire DRAM device. In contrast, GPU memory is much wider (32-bit wide for GDDR5/GDDR5X and 128-bit for HBM2) such that chipkill-level protection is not possi- ble without a prohibitively large memory access granularity; accordingly, current GPUs use single-bit correcting SEC-DED ECC for DRAM [24], [25]. These lesser correction capabilities lead to a relative increase in detected errors. For example, a ﬁeld study of the Blue Waters system [26] found that the DUE rate per GB of Kepler-era GDDR5 was roughly 5 times that of the chipkill-protected CPU memory. Given the high rate of DUEs expected in the future exascale systems, checkpoints will be more frequent, and so it is im- perative to design checkpointing mechanisms that can reduce the time that applications spend in checkpointing. C. Checkpointing Large-memory CUDA-UVM Applications UVM acts as an enabler for easily developing large-memory CUDA applications. UVM enables a GPU to transparently access host CPU and remote GPU memory, and hence solves the problem of otherwise manually managing data transfers. All of the host CPU’s memory is available, on-demand, by the GPU device. Conversely, all of the UVM memory on the GPU device is available to the CPU. In this situation, the CUDA application may use much more memory than is present on the device. The capacity of GPU memory is currently from 16 to 32 GB for a high- end GPU, while CPU memory often ranges from 128 to 256 GB. In the past, this forced GPU application developer to choose between: scaling out to many nodes and GPUs (hence incurring communication overhead); or manually managing the data transfers on a single GPU. Later, UVM made possible a third choice: transparently transferring data on a single GPU via UVM. However, the ease of developing such large- memory CUDA-UVM applications now places a larger burden on transparent checkpointing to support this large-memory overhead. III. CRUM: DESIGN AND IMPLEMENTATION To address the challenges described in Section II, this paper proposes CRUM, a novel framework that provides a checkpointing-based fault-tolerance mechanism. CRUM en- ables transparent, system-level checkpointing for CUDA and CUDA UVM applications. Figure 3 shows a high-level schematic of CRUM’s archi- tecture. Note especially the organization into two processes: a CUDA program (the user’s application), and a CUDA proxy (the only process that uses the CUDA library to communicate with the GPU). The ﬂow of control is: (i) to interpose on CUDA library calls made by the application process; (ii) to forward the requests to the proxy process; (iii) which then executes the calls via its CUDA library and GPU, on behalf of the application; and (iv) ﬁnally returns the results back to the application. (a) CUDA Original (b) CUDA Proxy Fig. 3: High-level architecture of CRUM In this section, we present the key subsystems in the design of CRUM. The ﬁrst research challenge is the propagation of UVM memory pages (already shared between GPU hardware and proxy process) to make them visible to the application process. Section III-B describes a shadow page scheme (sum- marized in Algorithm 1) for this purpose. The second research challenge is to extend this scheme to overlap checkpointing and computation for the sake of fast, forked checkpoint and future exascale needs. This is discussed in Section III-C. Finally, the implementation details of integrating CRUM with proxy processes is discussed in III-D. A. Post-CUDA 4: The Need for a Proxy Process Ideally, a single-process approach toward checkpointing seems simpler. But this approach for CUDA became non- viable with CUDA 4 and beyond, when NVIDIA implemented uniﬁed virtual addressing with zero-copy, an antecedent of uniﬁed memory [23]). At that point, it was no longer possible to re-initialize the CUDA library at the time of restart. We assume that this is due to the lack of clear semantics about what it means to re-initialize a CUDA library that still retains pointers to uniﬁed memory regions on host and device. One must choose either to free the host memory (thus sabotaging any CUDA application that retains a pointer to the uniﬁed memory region), or else to leave the host memory region intact (thus sabotaging any application assumptions about uniﬁcation of host and device memory). Note that a fresh restart will restore all host memory, but any uniﬁcation of host with device memory has already been lost. The core issue is that the CUDA uniﬁed memory model was developed for standard CUDA applications — and naturally did not include extensions for transparent checkpointing. An alternative workaround would have been, at restart time, to overwrite the text and data memory segments of any CUDA libraries with a fresh, uninitialized CUDA library (matching a freshly booted GPU), and then to call cudaInit(). Unfor- tunately, the CUDA library/driver appeared to have additional state, which made this workaround infeasible. B. Shadow Pages for the Support of UVM Recall the use of a proxy process, as seen in Figure 3(b). The core research challenge in this architecture is that UVM dictates that pages are transparently shared between the GPU hardware and the proxy process, but these shared UVM pages are not visible to the application process. The zero-copy memory of CUDA 4 implies that there are no CUDA calls on which to interpose. In direct-mapped memory, the device may read or write to the host mapped pinned memory of the proxy process at any time. But the separate ap- plication process remains unaware of modiﬁcations to memory in the proxy process. Thus, an approach using CUDA proxies is unable to support the newer and potentially more efﬁcient zero-copy memory for UVA. To overcome this situation, a new, transparent checkpointing approach for CUDA’s zero- copy memory is proposed, in which proxy and application reﬂect a single application with two “personalities”. The CUDA application process and the CUDA proxy pro- cess invoke the same application binary but execute two differ- ent state machines. The application process goes through three different states: CUDA call, read from device-mapped UVM memory, write to device-mapped UVM memory. Note that the state transitions are not dictated by the CRUM framework, but rather by the application logic. On the other hand, the CUDA proxy process is simply a passive listener for requests from the application process and executes the CUDA calls and the memory reads and writes as dictated by the application. Based on these observations, we introduce the concept of “shadow UVM pages”. For every CUDA UVM allocation request by the application, CRUM creates a corresponding shadow UVM region in the context of the application process. At the same time, the CUDA proxy process requests for a “real” UVM region from the device driver. The two processes, the application and the proxy, see two different views of the memory and data at any given point. Since there are no API calls to interpose on, this opens up the requirement for tracking the changes to the application process’s memory in order to keep the two sets of pages in sync. CRUM relies on the use of user-space page-fault tracking to accomplish this. There are currently two available mechanisms for page-fault tracking in Linux: userfaultfd; and segfault handler and page protection bits. While there are certain performance beneﬁts with the use of userfaultfd, the current work uses segfault handler and page protection bits to allow for evaluation on clusters employing older Linux kernels. The algorithm for synchronizing the data on shadow and real UVM pages is described in Algorithm 1. Algorithm 1 Shadow page synchronization algorithm upon event Page Fault do if addr ∈AllShadowPages then if isReadFault() then ReadDataFromRealPage() else MarkPageAsDirty() end if end if upon event CUDA call do if hasDirtyPages then SendDataToRealPages() ClearDirtyPages() end if upon event CUDA Create UVM region do uvmAddr ←CreateUvmRegionOnProxy() reg ←CreateShadowPage(uvmAddr) AllShadowPages ←AllShadowPages ∪reg When an application process requests for a new UVM region, a new shadow UVM region is created in the process’s memory (using the mmap system call). The shadow UVM region is given read-write permissions initially, and all the pages in the regions are marked as “dirty”. When the application makes a CUDA call where the device could potentially read or modify the UVM data (for example, a CUDA kernel launch), the data from dirty pages is “ﬂushed” to the real UVM pages on the proxy process, the dirty ﬂag is cleared for the UVM region, and the read-write permissions are removed (using the mprotect system call). This allows CRUM to interpose on either a read or write to uniﬁed memory. Standard Linux code for segfault handlers allows CRUM to detect an attempt to read or to write, and to distinguish the two cases. In the case of a read, PROT READ permission is set for all of the memory in the application process corresponding to uniﬁed memory. In the case of a write, PROT WRITE permission is set for all of the memory in the application process corresponding to uniﬁed memory. (See Section III-B1 for further discussion.) At a later time, when the application process tries to read the results of the GPU computation back from the shadow UVM regions, a read page fault is triggered; the permissions of the shadow UVM region are changed to read-only, and the results are read in from the corresponding real UVM region on the proxy. 1) Page permissions on Linux: Note that write to shadow UVM memory region requires PROT WRITE permission. Unfortunately, on Linux, PROT WRITE permission implies PROT READ permission also. Linux does not support write- only permission, but rather read-write permission instead. This has consequences for the three-state algorithm to support uniﬁed memory in CRUM. We make the assumption here that most applications will cycle through the three states in order (possibly omitting the read-only or write-only phase). Hence, a typical cycle would be invoked: CUDA-call/read- uniﬁed-memory/write-uniﬁed-memory. In fact, CRUM also supports overlapped execution of a CUDA call with reading and writing uniﬁed memory. The essential assumption is that read access must precede write access and a read-write cycle cannot be followed with a second read unless there is an intervening CUDA kernel. Normal CUDA calls such as cudaMemcpy are allowed at all times. As discussed earlier, unfortunately, Linux’s write-only per- mission for memory actually grants read-write permission. It is for this reason that a transition from write-uniﬁed-memory directly to read-uniﬁed-memory cannot be detected efﬁciently. Possible solutions are discussed at the end of this section. This assumption has been found to hold in each of the real- world applications that we have found for testing CRUM with uniﬁed memory. Nevertheless, it is important to also build in a (possibly slower) veriﬁed execution mode that will test an application to see if it violates this assumed cycle of CUDA- call/read-uniﬁed-memory/write-uniﬁed-memory. There is more than one way to implement a veriﬁed execu- tion mode. One of the difﬁculties is that a Linux segfault handler does not allow us to reset the page permission to allow only the pending read or write, and then reset the permission back to PROT NONE. Linux’s user-space page fault handling, userfaultfd, introduced with Linux 4.3, can ﬁx this, but that introduces other technical difﬁculties. (For example, it was only with Linux 4.11 that this was extended partially to support fork and shared memory.) Another alternative is to parse the pending read or write (load or store assembly instruction), temporarily allow read-write permission to the desired memory page, and then use the parsed information to read or write the data between register and memory, and ﬁnally to restore the previous memory permission. This might be more efﬁcient than user-space page faulting since it might have fewer transitions to a kernel system call. Linux kernel modiﬁcation to support write-only permissions for UVM shadow pages is another possibility. C. Fast, Forked Checkpoints UVM enables CUDA applications to use all of the host and GPU device memory transparently. This can make check- pointing, which is dominated by the time to write to the disk, prohibitively expensive. So while one could employ copy- on-write-based asynchronous checkpointing, UVM memory is incompatible with shared memory and fork on Linux. Fortunately, CRUM’s proxy-based architecture can be used to address this challenge. Note that the device state and the UVM memory regions are not directly a part of the application process’s context, but rather they are associated with the proxy process. This frees up the application process to use forked checkpointing for copy-on-write-based associated checkpointing for the application process. Forked checkpointing allows CRUM to invoke a minimal checkpointing delay in order to “drain the GPU device” of its data, after which, a child process of each MPI process can write to stable storage. This allows the system to overlap the CUDA computation with storage of the checkpoint image in stable storage. D. Checkpoint-Restart Methodology and Integration with Proxies Finally, for completeness, we discuss how CRUM integrates proxy concepts into the CUDA implementation requirements. Proxies have also been used by previous authors (see Sec- tion VI-d). At checkpoint time, CRUM suspends the user application threads, and “drains” the GPU kernel queue. It issues a device synchronize call (cudaDeviceSynchronize) to ensure that the kernels have ﬁnished execution and the memory state is consistent. Then, for all the active CUDA-MALLOC and CUDA-UVM memory regions, data is read in from the GPU to the host. The data is ﬁrst transferred from the GPU into the proxy process’s memory, and then from the memory of the proxy process into the memory of the user application process. The user application process then disconnects from the proxy process. This ensures that the problem reduces to the trivial problem of checkpointing a single-process application. Finally, the state of the process is saved to a checkpoint image ﬁle on stable storage. At the time of restart, CRUM starts a new process and recreates the user application threads. Then, the memory of the new process gets replaced by the saved state from the checkpoint image ﬁle. CRUM, then, starts a new proxy process, which starts a new GPU context. It recreates the active CUDA-MALLOC and CUDA-UVM memory regions by replaying the allocation calls. CUDA streams and events are similarly handled. (See Section V for further discussion.) Finally, CRUM transfers the data into the actual CUDA and CUDA-UVM regions through the proxy process and resumes the application threads. IV. EXPERIMENTAL EVALUATION The goal of this section is to present a detailed analysis of the performance of CRUM. In particular, this section answers the following questions: Q1 What’s the overhead of running a CUDA (or a CUDA UVM) application under CRUM? Q2 Does CRUM provide the ability to checkpoint CUDA (and CUDA UVM) applications? Q3 Can CRUM improve a CUDA UVM based application’s throughput by reducing the checkpointing overhead? Q4 Is the approach scalable? A. Setup To answer the above questions, we ﬁrst brieﬂy describe our experimental setup and methodology. 1) Hardware: The experiments were run on a local cluster with 4 nodes. Each node is equipped with 4 NVIDIA PCIe- attached Tesla P100 GPU devices, each with 16 GB of RAM. The host machine is running a 16-core Intel Xeon E5-2698 v3 (2.30 GHz) processor with 256 GB of RAM. Each node runs CentOS-7.3 with Linux kernel version 3.10. 2) Software: Each GPU runs NVIDIA CUDA version 8.0.44 with driver 396.26. Experiments use DMTCP [27] ver- sion 3.0. We developed a CRUM-speciﬁc DMTCP plugin [28] for checkpoint-restart of NVIDIA CUDA UVM applications. The DMTCP CRUM plugin (referred to as the CRUM plugin from here onwards) interposes on the CUDA calls made by the application. The interposition code is generated in a semi-automated way, where a user speciﬁes the prototype of a CUDA function, and whether the call needs to be logged. This not only allows us to cover the extensive CUDA API, but also allows for ease of maintainability and for future CUDA extensions. The plugin forwards the requests, over a SysV shared memory region, to a proxy process running on the same node. The forwarded request is then executed by the proxy process, which then returns the results back to the appli- cation. To improve the performance, we use well-studied concepts from pipelining of requests, to allow the application to send requests without blocking. Blocking requests, such as, cudaDeviceSynchronize, result in a pipeline ﬂush. For data transfers (both for UVM shadow page data and for cudaMalloc data) we use Linux’s Cross Memory Attach (CMA) to allow for data transfers using a single copy opera- tion. 3) Application Benchmarks: We use Rodinia 3.1 [29] benchmarks for evaluating CRUM for CUDA applications. Note that the Rodinia benchmarks do not use UVM, and can be run even with CUDA 2.x. They are included here to show comparability of the new approach with the older work from 2011 and earlier using CUDA 2.x [10], [12], [30]. We note that CheCUDA [10] does not work for modern CUDA (i.e., CUDA version 4 and above) because it relies on a single-process checkpoint-restart approach. CheCL [30] only supports OpenCL and does not work with CUDA. We tried compiling the CRCUDA [13] version available online [31], but it failed to compile with CUDA version 8. It didn’t work for the benchmarks used in our experiments, after applying our compilation ﬁxes. To evaluate CRUM using UVM-managed memory alloca- tion, we run a GPU-accelerated build of two DOE benchmarks: a high-performance geometric multigrid proxy application (HPGMG-FV [32]), and a test application using a production linear system solver library (HYPRE [33]). For the HYPRE library, we run the test driver for an unstructured matrix interface using the AMG-PCG solver. For HPGMG-FV, we evaluate two versions: the standard HPGMG-FV benchmark with one grid (the master branch, as described in [34]), and an AMR proxy modiﬁcation with multiple AMR levels (the amr proxy branch, as described in [21]). We focus on HPGMG-FV and HYPRE because they are scientiﬁc applications and libraries with potential importance in future exascale computing [35], and they have publicly available ports to UVM-enabled multi-GPU CUDA. HPGMG- FV has also been used as a benchmark for ranking the speeds of the top supercomputers [36]. To evaluate the relative performance of HPGMG-FV runs, we quote its throughput in degrees-of-freedom per second — the same metric used to rank supercomputer speeds [36]. Thus, larger numbers indicate higher performance. To evaluate the relative performance of HYPRE runs, we measure the wall clock time taken by each program execution. B. Runtime Overhead While the ability to checkpoint is important for improving the throughput of an application on a system with frequent failures, a checkpointing system that imposes excessive run- time overhead can render the framework ineffective, and in the worst case, reduce the throughput. We, therefore, bench- mark and analyze the sources of runtime overhead. For these experiments, no checkpoint or restart was invoked during the run of the application. The results demonstrate that CRUM is able to run the CUDA application with a worst case overhead of 12%, and a 6% overhead on average. We note that this is a prototype implementation and a production system could incorporate many optimizations to further reduce the overhead. TABLE I: Runtime parameters for Rodinia applications. Application Conﬁguration Parameter LUD “-s 2048 -v” Hotspot3D “512 8 1000 power 512x8 temp 512x8” Gaussian “-s 8192” LavaMD “-boxes1d 40” Figure 4(a) shows the runtimes for four applications (LUD, Hotspot3D, Gaussian, and LavaMD) from the Rodinia benchmark suite with and without CRUM. The applications mostly use the CUDA API’s from CUDA 2.x: cudaMalloc, cudaMemcpy, and cudaLaunch. Table I shows the conﬁg- uration parameters used for the experiments. We observe that the runtime overhead varies from 1% (for LUD) to 3% (in the case of LavaMD). The runtime overhead is dominated by the LUD Hotspot3D Gaussian LavaMD 0 20 40 60 80 Runtime (s) Native With CRUM (a) Rodinia Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 2 4 6 8 10 12 14 DOF/s Overhead (%) Level-1 Level-2 Level-3 (b) HPGMG-FV Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 400 500 600 700 800 900 Runtime (s) Native With CRUM (c) HYPRE Benchmark. Fig. 4: Runtime overheads for different benchmarks under CRUM. cost of data transfers from the application process to the proxy process. In a different experiment, using Unix domain sockets for data transfer, we observed overheads varying from 1.5% to 16.5%. The use of CMA reduces the overhead signiﬁcantly. Figure 4(b) shows the runtime results for the HPGMG- FV benchmark with increasing number of nodes and MPI ranks. As noted in Section IV-A3, we use the HPGMG-FV throughput metric DOF/s as a proxy for performance. We note that the DOF/s reported by the application running under CRUM are less than the native numbers by 6% to 12%. We present a more in-depth analysis below. In our experiments, we observed that a single MPI rank of the HPGMG-FV benchmark runs about 9 million CUDA kernels during its runtime of 3 minutes. This implies that each CUDA kernel runs for approximately 20 microseconds on average. Note that the cost of executing a cudaLaunch call itself can be up to 5 microseconds. The program allocates many CUDA UVM regions, sets up the data, and runs a series of kernels to operate on the data. Each MPI rank then exchanges the results with its neighbors. While the size of the UVM regions vary from 12 KB to 128 KB, the frequent reads and writes the application process, stresses the CRUM framework in two dimensions: (a) frequent interrupts and data transfer; and (b) frequent context switches and the need to synchronize with proxy process (because of the many CUDA calls that need to be executed). While the use of CMA (cross-memory attach) reduces the cost of data transfers, interestingly, we observed a lot of variability in the cost of a single CMA operation for the same data transfer size. The cost of a single page transfer varies from 1 microsecond to 1 millisecond, a difference of three orders of magnitude. We attribute this to two sources: (a) O/S jitter; (b) the pre-fetching algorithm employed by the UVM driver. In many cases, reading a UVM page is slowed down because of a previous read on a large UVM region, spanning several pages, because the driver gets busy pre-fetching the data for the large UVM region. To address the second source of overhead, we optimized the CRUM implementation to: (a) use a lock-free, inter- process synchronization mechanism over shared-memory; and (b) pipeline non-blocking CUDA calls from the application. A CUDA call, such has cudaLaunch, cudaMemsetAsync, is pipelined and the application is allowed to move ahead in its execution, while the proxy ﬁnishes servicing the request. At a synchronization point, like cudaDeviceSynchronize, the application must wait for a pipeline ﬂush, i.e., for the pending requests to be completed. Figure 4(c) shows the runtimes for the HYPRE benchmark for a different number of MPI ranks running on a varying num- ber of nodes. The benchmark observes up to 6.6% overhead when running under CRUM compared to native execution. The HYPRE benchmark presents different checkpointing challenges than HPGMG-FV. While the HYPRE benchmark invokes only about 100 CUDA kernels per second (10 millisec- onds on average per kernel) during its execution, it uses many large UVM regions (up to 900 MB). Thus, the overhead is dominated by the cost of data transfers between the application process and the proxy. In addition to CMA, CRUM employs a simple heuristic to help reduce the data transfer overhead. For small shadow UVM regions, it reads in all of the data from the real UVM pages on the proxy. However, for a read fault on a large shadow UVM region, it starts off by only reading the data for just one page containing the faulting address. On subsequent read faults on the same region, while in the read phase (see Section III), we exponentially increase (by powers of 2) the number of pages read in from the real UVM region on the proxy. This heuristic relies on the spatial and temporal locality of accesses. While there will be pathological cases where an application does “seemingly” random reads from different UVM regions, we have found this assumption to be valid in the two applications we tested. C. Checkpointing CUDA Applications: Rodinia and MPI Next, we evaluate the ability of CRUM to provide fault tolerance for CUDA and CUDA UVM applications using checkpoint-restart. Figure 5(a) shows the checkpoint times, restart times, and the checkpoint image sizes for the four applications from the Rodinia benchmark suite. The checkpointing overhead is dependent on the time to transfer the data from the device memory to the host memory, then transferring it from the LUD Hotspot3D Gaussian LavaMD 0 1 2 3 4 5 6 7 8 Time (s) 101MB 84MB 542MB 1.1GB Checkpoint Restart (a) Rodinia Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 0 1 2 3 4 5 6 7 8 Time (s) 113MB 113MB 113MB Checkpoint Restart (b) HPGMG-FV Benchmark. 1x8 2x8 4x8 Num. of MPI ranks 0 5 10 15 20 25 30 Time (s) 3.5GB 1.6GB 790MB Checkpoint Restart (c) HYPRE Benchmark. Fig. 5: Checkpoint-restart times and checkpoint image sizes for different benchmarks under CRUM. proxy process to the application process using CMA, and then ﬁnally writing to the disk. We observe that the time to write dominates the checkpointing time. Figure 5(b) shows the checkpoint times, restart times, and the checkpoint image sizes for HPGMG. The results are shown with increasing number of MPI ranks (and the number the nodes). We observe that as the total amount of checkpointing data increases from 904 MB (8 × 113 MB) to 3.6 GB (32 × 113 MB), the checkpoint time increases from 3 seconds to 8 seconds. We attribute the small checkpoint times to the buffer cache on Linux. We observed that forcing the ﬁles to be synced (by using an explicit call to fsync increased the checkpoint times by up to 3 times. The results for HYPRE are shown in Figure 5(c). The application divides a ﬁxed amount of data (approx. 28 GB in total) equally among its ranks. So, we observe that the checkpoint image size reduces by almost half every time we double the number of ranks. This helps improve the checkpoint cost especially with smaller process sizes, as the Linux buffer caches the writes, and the checkpoint times reduce from 31 seconds (for 8 ranks on 1 node) to 8 seconds (for 32 ranks over 4 nodes). D. Reducing the Checkpointing Overhead: A Synthetic Bench- mark for a Single GPU To showcase the beneﬁts of using CRUM to reduce check- pointing overhead for CUDA UVM applications, we develop a CUDA UVM synthetic benchmark. The synthetic benchmark allocates two vectors of 232 4-byte ﬂoating point numbers (32 GB in total) and computes the dot product of the two vectors. The ﬂoating point numbers are generated at random. Note that the total memory requirements are double of what is available on the GPU device (16 GB). However, UVM allows an application to use more than the available memory on the GPU device. The host memory, in this case, acts as “swap storage” for the device and the pages are migrated to the device or to the host on demand. Table II shows the checkpoint times for three different cases: (a) using a na¨ıve checkpointing approach; (b) using three different compression schemes, Gzip, Parallel Gzip, and LZ4, before writing to the disk; and (c) using CRUM’s forked TABLE II: Checkpoint times using different strategies for the synthetic benchmark. Strategy Ckpt Time Ckpt Size Data Migration Time Na¨ıve 45 s 33 GB (100% random) 4 s Gzip 1296 s 29 GB (100% random) 4 s Parallel gzip 86 s 29 GB (100% random) 4 s LZ4 62 s 33 GB (100% random) 4 s Forked Ckpting 4.1 s 32 GB (100% random) 4 s Gzip 749 s 15 GB (50% random) 4 s Parallel gzip 56 s 15 GB (50% random) 4 s LZ4 45 s 17 GB (50% random) 4 s checkpointing approach. The ﬁrst two approaches, na¨ıve and compression, use CRUM’s CUDA UVM checkpointing frame- work. The third approach adds the forked checkpointing opti- mization to the base CUDA UVM checkpointing framework. The three compression schemes use Gzip’s lowest compression level (-1 ﬂag). While parallel Gzip uses the same compression algorithm as Gzip, it launches as many threads as the number of cores on a node to compress input data. We observe that the forked checkpointing approach out- performs the other two approaches by up to three orders of magnitude. Since the program uses random ﬂoating point numbers, compression is ineffective at reducing the size of the checkpointing data (Table II). We note that the time taken by the compression algorithm is also correlated with the ran- domness of data. As an experiment, we introduced redundancy in the two input vectors to improve the “compressibility”. Of the 232 ﬂoating point elements in a vector, only half (216) of the elements were generated randomly and the rest were assigned the same ﬂoating point number. This improves the compression time and reduces the checkpoint time to 749 seconds and the checkpoint image size is reduced to 15 GB by using the Gzip-based strategy. Note that parallel Gzip may not be a practical option in many HPC scenarios, where an application often uses one MPI rank per core on a node. On the other hand, LZ4 provides a computationally fast compression algorithm at the cost of a lower compression ratio. E. Reducing the Checkpoint Overhead: Real-world MPI Ap- plications Finally, we present the results from using CRUM with the forked checkpointing optimization for the real-world CUDA UVM application benchmarks. The results reported here cor- respond to the largest scale of 4 CPU nodes, with 16 GPU devices, running 8 MPI ranks per node (32 processes in total). TABLE III: Checkpoint times using different strategies for real-world CUDA UVM applications. The numbers reported corresponds to running 32 MPI ranks over 4 nodes. The checkpoint size reported is for each MPI rank. The checkpoint times are normalized to the time for the na¨ıve checkpointing approach (1x). App. Strategy Ckpt Time Ckpt Size HPGMG-FV Gzip 0.78x 14 MB HPGMG-FV Parallel gzip 0.60x 14 MB HPGMG-FV LZ4 0.30x 16 MB HPGMG-FV Forked ckpting 0.025x 113 MB HYPRE Gzip 2x 176 MB HYPRE Parallel gzip 1x 176 MB HYPRE LZ4 1x 296 MB HYPRE Forked ckpting 0.032x 868 MB Table III shows the results for checkpointing time (and checkpoint image sizes) normalized to the checkpointing time using the na¨ıve checkpointing approach (as shown in Fig- ures 5(b) and 5(c)). The results are shown for HPGMG-FV and HYPRE. We observe trends similar to the synthetic benchmark case. While in the na¨ıve checkpointing approach, the checkpointing overhead is dominated by the cost of I/O, i.e., writing the data to the disk, under forked checkpointing, the overhead is dominated by the cost of in-memory data transfers: from the GPU to the proxy process, and from the proxy process’s address space to the application process’s address space. Fur- ther, the cost of quiescing the application process, quiescing the network (for MPI), and “draining” and saving the in-ﬂight network messages is 0.01% of the total cost. However, unlike the synthetic benchmark, using in-memory compression to reduce the size of data for writing is better in this case for both HPGMG and HYPRE. This indicates that the compression algorithm is able to efﬁciently reduce the size of the data, which helps lower the I/O overhead. Note that this is still worse than using forked checkpointing by an order of magnitude. V. DISCUSSION Driver support for restart: In order to restart a computa- tion, CRUM must re-allocate memory in the same locations as during the original execution—otherwise the correctness of pointer-based code cannot be guaranteed during re-execution. The current CRUM prototype relies on deterministic CUDA memory allocation, which we verify to work with the CUDA driver libraries via experimentation (for both explicit device memory and UVM-managed memory allocation). The as- sumption of deterministic memory re-allocation is shared by previous GPU checkpointing efforts [12]. Memory Overhead: In a CUDA program with large data resident on the host, the memory overhead due to an addi- tional proxy process could be a concern. In the special case of asynchronous checkpointing, the overhead could be even higher, although copy-on-write does prevent it from going too high. This could be ameliorated by future support for shared memory UVM pages between application and a proxy running CUDA. Advanced CUDA language features: Dynamic parallelism allows CUDA kernels to recurse and launch nested work; it is supported by CRUM without change. Device-side memory allocation allows kernel code to allocate and de-allocate mem- ory. It is partially supported by CRUM, with one important distinction—no live device-side allocations are allowed at a checkpoint time. Thus, device-side memory allocations are to be freed before the system is considered quiesced and ready for a checkpoint. We do not anticipate this constraint to be particularly difﬁcult to satisfy, since device-side mallocs tend to be used to store temporary thread-local state within a single kernel, whereas host-side CUDA memory allocation (which is supported by CRUM without restriction) is more often used for persistent storage. Using mprotect: Currently, in a Linux kernel, PROT_WRITE protection for a memory region implies read-write memory permission rather than write-only memory permission. Because of this, some compromises were needed in the implementation. This work has demonstrated the practical advantages of a write-only memory permission for ordinary Linux virtual memory. It is hoped that in the future, the kernel developers at NVIDIA will be encouraged to support write-only memory permission for this purpose. Another issue with an mprotect-based approach is that when kernel-space code page faults on a read/write protected page, it returns an error code to the user, EFAULT, rather than a segfault. This forces the implementation to be extended to handle such failures; the implementation cannot rely solely on a segfault handler [37]–[40]. Other APIs and Languages: This work provides checkpoint-restart capabilities for programs written in C/C++ with the CUDA runtime library. In our experience, the CRUM prototype should support the majority of GPU-accelerated HPC workloads; however, there are other APIs to that may be valuable for some users. Given the current framework of code auto-generation for CRUM, we believe that it will be straightforward to extend the implementation to support other APIs, such as OpenACC. The ability of CRUM to support UVM-managed memory would be especially useful for Ope- nACC programs, as PGI’s OpenACC compiler provides native and transparent support for high-performance UVM-managed programs, making UVM-accelerated OpenACC programs a low-design-effort route to performant GPU acceleration [41]. Future Versions of CUDA: Just as prior checkpointing methods for GPUs were unable to cope with versions of CUDA since CUDA 4 (released in 2011), it is likely that CRUM will need to be updated to support language fea- tures after CUDA 8. One such development is Heterogeneous Memory Management (HMM) [42], which is a kernel feature introduced in Linux 4.14 that removes the need for explicit cu- daMallocManaged calls (or use of the managed keyword) to denote UVM-managed data. Rather, with HMM the GPU is able to access any program state, including the entire stack and heap. Because the current CRUM prototype relies on wrapping cudaMallocManaged calls, it will need to be redesigned to support HMM. VI. RELATED WORK a) Use of proxy process: Zandy et al. [43] demonstrated the use of a “shadow” process for checkpointing currently running application processes that were not originally linked with a checkpointing library. This allows the application process to continue to access its kernel resources, such as open ﬁles, via RPC calls with the shadow process. Kharbutli et al. [44] use a proxy process for isolation of heap accesses by a process and for containment of attacks to the heap. b) GPU virtualization: A large number of previous HPC studies have focused on virtualizing the access to the GPU [8]– [10], [12], [13], [30], [45], [46]. Here we describe some of those studies, with an emphasis on the use for GPU checkpointing and GPU-as-a-Service in the cloud and HPC environments. Lagar-Cavilla et al. [45], Shi et al. [8], Gupta et al. [9], and Giunta et al. [46] focus on providing access to the GPU for processes running in a virtual machine (VM), as an alternative to PCI pass-through. The access is provided by forwarding GPU calls to a proxy process that runs outside the VM and has direct access to the GPU. c) GPU-as-a-Service: Two other efforts, DS-CUDA [47] and rCUDA [48], have focused on providing access to a remote GPU for the purposes of GPU-as-a-Service [49]–[55]. They also rely on a proxy process. Using the proxy process is similar to the one described in this work; however, the focus is on efﬁcient remote access by using the InﬁniBand’s RDMA API. To the best of our knowledge, none of the previous studies solve the problem of efﬁcient checkpointing of modern CUDA applications that use UVM. We note that the optimizations described in these works can be used in conjunction with CRUM for providing efﬁcient access to remote GPUs. d) GPU Checkpointing: Early work on virtualizing or checkpointing GPUs was based on CUDA 2.2 and earlier [8]– [12]. Those approaches stopped working with CUDA 4 (intro- duced in 2011), which introduced Uniﬁed Virtual Addressing (UVA). Presumably, it is the introduction of UVA that made it impossible to re-initialize CUDA 4. In 2016, CRCUDA [13], employed a proxy-based approach, similar to the 2011 approach of CheCL [30] that targeted OpenCL [56] (as opposed to CUDA) for GPUs. OpenCL does not support uniﬁed memory, and so CheCL and CRCUDA do not support NVIDIA’s uniﬁed memory [23] targeted here. VOCL-FT [57] aims to provide resilience against soft er- rors. VOCL-FT leverages the OpenCL programming model to reduce the amount of data movement: both to/from the device from/to the host, and to/from the disk. This allows them to do fast checkpointing and recovery. HiAL-Ckpt [58], HeteroCheckpoint [59], and cudaCR [60] use application-speciﬁc approaches for providing GPU check- pointing. None of the approaches described above work for CUDA UVM. CRUM focuses on providing efﬁcient runtime and checkpointing support for CUDA and CUDA-UVM based programs. We note that the techniques described in above approaches are complementary to CRUM and can be used to further optimize the runtime and checkpointing overheads. VII. CONCLUSION This paper introduced CRUM, a novel framework for checkpoint-restart for CUDA’s uniﬁed memory. The frame- work employs a proxy-based architecture along with a novel shadow page synchronization mechanism to efﬁciently run and checkpoint CUDA UVM applications. Furthermore, the architecture enables fast, copy-on-write-based, asynchronous checkpointing for large-memory CUDA UVM applications. Evaluation results with a prototype implementation show that average runtime overhead imposed is less than 6%, while improving the checkpointing overhead by up to 40 times."
Systems and Networking,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","March 15th, 2018",Transparently Checkpointing Software Test Benches to Improve Productivity of SoC Verification in an Emulation Environment,http://www.ccs.neu.edu/home/gene/papers/dvcon-us-18.pdf," ""Transparently Checkpointing Software Test Benches to Improve Productivity of SoC Verification in an Emulation Environment"", Ankit Garg, Suresh Krishnamurthy, Gene Cooperman, Rohan Garg, and Jeff Evans, <em>2018 Design and Verification Conference and Exhibition</em> (DVCON-US 2018)","Traditionally hardware emulation has been used in in-circuit emulation (ICE) mode where the design under test (DUT) executes inside the emulator and connected to the real target, which acts as a testbench. Over time, the software testbench-based emulation environments have become very popular, since the users can control its operation remotely from their desktops. This makes emulators an enterprise resource, accessible to a multitude of users spread across continents and multiple time zones. And since emulators are expensive resources, it is important to utilize them efficiently. Full checkpoint save/restore capability of emulation jobs helps the utilization by enabling flexible job scheduling, shortening of jobs by jumping ahead to interesting points for debug, carrying out what-if analysis, etc. Emulators have native save/restore capabilities for the model on the emulator. The software testbenches can be complex in multiple dimensions. For example, they may be using C/C++, SystemC, SystemVerilog, etc. They may be multi- threaded and based on multiple processes, they may be using IPC, and so on. It then becomes a challenge to save the states of such sophisticated software testbenches both transparently and through a uniform, reliable, mechanism. Since the objective is to solve the problem at an enterprise level, it is critical to find a uniform solution for a diverse set of software testbenches throughout the enterprise. The DMTCP (Distributed MultiThreaded Checkpointing) package supports such a uniform solution. This paper describes the integration of DMTCP with a virtual testbench-based emulation. This brings large benefits to a real life environment that includes multiple emulators within the larger set of enterprise resources. There are other, additional applications of full checkpoint save/restore for emulation jobs that become apparent only after having gained experience with its use in job management. For example, after having observed the behavior of an application prior to checkpoint, additional triggers can be inserted at the time of restore, to enhance debugging of an exception or other unusual behavior.s. II. BACKGROUND: CO-MODELING AND DMTCP A. Review of Co-Modeling Architecture Co-emulation, or (transaction-level) Co-Modeling, is the process of modeling cycle-accurate synthesizable hardware models (DUTs) running on an emulator, communicating with testbenches at transaction level via a high- speed link between the emulator and the host system. The reusable testbenches are interfaced to synthesizable transactors co-located with the DUT in the emulator. These “accelerated” transactors convert high-level transactions to signal-level stimuli to drive the DUT. During an emulation run, the hardware communicates with the software testbench using a high-speed link. For every DUT clock or for each time point in the model execution, communication may be required by one or more synthesizable transactors. In the general case, the DUT clocks will be suspended at times to complete all the communication requests for a given point in the model execution. So at any point of time, there could be inflight data in the link. Figure 1. Co-Modeling Architecture B. Review of DMTCP flow DMTCP is an open-source package that provides a capability for Checkpoint/Restart in applications involving multiple processes/threads distributed across multiple hosts and connected by socket connections. The package can be downloaded from: http://dmtcp.sourceforge.net It operates under Linux, with no modifications to the Linux kernel or to the user code, and it can be used by unprivileged users (no root privilege needed). One can later restart from a checkpoint, or even migrate the processes by moving the checkpoint files to another host prior to restarting. Figure 2 shows the typical flow of a user job under DMTCP. Figure 2. Typical flow of a user job under DMTCP A detailed description on DMTCP (Distributed MultiThreaded Checkpointing) internals can be found in [1]. DMTCP also provides a flexible plugin model that supports the ability to write an add-on library that can: support DMTCP event hooks; add custom wrappers around system calls; and add a custom distributed name service facility [2, 3]. III. MOTIVATING USE CASES Checkpoint-restore of emulation jobs opens a wide of range of applications, which will help users build a fault- tolerant emulation system, powerful debug mechanisms, and increase usage efficiency of critical resources like hardware emulators. This section describes different use cases for such capability in emulation. The following section, Section IV, then presents the Checkpoint/Restore Framework in detail. A. Skipping repeated initial sequences Designs may have an initialization phase that is always executed for each test. This may be a hardware reset phase or boot-up, which takes a great deal of time before an actual test can start. We can save much of the emulation runtime by taking a checkpoint right after this repeated initial sequence. New tests can then just restart immediately after this initial sequence, thus saving a great deal of regression time. Another application of this is to do what-if analysis after reaching an interesting point in the execution. In Figure 3, each test Test0, Test1, Test2 executes the same initial sequence, which takes C1 time. Figure 3. Job Progress with fixed initialization sequence The tests then follow their own unique paths, completing the test in different time intervals (T0, T1, and T2). This C1 time is saved if we checkpoint just after completion of the initial sequence and then restore from the checkpoint at the original state in each test. B. Better Job Management Policies One of the advantages of virtualization of test environments is the ability to use emulators remotely. This allows emulation to be moved to the data center, with jobs being managed by a workload management platform such as LSF. However, the non-pre-emptive nature of emulation jobs forbids pre-emptive scheduling policies. This prevents a high-priority job from acquiring the resources occupied by a currently executing low-priority job. For example, a currently running long job cannot be removed even though a short job has just arrived. The short job has to wait until the long job exits and frees up the resources. Hence, the non-pre-emptive nature of emulation jobs can easily lead to inefficient use of emulators. Consider the scenario depicted in Figure 4: Figure 4. Job Management Job1 occupies 4 slots on Emulator1 and Job2 occupies 2 slots on Emulator2. Further a high-priority request from Job3 arrives for 6 resources. As resources are fragmented, irrespective of its priority, Job3 has to wait for either of Job1 or Job2 to finish. The Checkpoint/Restore capability can remove this shortcoming and enable a pre-emptive scheduling policy. We could have checkpointed either of Job1 or Job2 and freed up the resources for Job3. Whenever resources become available, the checkpointed job can then be restarted using the DMTCP restart scripts. Further, this situation also leads to inefficient use of emulators due to resource fragmentation within each emulator. Using Checkpoint/Restore we could have migrated an interfering job to a different emulator, thereby making contiguous resources available for the new job. In Figure 5, the checkpoint of Job2 is taken first, and then Job2 is restarted on Emulator1. This frees up all 6 slots on Emulator2 for Job3. Job3 can now be allocated using contiguous resources on Emulator2. Figure 5. Resource free with Job Migration Another interesting example concerns the issue of “fairness” for large-capacity jobs. In a distribution system dominated by high-priority, small-capacity jobs, a low-priority large-capacity job may never actually get a chance to run if the policy is to wait for required slots to become available. Further, a high-priority long job can lead to underutilization of emulation resources as freed slots cannot be allocated for other small jobs if they have to be pooled for a high-priority long job waiting in the queue. With checkpoint-restart, one can save all the small jobs at once, run the large job and then restart the small jobs. A long low-priority job might otherwise never get an opportunity to complete, if not for the use of checkpoint-restart. There are several such instances where checkpoint- restart can offer this kind of flexibility in an emulation job scheduling system. C. Debugging from past simulation time Debugging is an important requirement for verification engineers. Much time is spent in debugging functional issues in design as well as integration issues with software during validation of a full SoC. The ability to take a checkpoint of the full system can provide capabilities to start debugging just before an issue occurs, by restarting from the point of a previous checkpoint state. For example, one can take periodic checkpoints and when a problem is seen: start from the last checkpoint, run again and capture more debug information. This is a tremendous advantage when a debugging issue occurs only after a run of long duration. A large amount of time is saved, since one no longer needs to wait for the test to arrive at the point of interest. IV. CHECKPOINT/RESTORE FRAMEWORK In this section we describe the implementation of checkpoint-restore for emulation jobs. We address checkpoint and restore separately. Figure 6 shows the typical flow of an emulation job under DMTCP. Note that in Figure 2, the checkpoint was invoked by the user at an arbitrary point in time. However, in the emulation flow (Figure 6), the user invokes the checkpoint programmatically from within the user code, when the simulation reaches certain number of cycles. Figure 6. Typical flow of an emulation job under DMTCP. A. Checkpoint Checkpointing of emulation jobs involves both the emulator and the testbench on the workstation. One must save the simulation state for the design running on an emulator, while also employing binary-level checkpointing using DMTCP for the processes representing the testbench side and running on the workstation. As always, we have to make sure that the hardware side stops generating further transactions when a checkpoint is in progress. In addition, checkpointing requires that we make sure that there is no in-flight data inside the high-speed interface between the emulator and the testbench at the time of checkpointing. In the case that a checkpoint is taken without flushing the in-flight data, those transactions will be lost at the time of restoring from the checkpoint, thus resulting both in incorrect hardware state and incorrect state on the testbench side. Figure 7. Flowchart for Checkpoint Algorithm The following is the integrated algorithm for checkpointing of an emulation job. These steps are also depicted graphically in Figure 7. 1) A Checkpoint Request is received by the testbench either through an external utility such as dmtcp_command or through a user-exposed API, ScheduleCheckpoint, made by testbench itself. The request is sent to the emulator in order to freeze the running design. This will stop both the user design clocks and the simulation time itself. 2) Once a design is frozen, additional transactions from the hardware side will no longer be generated. At this point the data in flight inside the link is flushed until all of them have been processed and the emulator reaches a quiescent state. 3) When the emulator has reached a quiescent state, the system initiates the checkpoint of the emulator model by using the checkpoint technology native to the emulator. This will store the current design state to disk. 4) The system then disconnects from the emulator and makes sure that all emulator-related processes are terminated. This step will ensure that any external connections not running under DMTCP are removed from consideration. For example, there may be connections to a licensing server, to some waveform dump servers, and so on. This also reduces the overall checkpoint time by freeing up memory used by these external connections that the binary checkpointing procedure would otherwise have to save, as described in the next step. Further, checkpoints taken this way are independent of which specific emulator the session was running on, and this makes it easy to relocate saved emulation jobs to other emulators. 5) Next, the system initiates DMTCP-based checkpointing on the testbench side. This involves calling dmtcp_checkpoint, a part of the DMTCP API. Here, one writes an emulation-specific external plugin for DMTCP, which will specify which files must be checkpointed. The DMTCP plugin is specific to the emulator and testbench infrastructure, and must also specify the path to the checkpoint database, where DMTCP checkpoint image files are saved alongside the hardware database, as a consolidated database within the user specified path. . DMTCP also allows a user to have certain processes be explicitly excluded from checkpointing. This might be required for the case where processes have been spawned from an external library linked into the user testbench. Those processes are not required as part of a correct checkpoint state. And further, attempting to save such superfluous processes leads to other issues when restoring from a checkpoint if those processes were communicating with external processes that were not running under DMTCP. The details of DMTCP external plugins can be found in [2, 3]. 6) After the invocation of dmtcp_checkpoint returns, we can choose either to exit the current emulation or to resume the current run. 7) After resuming from a checkpoint, the following steps are taken: ● Re-connect to same emulator and configure the same design again. ● Restart the tool-specific processes and re-connect to servers from which they exited in step (4). This step is isolated to the emulation tool’s internal workings, and does not require the collaboration of DMTCP. ● Perform a hardware design restore from the same checkpoint database and start the design clocks. ● Note that after being restored from a checkpoint, the testbench side resumes in a correct state at the end of this procedure, and so nothing more is required on the testbench side. B. Restore/Restart The DMTCP package dumps a restart script at the time of checkpoint. This script takes care of restarting the entire testbench tree of processes. This script also takes care of restarting processes that were on remote machines. After restart, the system will “wake up” in step (6) in the checkpoint sequence above, as if we have just returned from the call to dmtcp_checkpoint. So the remaining steps to restart are the same as those described in step (7) above. Figure 8. Flowchart for Restore The following is the algorithm for the restart/resume. These steps are also depicted graphically in Figure 8. 1) Use the restart script generated by DMTCP that relies on the checkpoint database (the checkpoint image files) in order to bring the whole testbench tree of processes up and running. 2) After DMTCP’s restore/restart, we return from the call to dmtcp_checkpoint. The next steps bring the HDL side up and fork any tool-specific processes. 3) The system then re-connects the emulator resource and downloads the design. 4) The system then restarts the tool-specific processes and re-connects to external servers such as license servers, waveform collection servers, etc. This step is completely isolated from the internal workings of the emulator tool. 5) The system then performs the hardware design restore using the same checkpoint database, and it starts the design clocks. This is the last step in successfully restoring the emulation. V. CASE STUDY: SKIPPING THE OS BOOT IN AN OEM COMPANY’S SOC VALIDATION ENVIRONMENT For this case study, the SoC validation environment instantiates a SoC containing a CPU, Memory Subsystem, Switching Fabric, and peripherals. The SoC is modeled in a hybrid environment whereby part of the SoC is modeled on the workstation and part of the SoC is modeled in the emulator, and the Switching Fabric is the bridge between the models. Many of the SoC validation use cases require firstly booting an operating system (OS) in order to run the applications that are being validated. The boot of the OS takes on the order of hours to days. Often the applications that are being validated execute in a fraction of the time that it takes to boot the OS. This means that only a fraction of the emulation time was used to validate the application and the larger portion of the time is spent booting the OS in order to be able to run the application. Thus the overall boot time has an important impact not only on how many applications can be run per user per day, but also, from an emulation efficiency standpoint, how much emulation time is spent just getting the OS booted versus running the applications that are used to validate the SoC and/or the application. The ideal scenario would be to eliminate the time it takes to boot the OS. A solution close to the ideal scenario is to checkpoint the SoC validation environment after the boot of the OS. This OS boot can then be delivered as part of a checkpoint image that is bundled with the rest of the SoC validation environment. The checkpoint of the hardware can be taken by technology native to the emulator. The complexity is in the checkpointing of the part of the SoC validation environment that is executing on the workstation. There were two early attempts to checkpoint, before settling on DMTCP as the preferred solution. A first attempt at checkpointing, prior to the use of DMTCP, was to capture all of the stimulus from the hardware during the OS boot along with a checkpoint of the hardware, and then to restore the OS boot using the stimulus. This stimulus was replayed into the software test bench to re-establish the state of the software, and then finally restore the state of the emulator using the hardware checkpoint. This method worked, except it had two notable drawbacks: (1) Depending on how much stimulus needed to be captured, the size of the replay database could become quite large. (2) The time it took the software testbench to execute, controlled the time it took to perform the restoration of the software testbench. A second attempt at checkpointing prior to employing DMTCP was to leverage the Boost C++ libraries to make each of the software components of the SoC validation environment checkpoint-able. This method worked except that it had a major drawback in that each SoC validation software component had to be developed with checkpointing in mind and if there was just one component that didn¹t support checkpointing or did the checkpointing incorrectly, the SoC validation environment was not checkpoint-able. Hence, the ideal solution would be to transparently checkpoint the software in the same way as we checkpoint the hardware. The DMTCP-based approach was able to overcome the limitations of these first two attempts by transparently checkpointing the software on the workstation, which includes the part of the SoC modeled on the workstation, the software testbench, and the emulation software. The checkpoint is taken without concern for how the software has been modeled and also without concern for the speed at which the software testbench executes. This resulted in making an “OS boot” checkpoint available to the users along with the SoC verification environment. This allows those users to restore the checkpoint in less than 5 minutes, and to then to run their applications for SoC validation in an environment after the OS boot. Now, users requiring this use case can focus their emulation time on running their application, and skipping the lengthy OS boot time. The DMTCP-based approach has accelerated the time to run an application by close to a factor of two. Additionally, it has freed up the emulation time that would have been spent in “OS boot”, thus saving many hours of emulation time. In addition, the DMTCP approach also saves the state of the emulation runtime software itself. This feature provides added value, as compared to the two earlier checkpointing approaches. The checkpoint-restore flow can now set up to address additional aspects of the emulation runtime environment, such as triggers, which are important for debugging in the case of an exception. This enables the designer to create an environment closer to a “turnkey solution”, in which the end user no longer has to remember to load the trigger prior to starting the run of their application. First use case of DMTCP at an OEM company was on a regression suite consisting of 40 jobs. The 40 jobs had a similar profile in that they required around 1 hour to boot the OS and then 1 hour of execution of test content. DMTCP was used to create a checkpoint just after the boot of the OS. This checkpoint then becomes part of the collateral of that database. Any future user of that database and software configuration can restore the checkpoint rather than re-running the boot of the OS. The restoration of database using the checkpoint takes around 5 minutes. That is a time savings of 55 minutes per job for this regression. As a reminder, this environment is a hybrid environment and pending the configuration of whether the CPU is in software or RTL and also the type of OS can greatly impact the ""OS boot"" time from an hour to days. For this regression the 40 jobs were able to run in around 22 hours versus previously they would have taken 40 hour. Job throughput was increased close to 2x. This results not only in a substantial time savings but also a substantial cost savings. Figure 9. Regression Suite Emulation Time Comparison Figure 9 is highlighting the changes introduced by adding DMTCP into this use case; the following points are related to the numbers in the figure 9. 1) Checkpoint is created for OS boot as part of preparing the database for users. 2) Restoring OS boot from checkpoint saves 55 minutes for each job. 3) First job is completed after 1.1 hours with DMTCP versus 2 hours without DMTCP. 4) Full regression is completed after 22.9 hours with DMTCP versus 40 hours without DMTCP. During the integration of DMTCP with the emulation and SoC validation environment, we faced several challenges: (1) Very large read-only files were being saved as part of the checkpoint. This had an impact on both checkpoint/restore time as well as the memory footprint on disk. This was solved by having the DMTCP Emulation plugin detect and decide not to checkpoint such read-only files. The tool-specific file list was written into the DMTCP plugin to identify just those files that needed to be checkpointed. (2) Checkpoints were not portable to another site, due to some file paths that were preserved as part of the checkpoint. This was solved by using the existing file path virtualization plugin of DMTCP, allowing these paths to be changed at restore time. Limitations found when deploying DMTCP 1) Some use cases have a remote process connected via TCP on Windows machine. Additional work would be required to support this kind of use case both within the DMTCP community as well as in the testbench infrastructure. 2) Some use cases require changing the initial state of the environment which is part of the DMTCP checkpoint. For instance, if you had a SoC that had a programmable number of DIMMs and the number of DIMMs was changing per test you wouldn't be able to re-use a single checkpoint. DMTCP was found to be easy to integrate, and it required minimal changes to emulation environment. This has demonstrated the success of this approach toward OS-based use case validation during emulation. VI. FUTURE WORK We intend to work on developing preemptive capabilities for emulation jobs in a workload management system such as LSF. A case study in this environment is needed in order to discover the practical challenges thereof, and to help deploy this technology for more flexible job scheduling management. Finally, verification of a network switch presents an additional interesting case study for the future. In this scenario, a virtual machine is often required, so that simulated Ethernet traffic can be injected by the virtual machine into the environment for verification test coverage. This makes transparent checkpointing on the testbench side more difficult, since even though the virtual machine can be modified to inject Ethernet traffic, the virtual machine snapshot facility is difficult to modify, and so the state of the Ethernet traffic generator will not be checkpointed. On restore, some steps in the state diagram for Ethernet packets may be lost. As part of future work, it is proposed to use the ability of DMTCP to checkpoint a virtual machine from the outside. DMTCP has the ability to carry out a snapshot from the outside for the case of a QEMU virtual machine over KVM [4]. That same work [4] also presents DMTCP's ability to checkpoint a network of virtual machines. This latter case makes possible verification for end-to-end Ethernet test coverage between two virtual machines. VII. CONCLUSION This paper describes the approach to transparently checkpoint/restore emulation jobs and various key benefits it brings along with it. This integration was successfully tried in an OEM company’s SOC validation environment, which not only reduced total regression time but also increased Emulator efficiency. Emulation time is precious and any saving of this time directly affects one’s total verification cost. Experimental results have shown that this approach, when integrated with job management system, has increased the emulator utilization and has also increased the productivity of the verification engineers by providing them with a window to look back in time. The integration has also been successfully tried for a variety of software testbenches including C/C++/SystemC, and a SystemVerilog testbench running on a simulator. These testbenches can have multiple threads or multiple processes spread across different machines."
Systems and Networking,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","November 1st, 2016",Recent Developments in Geant4,http://www.ccs.neu.edu/home/gene/papers/geant4-16.pdf," ""Recent Developments in Geant4"", J. Allison et al. (99 co-authors in total, including G. Cooperman), <em>Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</em> <b>835</b>, pp. 186--225, Nov 1, 2016","classes for both detector sensitivity and hit had thus far been provided in the toolkit. A user was therefore required to have the expertise ne- cessary to implement the details of how hits were deﬁned, col- lected and stored. To relieve many users of this burden, concrete primitive scorers of physics quantities such as dose and ﬂux have been provided which cover general-use simulations. Flexibly designed base classes allow users to implement their own primitive scorers for use anywhere a sensitive detector needs to be simulated. Primitive scorers were built upon three classes, G4Multi- FunctionalDetector, G4VPrimitiveScorer and G4VSDFil- ter. G4MultiFunctionalDetector is a concrete class derived from G4VSensitiveDetector and attached to the detector component. Primitive scorers were developed on top of the base class G4VPrimitiveScorer, and as such represent classes to be registered to the G4MultiFunctionalDetector. G4VSDFilter is an abstract class for a track ﬁlter to be associated with a G4MultiFunctionalDetector or a primitive scorer. Concrete track ﬁlter classes are also provided. One example is a charged track ﬁlter and a particle ﬁlter that accept for scoring only charged tracks and a given particle species, respectively. A primitive scorer creates a G4THitsMap object for storing one physics quantity for an event. G4THitsMap is a template class for mapping an integer key to a pointer value. Since a physics quantity such as dose is generally accumulated in each cell of a detector component during an event or run, a primitive scorer generates a < > G4THitsMap G4double object that maps a pointer to a G4double for a physics quantity, and uses the cell number as the integer key. If a cell has no value, the G4THitsMap object has no corresponding entry and the pointer to the physics quantity returns a null. This was done to reduce memory consumption, and to distinguish an unﬁlled cell from one that has a value of zero. The integer key of the cell is taken from the copy number of the G4LogicalVolume of the de- tector component by default. GEANT4 also provides primitive scorers for three-dimensional structured geometry, in which copy numbers are taken at each of the depth levels at which of the logical volumes are nested in the geometric structure. These copy numbers are then serialized into integer keys. 3.conclusion not found"
Systems and Networking,Gene Cooperman,https://www.khoury.northeastern.edu/people/gene-cooperman/,"Gene Cooperman is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is an affiliated faculty member in the College of Engineering.","Fault tolerance and transparent checkpointing; Supercomputing, parallel computing, cloud computing; Formal verification; Cybersecurity","PhD in Applied Mathematics, Brown University; BS in Mathematics and Physics, University of Michigan","May 7th, 2014",Transparent Checkpoint-Restart over InfiniBand,http://www.ccs.neu.edu/home/gene/papers/hpdc14.pdf," ""Transparent Checkpoint-Restart over InfiniBand"", Jiajun Cao, Gregory Kerr, Kapil Arya and Gene Cooperman, ACM Symposium on High Performance Parallel and Distributed Computing (HPDC'14), pp. 13--24, ACM Press, 2014.","Transparently saving the state of the InﬁniBand network as part of distributed checkpointing has been a long-standing challenge for researchers. The lack of a solution has forced typical MPI implementations to include custom checkpoint- restart services that “tear down” the network, checkpoint each node in isolation, and then re-connect the network again. This work presents the ﬁrst example of transpar- ent, system-initiated checkpoint-restart that directly sup- ports InﬁniBand. The new approach simpliﬁes current prac- tice by avoiding the need for a privileged kernel module. The generality of this approach is demonstrated by applying it both to MPI and to Berkeley UPC (Uniﬁed Parallel C), in its native mode (without MPI). Scalability is shown by check- pointing 2,048 MPI processes across 128 nodes (with 16 cores per node). The run-time overhead varies between 0.8% ands (Section 7) are presented. 2. BACKGROUND Section 2.1 reviews some concepts of InﬁniBand, necessary for understanding the checkpointing approach described in Section 3. Section 2.2 describes the use of plugins in DMTCP. 2.1 InﬁniBand Verbs API In order to understand the algorithm, we review some concepts from the Verbs API of InﬁniBand. While there are several"
Systems and Networking,Engin Kirda,https://www.khoury.northeastern.edu/people/engin-kirda/,"Engin Kirda is a professor in the Khoury College of Computer Sciences and the College of Engineering at Northeastern University, based in Boston.","Systems, software, and network security; Web security; Binary analysis; Malware detection","PhD in Computer Science, Technical University of Vienna — Austria; MS in Computer Science, Technical University of Vienna — Austria; BS in Computer Science, Technical University of Vienna — Austria","August 11th, 2016","UNVEIL: A Large-Scale, Automated Approach to Detecting Ransomware",https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_kharraz.pdf," A. Kharraz, S. Arshad, C. Mulliner, W. Robertson, E. Kirda. ""UNVEIL: A Large-Scale, Automated Approach to Detecting Ransomware"". In USENIX Security Symposium Austin, TX US, Aug 2016.","Although the concept of ransomware is not new (s In this paper we presented UNVEIL, a novel approach to detecting and analyzing ransomware. Our system is the first in the literature to specifically identify typical behavior of ransomware such as malicious encryption of files and locking of user desktops. These are behaviors that are difficult for ransomware to hide or change. The evaluation of UNVEIL shows that our approach was able to correctly detect 13,637 ransomware samples from multiple families in a real-world data feed with zero false positives. In fact, UNVEIL outperformed all ex- isting AV scanners and a modern industrial sandboxing technology in detecting both superficial and technically sophisticated ransomware attacks. Among our findings was also a new ransomware family that no security com- pany had previously detected before we submitted it to VirusTotal. 9"
Systems and Networking,Alan Mislove,https://www.khoury.northeastern.edu/people/alan-mislove/,"Alan Mislove is a professor and the senior associate dean for academic affairs in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Network measurement; Networking; Online social networks; Security and privacy,"PhD in Computer Science, Rice University; MS in Computer Science, Rice University; BA in Computer Science, Rice University","November 2nd, 2018",Is the Web Ready for OCSP Must-Staple?,https://mislove.org/publications/OCSP-IMC.pdf," Taejoong Chung, Jay Lok, Balakrishnan Chandrasekaran, David Choffnes, Dave Levin, Bruce M. Maggs, Alan Mislove, John Rula, Nick Sullivan, and Christo Wilson. 2018. Is the Web Ready for OCSP Must-Staple?. In 2018 Internet Measurement Conference (IMC ’18), October 31-November 2, 2018, Boston, MA, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/ 10.1145/3278532.3278543","TLS, the de facto standard protocol for securing communications over the Internet, relies on a hierarchy of certificates that bind names to public keys. Naturally, ensuring that the communicating parties are using only valid certificates is a necessary first step in or- der to benefit from the security of TLS. To this end, most certificates and clients support OCSP, a protocol for querying a certificate’s revocation status and confirming that it is still valid. Unfortunately, however, OCSP has been criticized for its slow performance, unre- liability, soft-failures, and privacy issues. To address these issues, the OCSP Must-Staple certificate extension was introduced, which requires web servers to provide OCSP responses to clients during the TLS handshake, making revocation checks low-cost for clients. Whether all of the players in the web’s PKI are ready to support OCSP Must-Staple, however, remains still an open question. In this paper, we take a broad look at the web’s PKI and deter- mine if all components involved—namely, certificate authorities, web server administrators, and web browsers—are ready to sup- port OCSP Must-Staple. We find that each component does not yet fully support OCSP Must-Staple: OCSP responders are still not fully reliable, and most major web browsers and web server implemen- tations do not fully support OCSP Must-Staple. On the bright side, only a few players need to take action to make it possible for web server administrators to begin relying on certificates with OCSP ∗This work was done while the author was a postdoctoral re- searcher at Northeastern University. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. IMC ’18, October 31-November 2, 2018, Boston, MA, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5619-0/18/10...$15.00 https://doconclusion not found"
Systems and Networking,Alan Mislove,https://www.khoury.northeastern.edu/people/alan-mislove/,"Alan Mislove is a professor and the senior associate dean for academic affairs in the Khoury College of Computer Sciences at Northeastern University, based in Boston.",Network measurement; Networking; Online social networks; Security and privacy,"PhD in Computer Science, Rice University; MS in Computer Science, Rice University; BA in Computer Science, Rice University","May 24th, 2018",Privacy Risks with Facebook’s PII-based Targeting: Auditing a Data Broker’s Advertising Interface,https://mislove.org/publications/PII-Oakland.pdf," G. Venkatadri et al., ""Privacy Risks with Facebook's PII-Based Targeting: Auditing a Data Broker's Advertising Interface,"" 2018 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, 2018, pp. 89-107.","—Sites like Facebook and Google now serve as de facto data brokers, aggregating data on users for the purpose of implementing powerful advertising platforms. Historically, these services allowed advertisers to select which users see their ads via targeting attributes. Recently, most advertising platforms have begun allowing advertisers to target users directly by uploading the personal information of the users who they wish to advertise to (e.g., their names, email addresses, phone numbers, etc.); these services are often known as custom audiences. Custom audiences effectively represent powerful linking mechanisms, allowing advertisers to leverage any PII (e.g., from customer data, public records, etc.) to target users. In this paper, we focus on Facebook’s custom audience implementation and demonstrate attacks that allow an adversary to exploit the interface to infer users’ PII as well as to infer their activity. Speciﬁcally, we show how the adversary can infer users’ full phone numbers knowing just their email address, determine whether a particular user visited a website, and de-anonymize all the visitors to a website by inferring their phone numbers en masse. These attacks can be conducted without any interaction with the victim(s), cannot be detected by the victim(s), and do not require the adversary to spend money or actually place an ad. We propose a simple and effective ﬁx to the attacks based on reworking the way Facebook de-duplicates uploaded information. Facebook’s security team acknowledged the vulnerability and has put into place a ﬁx that is a variant of the ﬁx we propose. Overall, our results indicate that advertising platforms need to carefully consider the privacy implications of their interfaces.The vast amounts of user data that social networking services have collected is now utilized by their advertising platforms to allow advertisers to target users via their PII. In this paper, we have shown how the inclusion of PII-based targeting opens up new privacy leaks in advertising platforms. By giving advertisers ﬁne-grained control over the set of users targeted, and by providing them with coarse-grained statistics of audience sizes, the platforms open themselves to powerful attacks that can let an adversary learn private information about users. While we have proposed a solution to the attacks we uncovered, our work shows that platforms need to carefully audit their interfaces when introducing PII-based targeting."
Systems and Networking,Cristina Nita-Rotaru,https://www.khoury.northeastern.edu/people/cristina-nita-rotaru/,"Cristina Nita-Rotaru is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. She is a founding member of Northeastern's Cybersecurity and Privacy Institute.",Distributed systems and network security; Insider-resilient systems; Analytics for security and fault tolerance; Automated testing and verification,"PhD, Johns Hopkins University; MS, Politechnica University of Bucharest — Romania; BS, Politechnica University of Bucharest —Romania","November 15th, 2017",Taking a Long Look at QUIC: An Approach for Rigorous Evaluation of Rapidly Evolving Transport Protocols,https://www.sjero.net/pubs/2017_IMC_QUIC.pdf," Arash Molavi Kakhki, Samuel Jero, David Choffnes, Alan Mislove, and Cristina Nita-Rotaru In Proceedings of ACM Internet Measurement Conference (IMC'17), London, United Kingdom, Nov 2017.","Google’s QUIC protocol, which implements TCP-like properties at the application layer atop a UDP transport, is now used by the vast majority of Chrome clients accessing Google properties but has no formal state machine specification, limited analysis, and ad-hoc evaluations based on snapshots of the protocol implementation in a small number of environments. Further frustrating attempts to evaluate QUIC is the fact that the protocol is under rapid develop- ment, with extensive rewriting of the protocol occurring over the scale of months, making individual studies of the protocol obsolete before publication. Given this unique scenario, there is a need for alternative tech- niques for understanding and evaluating QUIC when compared with previous transport-layer protocols. First, we develop an ap- proach that allows us to conduct analysis across multiple versions of QUIC to understand how code changes impact protocol effec- tiveness. Next, we instrument the source code to infer QUIC’s state machine from execution traces. With this model, we run QUIC in a large number of environments that include desktop and mobile, wired and wireless environments and use the state machine to understand differences in transport- and application-layer perfor- mance across multiple versions of QUIC and in different environ- ments. QUIC generally outperforms TCP, but we also identified performance issues related to window sizes, re-ordered packets, and multiplexing large number of small objects; further, we identify that QUIC’s performance diminishes on mobile devices and over cellular networks. ACM Reference Format: Arash Molavi Kakhki, Samuel Jero, David Choffnes, Cristina Nita-Rotaru, and Alan Mislove. 2017. Taking a Long Look at QUIC. In Proceedings of IMC ’17, London, United Kingdom, November 1–3, 2017, 14 pages. https://dos. For example, consider the case of Google App Engine (GAE), which supports QUIC and allows us to host our own content for testing. While the latency to GAE frontends was low and constant over time, we found a variable wait time between connection estab- lishment and content being served (Fig. 2, middle bar). We do not know the origins for these variable delays, but we suspect that the constant RTT is due to proxying at the frontend, and the variable delay component is due to GAE’s shared environment without re- source guarantees. The variability was present regardless of time of day, and did not improve when requesting the same content sequentially (thus making it unlikely that the GAE instance was spun down for inactivity). Such variable delay can dominate PLT measurements for small web pages, and cannot reliably be isolated when multiplexing requests. To avoid these issues, we opted instead to run our own QUIC servers. This raises the question of how to configure QUIC pa- rameters to match those used in deployment. We use a two-phase approach. First, we extract all parameters that are exchanged be- tween client and server (e.g., window sizes) and ensure that our QUIC server uses the same ones observed from Google. For parameters not exposed by the QUIC server to the client, we use grey-box testing to infer the likely parameters being used. Specifically, we vary server-side parameters until we obtain perfor- mance that matches QUIC from Google servers. The end result is shown in Fig. 2. The left bar shows that QUIC as configured in the public code release takes twice as long to download a large file when compared to the configuration that most closely matches Google’s QUIC performance (right bar)10. We made two changes to achieve parity with Google’s QUIC servers. First, we increased the maximum allowed congestion win- dow size. At the time of our experiments, this value was 107 by default in Chrome. We increased this value to 430, which matched the maximum allowed congestion window in Chromium’s devel- opment channel. Second, we found and fixed a bug in QUIC that prevented the slow start threshold from being updated using the 10We focus on PLT because it is the metric we use for end-to-end performance comparisons through- out the paper. State Description Init Initial connection establishment Slow Start Slow start phase Congestion Avoidance (CA) Normal congestion avoidance CA-Maxed Max allowed win. size is reached Application Limited Current cong. win. is not being uti- lized, hence window will not be increased Retransmission Timeout Loss detected due to timeout for ACK Recovery Proportional rate reduction fast re- covery Tail Loss Probe [22] Recover tail losses Table 3: QUIC states (Cubic CC) and their meanings. receiver-advertised buffer size. Failure to do so caused poor perfor- mance due to early exit from slow start.11 Prior work did no such calibration. This explains why they ob- served poor QUIC performance in high bandwidth environments or when downloading large web pages [16, 20, 30]. 4.2 Instrumentation While our tests can tell us how QUIC and TCP compare to each other under different circumstances, it is not clear what exactly causes these differences in performance. To shed light on this, we compile QUIC clients and servers from source (using QUIC versions 25 and 34) and instrument them to gain access to the inner workings of the protocol. QUIC implements TCP-like congestion control. To reason about QUIC’s behavior, we instrumented our QUIC server to collect logs that allow us to infer QUIC’s state machine from execution traces, and to track congestion window size and packet loss detection. Table 3 lists QUIC’s congestion control states. We use statistics about state transitions and the frequency of visiting each state to understand the root causes behind good or bad performance for QUIC. For example, we found that the reason QUIC’s performance suffers in the face of packet re-ordering is that re-ordered packets cause QUIC’s loss-detection mechanism to report high numbers of false losses. Note that we evaluate QUIC as a whole, in lieu of isolating the im- pact of protocol components (e.g., congestion avoidance, TLP, etc.). We found that disentangling and changing other (non-modular) parts of QUIC (e.g., to change loss recovery techniques, add HOL blocking, change how packets are ACKed) requires rewriting sub- stantial amount of code, and it is not always clear how to replace them. This is an interesting topic to explore in future work. 5 ANALYSIS In this section, we conduct extensive measurements and analysis to understand and explain QUIC performance. We begin by focus- ing on the protocol-layer behavior, QUIC’s state machine, and its fairness to TCP. We then evaluate QUIC’s application-layer per- formance, using both page load times (PLT) and video streaming as example application metrics. Finally, we examine the evolution 11We confirmed our changes with a member of the QUIC team at Google. He also confirmed our bug report. Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom (a) QUIC’s Cubic CC (b) QUIC’s BBR CC Figure 3: State transition diagram for QUIC’s CC. 0 1 2 3 4 5 0 20 40 60 80 100 Throughput (Mbps) Time (s) QUIC TCP (a) QUIC vs. TCP 0 1 2 3 4 5 0 20 40 60 80 100 Throughput (Mbps) Time (s) QUIC TCP1 TCP2 (b) QUIC vs. two TCP flows Figure 4: Timeline showing unfairness between QUIC and TCP when transferring data over the same 5 Mbps bottle- neck link (RTT=36ms, buffer=30 KB). of QUIC’s performance and evaluate the performance that QUIC “leaves on the table” by encrypting transport-layer headers that prevent transparent proxying commonly used in cellular (and other high-delay) networks. 5.1 State Machine and Fairness In this section, we analyze high-level properties of the QUIC proto- col using our framework. State machine. QUIC has only a draft formal specification and no state machine diagram or formal model; however, the source code is made publicly available. Absent such a model, we took an empirical approach and used traces of QUIC execution to infer the state machine to better understand the dynamics of QUIC and their impact on performance. Specifically, we use Synoptic [15] for automatic generation of QUIC state machine. While static analysis might generate a more complete state machine, a complete model is not necessary for understanding performance changes. Rather, as we show in Sec- tion 5.2, we only need to investigate the states visited and transitions between them at runtime. Scenario Flow Avg. throughput (std. dev.) QUIC vs. TCP QUIC 2.71 (0.46) TCP 1.62 (1.27) QUIC vs. TCPx2 QUIC 2.8 (1.16) TCP 1 0.7 (0.21) TCP 2 0.96 (0.3) QUIC vs. TCPx4 QUIC 2.75 (1.2) TCP 1 0.45 (0.14) TCP 2 0.36 (0.09) TCP 3 0.41 (0.11) TCP 4 0.45 (0.13) Table 4: Average throughput (5 Mbps link, buffer=30 KB, av- eraged over 10 runs) allocated to QUIC and TCP flows when competing with each other. Despite the fact that both pro- tocols use Cubic congestion control, QUIC consumes nearly twice the bottleneck bandwidth than TCP flows combined, resulting in substantial unfairness. Fig. 3a shows the QUIC state machine automatically generated using traces from executing QUIC across all of our experiment configurations. The diagram reveals behaviors that are common to standard TCP implementations, such as connection start (Init, SlowStart), congestion avoidance (CongestionAvoidance), and receiver-limited connections (ApplicationLimited). QUIC also includes states that are non-standard, such as a maximum sending rate (CongestionAvoidanceMaxed), tail loss probes, and propor- tional rate reduction during recovery. Note that capturing the empirical state machine requires in- strumenting QUIC’s source code with log messages that capture transitions between states. In total, this required adding 23 lines of code in 5 files. While the initial instrumentation required approxi- mately 10 hours, applying the instrumentation to subsequent QUIC versions required only about 30 minutes. To further demonstrate how our approach applies to other congestion control implementa- tions, we instrumented QUIC’s experimental BBR implementation and present its state transition diagram in Fig. 3b. This instrumen- tation took approximately 5 hours. Thus, our experience shows that our approach is able to adapt to evolving protocol versions and implementations with low additional effort. IMC ’17, November 1–3, 2017, London, United Kingdom A. Molavi Kakhki et al. 0 20 40 60 80 0 10 20 30 40 50 60 70 80 90 100 Cong. Win. (KB) QUIC TCP (a) QUIC vs. TCP 0 20 40 60 80 20 21 22 23 24 25 Cong. Win. (KB) Time (s) (b) 5-second zoom of above figure Figure 5: Timeline showing congestion window sizes for QUIC and TCP when transferring data over the same 5 Mbps bottleneck link (RTT=36ms, buffer=30 KB). We used inferred state machines for root cause analysis of per- formance issues. In later sections, we demonstrate how they helped us understand QUIC’s poor performance on mobile devices and in the presence of deep packet reordering. Fairness. An essential property of transport-layer protocols is that they do not consume more than their fair share of bottleneck bandwidth resources. Absent this property, an unfair protocol may cause performance degradation for competing flows. We evaluated whether this is the case for the following scenarios, and present aggregate results over 10 runs in Table 4. We expect that QUIC and TCP should be relatively fair to each other because they both use the Cubic congestion control protocol. However, we find this is not the case at all. • QUIC vs. QUIC. We find that two QUIC flows are fair to each other. We also found similar behavior for two TCP flows. • QUIC vs. TCP. QUIC multiplexes requests over a single con- nection, so its designers attempted to set Cubic congestion control parameters so that one QUIC connection emulates N TCP connections (with a default of N = 2 in QUIC 34, and N = 1 in QUIC 37). We found that N had little impact on fair- ness. As Fig. 4a shows, QUIC is unfair to TCP as predicted, and consumes approximately twice the bottleneck bandwidth of TCP even with N = 1. We repeated these tests using different buffer sizes, including those used by Carlucci et al. [17], but did not observe any significant effect on fairness. This directly contradicts their finding that larger buffer sizes allow TCP and QUIC to fairly share available bandwidth. • QUIC vs. multiple TCP connections. When competing with M TCP connections, one QUIC flow should consume 2/(M + 1) of the bottleneck bandwidth. However, as shown in Table 4 and Fig. 4b, QUIC still consumes more than 50% of the bottleneck bandwidth even with 2 and 4 competing TCP flows. Thus, QUIC is not fair to TCP even assuming 2-connection emulation. (a) Varying object size (b) Varying object count Figure 6: QUIC (version 34) vs. TCP with different rate lim- its for (a) different object sizes and (b) with different num- bers of objects. Each heatmap shows the percent difference between QUIC over TCP. Positive numbers—colored red— mean QUIC outperforms TCP and has smaller page-load time. Negative numbers—colored blue—means the opposite. White cells indicate no statistically significant difference. Figure 7: QUIC with and without 0-RTT. Positive numbers— colored red—show the performance gain achieved by 0-RTT. The gain is more significant for small objects, but becomes insignificant as the bandwidth decreases and/or objects be- come larger, where connection establishment is a tiny frac- tion of total PLT. To ensure fairness results were not an artifact of our testbed, we repeated these tests against Google servers. The unfairness results were similar. We further investigate why QUIC is unfair to TCP by instrument- ing the QUIC source code, and using tcpprobe [13] for TCP, to extract the congestion window sizes. Fig. 5a shows the congestion window over time for the two protocols. When competing with TCP, QUIC is able to achieve a larger congestion window. Taking a closer look at the congestion window changes (Fig. 5b), we find that while both protocols use Cubic congestion control scheme, QUIC increases its window more aggressively (both in terms of slope, and in terms of more frequent window size increases). As a result, QUIC is able to grab available bandwidth faster than TCP does, leaving TCP unable to acquire its fair share of the bandwidth. 5.2 Page Load Time This section evaluates QUIC performance compared to TCP for loading web pages (i.e., page load time, or PLT) with different sizes and numbers of objects. Recall from Sec. 3 that we measure PLT using information gathered from Chrome, that we run TCP and QUIC experiments back-to-back, and that we conduct experiments in a variety of emulated network settings. Note that our servers add all necessary HTTP directives to avoid caching content. We also clear the browser cache and close all sockets between experi- ments to prevent “warmed up” connections from impacting results. Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom (a) Varying object size,1% Loss (b) Varying object size, 112 ms RTT (c) Varying object size, 112 ms RTT with 10ms jitter that causes packet reordering (d) Varying #object, 1% Loss (e) Varying #object, 112 ms RTT (f) Varying #object, 112 ms RTT with 10ms jitter that causes packet reordering Figure 8: QUIC v34 vs. TCP at different rate limits, loss, and delay for different object sizes (a, b, and c) and different numbers of objects (d, e, and f). However, we do not clear the state used for QUIC’s 0-RTT con- nection establishment. Furthermore, our PLTs do not include any DNS lookups. This is achieved by extracting resource loading time details from Chrome and excluding the DNS lookups times. In the results that follow, we evaluate whether the observed performance differences are statistically significant or simply due to noise in the environment. We use the Welch’s t-test [14], a two- sample location test which is used to test the hypothesis that two populations have equal means. For each scenario, we calculate the p-value according to the Welch’s t-test. If the p-value is smaller than our threshold (0.01), then we reject the null hypothesis that the mean performance for TCP and QUIC are identical, implying the difference we observe between the two protocols is statistically significant. Otherwise the difference we observe is not significant and is likely due to noise. Desktop environment. We begin with the desktop environ- ment and compare QUIC with TCP performance for different rates, object sizes, and object counts—without adding extra delay or loss (RTT = 36ms and loss = 0%). Fig. 6 shows the results as a heatmap, where the color of each cell corresponds to the percent PLT dif- ference between QUIC and TCP for a given bandwidth (vertical dimension) and object size/number (horizontal direction). Red indi- cates that QUIC is faster (smaller PLT), blue indicates that TCP is faster, and white indicates statistically insignificant differences. Our key findings are that QUIC outperforms TCP in every sce- nario except in the case of large numbers of small objects. QUIC’s performance gain for smaller object sizes is mainly due to QUIC’s 0-RTT connection establishment—substantially reducing delays related to secure connection establishment that corresponds to a substantial portion of total transfer time in these cases. To isolate the impact of 0-RTT, we plotted performance differences between QUIC with and without 0-RTT enabled in Fig. 7. As expected, the benefit is relatively large for small objects and statistically insignif- icant for 10MB objects. To investigate the reason why QUIC performs poorly for large numbers of small objects, we explored different values for QUIC’s Maximum Streams Per Connection (MSPC) parameter to control the level of multiplexing (the default is 100 streams). We found there was no statistically significant impact for doing so, except when setting the MSPC value to a very low number (e.g., 1), which worsens performance substantially. Instead, we focused on QUIC’s congestion control algorithm and identified that in such cases, QUIC’s Hybrid Slow Start [24] causes early exit from Slow Start due to an increase in the minimum observed RTT by the sender, which Hybrid Slow Start uses as an indication that the path is getting congested. This can hurt the PLT significantly when objects are small and the total transfer time is not long enough for the congestion window to increase to its maximum value. Note that the same issue (early exit from Hybrid Slow Start) affects the scenario with a large number of large objects, but QUIC nonetheless outperforms TCP because it has enough time to increase its congestion window and remain at high utilization, thus compensating for exiting Slow Start early.12 Desktop with added delay and loss. We repeat the experi- ments in the previous section, this time adding loss, delay, and jitter. Fig. 8 shows the results, again using heatmaps. Our key observations are that QUIC outperforms TCP under loss (due to better loss recovery and lack of HOL blocking), and in high- delay environments (due to 0-RTT connection setup). However, in the case of high latency, this is not enough to compensate for QUIC’s poor performance for large numbers of small objects. Fig. 9 shows the congestion window over time for the two protocols at 100Mbps and 1% loss. Similar to Fig. 5, under the same network conditions QUIC better recovers from loss events and adjusts its congestion window faster than TCP, resulting in a larger congestion window on average and thus better performance. Under variable delays, QUIC performs significantly worse than TCP. Using our state machine approach, we observed that under variable delay QUIC spends significantly more time in the recovery state compared to relatively stable delay scenarios. To investigate 12We leave investigating the reason behind sudden increase in the minimum observed RTT when multiplexing many objects to future work. IMC ’17, November 1–3, 2017, London, United Kingdom A. Molavi Kakhki et al. 10 30 50 70 Cong. Win. (KB) QUIC 10 30 50 70 1 2 3 4 5 6 7 8 9 10 Cong. Win. (KB) Time (s) TCP Figure 9: Congestion window over time for QUIC and TCP at 100Mbps rate limit and 1% loss. Figure 10: QUIC vs. TCP when downloading a 10MB page (112 ms RTT with 10ms jitter that causes packet reordering). Increasing the NACK threshold for fast retransmit allows QUIC to cope with packet reordering. this, we instrumented QUIC’s loss detection mechanism, and our analysis reveals that variable delays cause QUIC to incorrectly infer packet loss when jitter leads to out-of-order packet delivery. This occurs in our testbed because netem adds jitter by assigning a delay to each packet, then queues each packet based on the adjusted send time, not the packet arrival time—thus causing packet re-ordering. The reason that QUIC cannot cope with packet re-ordering is that it uses a fixed threshold for number of NACKs (default 3) before it determines that a packet is lost and responds with a fast retransmit. Packets reordered deeper than this threshold cause false positive loss detection.13 In contrast, TCP uses the DSACK algorithm [41] to detect packet re-ordering and adapt its NACK threshold accordingly. As we will show later in this section, packet reordering occurs in the cellular networks we tested, so in such cases QUIC will benefit from integrating DSACK. We quantify the impact of using larger DSACK values in Fig. 10, demonstrating that in the presence of packet reordering larger NACK thresholds substantially improve end to end performance compared to smaller NACK thresholds. We shared this result with a QUIC engineer, who subsequently informed us that the QUIC team is experimenting with dynamic threshold and time-based solutions to avoid falsely inferring loss in the presence of reordering. Desktop with variable bandwidth. The previous tests set a static threshold for the available bandwidth. However, in practice 13Note that reordering impact when testing small objects is insignificant because QUIC does not falsely detect losses until a sufficient number of packets are exchanged. 0 40 80 120 0 50 100 150 200 Throughput (Mbps) Time (s) TCP QUIC Figure 11: QUIC vs. TCP when downloading a 210MB object. Bandwidth fluctuates between 50 and 150Mbps (randomly picks a rate in that range every one second). Averaging over 10 runs, QUIC is able to achieve an average throughput of 79Mbps (STD=31) while TCP achieves an average through- put of 46Mbps (STD=12). such values will fluctuate over time, particularly in wireless net- works. To investigate how QUIC and TCP compare in environments with variable bandwidth, we configured our testbed to change the bandwidth randomly within specified ranges and with different frequencies. Fig. 11 shows the throughput over time for three back-to-back TCP and QUIC downloads of a 210MB object when the bandwidth randomly fluctuates between 50 and 150Mbps. As shown in this figure, QUIC is more responsive to bandwidth changes and is able to achieve a higher average throughput compared to TCP. We re- peated this experiment with different bandwidth ranges and change frequencies and observed the same behavior in all cases. Mobile environment. Due to QUIC’s implementation in userspace (as opposed to TCP’s implementation in the OS kernel), resource contention might negatively impact performance indepen- dent of the protocol’s optimizations for transport efficiency. To test whether this is a concern in practice, we evaluated an increasingly common resource-constrained deployment environment: smart- phones. We use the same approach as in the desktop environment, controlling Chrome (with QUIC enabled) over two popular Android phones: the Nexus 6 and the MotoG. These phones are neither top- of-the-line, nor low-end consumer phones, and we expect that they approximate the scenario of a moderately powerful mobile device. Fig. 12 shows heatmaps for the two devices when varying band- width and object size.14 We find that, similar to the desktop envi- ronment, in mobile QUIC outperforms TCP in most cases; however, its advantages diminish across the board. To understand why this is the case, we investigate the QUIC congestion control states visited most in mobile and non-mobile scenarios under the same network conditions. We find that in mo- bile QUIC spends most of its time (58%) in the “Application Limited” state, which contrasts substantially with the desktop scenario (only 7% of the time). The reason for this behavior is that QUIC runs in a userspace process, whereas TCP runs in the kernel. As a result, QUIC is unable to consume received packets as quickly as on a desk- top, leading to suboptimal performance, particularly when there is ample bandwidth available.15 Fig. 13 shows the full state diagram (based on server logs) in both environments for 50Mbps with no added latency or loss. By revealing the changes in time spent in 14We omit 100 Mbps because our phones cannot achieve rates beyond 50 Mbps over WiFi, and we omit results from varying the number of objects because they are similar to the single-object cases. 15A parallel study from Google [26] using aggregate data identifies the same performance issue but does not provide root cause analysis. Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom (a) MotoG, No added loss or latency (b) MotoG, 1% Loss (c) MotoG, 112 ms RTT (d) Nexus6, No added loss or latency (e) Nexus6, 1% Loss (f) Nexus6, 112 ms RTT Figure 12: QUICv34 vs. TCP for varying object sizes on MotoG and Nexus6 smartphones (using WiFi). We find that QUIC’s improvements diminish or disappear entirely when running on mobile devices. (a) MotoG (b) Desktop Figure 13: QUIC state transitions on MotoG vs. Desktop. QUICv34, 50 Mbps, no added loss or delay. Red numbers in- dicate the fraction of time spent in each state, and black numbers indicate the state-transition probability. The fig- ure shows that poor performance for QUIC on mobile de- vices can be attributed to applications not processing pack- ets quickly enough. Note that the zero transition probabili- ties are due to rounding down. each state, such inferred state machines help diagnose problems and develop a better understanding of QUIC dynamics. Tests on commercial cellular networks. We repeated our PLT tests—without any network emulation—over Sprint’s and Veri- zon’s cellular networks, using both 3G and LTE. Table 5 shows the characteristics of these networks at the time of the experiment. To isolate the impact of the network from that of the device they run on, we used our desktop client tethered to a mobile network instead of using a mobile device (because the latter leads to suboptimal performance for QUIC, shown in Fig. 12 and 13). We otherwise keep the same server and client settings as described in Sec. 3.1. Fig. 14 shows the heatmaps for these tests. For LTE, QUIC per- forms similarly to a desktop environment with low bandwidth (Fig. 7). In these cell networks, the benefit of 0-RTT is larger for the 1MB page due to higher latencies in the cellular environment. (a) Varying object size (b) Varying object count Figure 14: QUICv34 vs. TCP over Verizon and Sprint cellular networks. Thrghpt. (Mbps) RTT (STD) (ms) Reordering (%) Loss (%) 3G LTE 3G LTE 3G LTE 3G LTE Verizon 0.17 4.0 109 (20) 62 (14) 9 0.25 0.05 0 Sprint 0.31 2.4 70 (39) 55 (11) 1.38 0.13 0.02 0.02 Table 5: Characteristics of tested cell networks. Throughput and RTT represent averages. In the case of 3G, we see the benefits of QUIC diminish. Com- pared to LTE, the bandwidth in 3G is much lower and the loss is higher—which works to QUIC’s benefit (see Fig. 8a). However, the packet reordering rates are higher compared to LTE, and this works to QUIC’s disadvantage. Note that in 3G scenarios, in many cases QUIC had better performance on average (i.e., lower average PLT); however, the high variance resulted in high p-values, which means we cannot reject the null hypothesis that the two sample sets were drawn form the same (noisy) distribution. 5.3 Video-streaming Performance This section investigates QUIC’s impact on video streaming in the desktop environment. Unlike page load times, which tend to be limited by RTT and multiplexing, video streaming relies on the transport layer to load large objects sufficiently quickly to maintain smooth playback. This exercises a transport-layer’s ability to quickly ramp up and maintain high utilization. We test video-streaming performance using YouTube, which supports both QUIC and TCP. We evaluate the protocols using well known QoE metrics for video such as the time spent waiting for IMC ’17, November 1–3, 2017, London, United Kingdom A. Molavi Kakhki et al. (a) MACW=430 (b) MACW=2000 Figure 15: QUIC (version 37) vs. TCP with different max- imum allowable congestion window (MACW) size. In (a), MACW=430 and QUIC versions 34 and 37 have identical per- formance (see Fig. 6a), In (b), we use the default MACW=2000 for QUIC 37, which results in higher throughput and larger performance gains for large transfers in high bandwidth networks. initial playback, and the number of rebuffering events. For the latter metric, Google reports that, on average users experience 18% fewer re-buffers when watching YouTube videos over QUIC [26]. We developed a tool for automatically streaming a YouTube video and logging quality of experience (QoE) metrics via the API mentioned in Sec. 3.3. The tool opens a one-hour-long YouTube video, selects a specific quality level, lets the video run for 60 sec- onds, and logs the following QoE metrics: time to start the video, video quality, quality changes, re-buffering events, and fraction of video loaded. As we demonstrated in previous work [31], 60 seconds is sufficient to capture QoE differences. We use this tool to stream videos using QUIC and TCP and compare the QoE for the two protocols. Table 6 shows the results for 100 Mbps bandwidth and 1% loss,16 a condition under which QUIC outperforms TCP (Sec. 8). In this environment, at low and medium resolutions we see no significant difference in QoE metrics, but for the highest quality, hd2160, QUIC is able to load a larger fraction of the video in 60 seconds and experience fewer rebuffers per time played, which is consistent with our PLT test results (Sec. 5.2) and with what Google reported. Thus, to refine their observations, we find that QUIC can outperform TCP for video streaming, but this matters only for high resolutions. 5.4 Historical Comparison To understand how QUIC performance has changed over time, we evaluated 10 QUIC versions (25 to 34)17 in our testbed. In order to only capture differences due to QUIC version changes, and not due to different configuration values, we used the same version of Chrome and the same QUIC parameter configuration when testing different QUIC versions. We found that when using the same configuration most QUIC versions in this range yielded nearly identical results, despite sub- stantial changes to the QUIC codebase (including reorganization of code that frustrated our attempts to instrument it). This is corrobo- rated by changelogs [12] that indicate most modifications were to the cryptography logic, QUIC flags, and connection IDs. Based on the relatively stable QUIC performance across recent versions, we expect that our observations about its performance using its current congestion control algorithm are likely to hold in 16We observed similar results for 10 and 50 Mbps under similar loss. 17The stable version of Chrome at the time of analysis (52) did not support QUIC versions earlier than 25. To avoid false inferences from using different Chrome versions and different QUIC versions, we only tested QUIC versions 25 and higher. Client's machine Server Router (running network emulator) Proxy 40ms RTT 40ms RTT 80ms RTT Figure 16: QUIC proxy test setup. The proxy is located mid- way between client and server. the future (except in places where we identified opportunities to improve the protocol). Note that at the time of writing, the recently proposed BBR congestion control algorithm has not been deployed in the “stable” branch and thus we could not evaluate its performance fairly against Cubic in QUIC or TCP. Private communication with a QUIC team member indicated that BBR is “not yet performing as well as Cubic in our deployment tests.” Comparison with QUIC 37. At the time of publication, the latest stable version of Chromium was 60.0.3112.101, which includes QUIC 37 as the latest stable version. To enhance our longitudinal analysis and demonstrate how our approach easily adapts to new versions of QUIC, we instrumented, tested, and compared QUIC 37 with 34 (the one used for experiments through out this paper). We found that the main change in QUIC 37 is that the maximum allowed congestion window (MACW) increased to 2000 (from 430 used in our experiments) in the new versions of Chromium. This al- lows QUIC to achieve much higher throughput compared to version 34, particularly improving performance when compared with TCP for large transfers in high bandwidth networks. Fig. 15 shows the comparison between TCP and QUIC version 37 for various object sizes with MACW of 430 (Fig. 15a) and 2000 (Fig. 15b). When com- paring Fig. 15a and 6a, we find that QUIC versions 34 and 37 have almost identical performance when using the same MACW; this is corroborated by QUIC version changelogs [12]. All our previous findings, e.g., QUIC performance degradation in presence of deep packet reordering, still hold for this new version of QUIC. 5.5 Impact of Proxying We now test the impact of QUIC’s design decisions on in-network performance optimization. Specifically, many high-latency net- works use transparent TCP proxies to reduce end-to-end delays and improve loss recovery [40]. However, due to the fact that QUIC en- crypts not only payloads but also transport headers, such proxying is impossible for in-network devices. We evaluate the extent to which this decision impacts QUIC’s potential performance gains. Specifically, we wrote a QUIC proxy, and co-located it with a TCP proxy so that we could compare the impact of proxying on end-to-end performance (Fig. 16). For these experiments, we consider PLTs as done in previous sections. We present two types of comparison results: QUIC vs. proxied TCP (as this is what one would expect to find in many cellular net- works), and QUIC vs. proxied QUIC (to determine how QUIC would benefit if proxies could transparently terminate QUIC connections). For the former case, we find that QUIC continues to outperform Taking a Long Look at QUIC IMC ’17, November 1–3, 2017, London, United Kingdom Quality Time to Start video loaded Buffer/Play Time† (%) #rebuffers #rebuffers (secs) in 1 min (%) per playing secs QUIC TCP QUIC TCP QUIC TCP QUIC TCP QUIC TCP tiny 0.5 (0.11) 0.5 (0.21) 33.8 (0.01) 33.8 (0.01) 0.9 (0.17) 0.9 (0.34) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) medium 0.9 (1.04) 0.5 (0.13) 17.9 (0.01) 12.9 (0.92) 1.4 (1.62) 0.9 (0.22) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) hd720 0.7 (0.16) 0.7 (0.18) 8.0 (0.27) 4.3 (0.28) 1.1 (0.27) 1.1 (0.27) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) hd2160 5.9 (2.73) 5.9 (2.51) 0.8 (0.05) 0.4 (0.01) 50.2 (3.01) 73.1 (1.91) 6.7 (0.46) 4.9 (0.3) 0.2 (0.01) 0.3 (0.01) Table 6: Mean (std) of QoE metrics for a YouTube video in different qualities, averaged over 10 runs. 100Mbps, 1% loss. QUIC benefits are clear for high qualities. While the absolute number of rebuffers for QUIC is higher for hd2160, it is able to load and play more of the video in a given time compared to TCP, with fewer (about 30%) rebuffers per playing second. †Buffer/Play Time is the time spent while buffering the video divided by the time playing video. (a) No added loss or latency (b) 1% Loss (c) 100ms added RTT Figure 17: QUIC vs. TCP proxied, where red cells indicate that QUIC performs better. (a) No added loss or latency (b) 1% Loss (c) 100ms added RTT Figure 18: QUIC with and without proxy when downloading objects with different sizes. Positive numbers (red cells) mean QUIC performs better connecting to the server directly. TCP in many scenarios, but its benefits diminish or entirely dis- appear compared to unproxied TCP in low loss/latency cases, and when there is 1% loss. In the case of high delay links, QUIC still outperforms TCP (Fig. 17). Thus, proxies can help TCP to recover many of the benefits of QUIC, but primarily in lossy scenarios, and when the proxy is equidistant from the client and server. In the case of implementing a QUIC proxy (Fig. 18), we find that a proxy hurts performance for small object sizes (likely due to inefficiencies and the inability to establish connections via 0- RTT), but performance is better under loss for large objects. Taken together, our initial attempt at a QUIC proxy provides mixed results, and identifying any other potential benefits will require additional tuning of the proxy code. 6 CONCLUSION In this paper, we address the problem of evaluating an application- layer transport protocol that was built without a formal specifica- tion, is rapidly evolving, and is deployed at scale with nonpublic configuration parameters. To do so, we use a methodology and testbed that allows us to conduct controlled experiments in a vari- ety of network conditions, instrument the protocol to reason about its performance, and ensure that our evaluations use settings that approximate those deployed in the wild. We used this approach to evaluate QUIC, and found cases where it performs well and poorly— both in traditional desktop environments but also in mobile and proxy scenarios not previously tested. With the help of an inferred protocol state machine and information about time spent in each state, we explained the performance results we observed. There are a number of open questions we plan to address in future work. First, we will evaluate performance in additional oper- ational networks, particularly in more mobile ones and data centers. Second, we will investigate techniques to improve QUIC’s fairness to TCP while still maintaining high utilization. Third, we will au- tomate the steps used for analysis in our approach and port it to other application layer protocols. This includes adapting our state- machine inference approach to other protocols, and we encourage developers to annotate state transitions in their code to facilitate such analysis. We believe doing so can lead to a more performant, reliable evolution for such network protocols."
Systems and Networking,Cristina Nita-Rotaru,https://www.khoury.northeastern.edu/people/cristina-nita-rotaru/,"Cristina Nita-Rotaru is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. She is a founding member of Northeastern's Cybersecurity and Privacy Institute.",Distributed systems and network security; Insider-resilient systems; Analytics for security and fault tolerance; Automated testing and verification,"PhD, Johns Hopkins University; MS, Politechnica University of Bucharest — Romania; BS, Politechnica University of Bucharest —Romania","August 1st, 2017",Identifier Binding Attacks and Defenses in Software-Defined Networks,https://nds2.ccs.neu.edu/papers/persona_usenix2017.pdf," Samuel Jero, William Koch, Richard Skowyra, Hamed Okhravi, Cristina Nita-Rotaru, David Bigelow. USENIX Security 2017, August 2017",Failed to download
Systems and Networking,Cristina Nita-Rotaru,https://www.khoury.northeastern.edu/people/cristina-nita-rotaru/,"Cristina Nita-Rotaru is a professor in the Khoury College of Computer Sciences at Northeastern University, based in Boston. She is a founding member of Northeastern's Cybersecurity and Privacy Institute.",Distributed systems and network security; Insider-resilient systems; Analytics for security and fault tolerance; Automated testing and verification,"PhD, Johns Hopkins University; MS, Politechnica University of Bucharest — Romania; BS, Politechnica University of Bucharest —Romania","June 26th, 2017",SymCerts: Practical Symbolic Execution For Exposing Noncompliance in X.509 Certificate Validation Implementations,https://www.ieee-security.org/TC/SP2017/papers/231.pdf," Sze Yiu Chau, Omar Chowdhury, Endadul Hoque, Huangyi Ge, Aniket Kate, Cristina Nita-Rotaru, Ninghui Li. IEEE Security and Privacy, May 2017","—The X.509 Public-Key Infrastructure has long been used in the SSL/TLS protocol to achieve authentication. A recent trend of Internet-of-Things (IoT) systems employing small foot- print SSL/TLS libraries for secure communication has further propelled its prominence. The security guarantees provided by X.509 hinge on the assumption that the underlying implementa- tion rigorously scrutinizes X.509 certiﬁcate chains, and accepts only the valid ones. Noncompliant implementations of X.509 can potentially lead to attacks and/or interoperability issues. In the literature, black-box fuzzing has been used to ﬁnd ﬂaws in X.509 validation implementations; fuzzing, however, cannot guarantee coverage and thus severe ﬂaws may remain undetected. To thoroughly analyze X.509 implementations in small footprint SSL/TLS libraries, this paper takes the complementary approach of using symbolic execution. We observe that symbolic execution, a technique proven to be effective in ﬁnding software implementation ﬂaws, can also be leveraged to expose noncompliance in X.509 implementations. Directly applying an off-the-shelf symbolic execution engine on SSL/TLS libraries is, however, not practical due to the problem of path explosion. To this end, we propose the use of SymCerts, which are X.509 certiﬁcate chains carefully constructed with a mixture of symbolic and concrete values. Utilizing SymCerts and some domain-speciﬁc optimizations, we symbolically execute the certiﬁcate chain validation code of each library and extract path constraints describing its accepting and rejecting certiﬁcate universes. These path constraints help us identify missing checks in different libraries. For exposing subtle but intricate noncom- pliance with X.509 standard, we cross-validate the constraints extracted from different libraries to ﬁnd further implementation ﬂaws. Our analysis of 9 small footprint X.509 implementations has uncovered 48 instances of noncompliance. Findings and suggestions provided by us have already been incorporated by developers into newer versions of their libraries.s about whether a given certiﬁcate chain is valid, even though it is not clear which implementation is noncompliant, we can conclude that one of the libraries is noncompliant. Precisely, for any two implementations I1 and I2 and their corresponding sets A1, R1, A2, and R2, any c ∈C such that (1) c ∈A1 ∧c ∈R2 or (2) c ∈A2 ∧c ∈R1 represents an instance of noncompliance. One can utilize the path constraints from two different im- plementations to ﬁnd inconsistent conclusions in the following two ways. In our analysis, we follow approach 2. Let us assume for any two given implementations Ip and Iq, we have the following sets: Ap = {ap 1, ap 2, . . . , ap n} (accepting certiﬁcate universe of Ip) Rp = {r p 1 , r p 2 , . . . , r p m} (rejecting certiﬁcate universe of Ip) Aq = {aq 1, aq 2, . . . , aq s} (accepting certiﬁcate universe of Iq) Rq = {r q 1 , r q 2 , . . . , r q t } (rejecting certiﬁcate universe of Iq) Approach 1: To detect inconsistencies between Ip and Iq, one can check to see whether either of the following formulas is satisﬁable: ¬(W (1≤i≤n) ap i ↔W (1≤j≤s) aq j ) and ¬(W (1≤i≤m) r p i ↔W (1≤j≤t) r q j ) (↔stands for logical equiva- lence). The ﬁrst (resp., second) formula asserts that the accept- ing (resp., rejecting) paths of Ip and Iq are not equivalent. Any model of either of the formulas will signify a noncompliant instance. We, however, do not utilize this approach to detect noncompliance for the following three reasons: (1) For each satisﬁability query the SMT solver will present one model (i.e., one noncompliant instance) even in the presence of multiple noncompliant instances (We desire as many noncompliant instances instead of just one at a time); (2) The resulting formulas are large and it may put heavy burden on the SMT solver; (3) Due to the incompleteness caused by techniques used to relieve path explosion, the extracted sets A and R may not be exhaustive (i.e., complete), yielding false positives. Approach 2: In this approach, we ﬁrst take each accepting path ap i from Ap and each rejecting path r q j from Rq where 1 ≤i ≤n, 1 ≤j ≤t, and check to see whether the formula ap i ∧r q j is satisﬁable by consulting an SMT solver. If the formula is satisﬁable, it signiﬁes that there is at least one certiﬁcate chain that Ip accepts but Iq rejects. The model obtained for the formula from the SMT solver, can be used to construct a concrete certiﬁcate chain signifying an evidence of inconsistency. We can then repeat the same process by taking each accepting path from Iq and each rejecting path from Ip. Note that, multiple pairs may induce inconsistencies due to the same noncompliant behavior and sometimes best- effort manual analysis of the source code is needed to detect the root cause. D. Scalability Challenges of Applying Symbolic Execution The application of symbolic execution in a straightforward way to extract the sets A and R, considering all certiﬁcates in the chains and other arguments to the CCVL function to have symbolic values, will not yield a scalable noncompliance detection approach. Our feasibility evaluation have veriﬁed this observation. We have also tried only one of the certiﬁcates in the chain to have symbolic values and even then the symbolic execution did not ﬁnish due to resource exhaustion. The scalability problem is predominantly due to symbolic value dependent loops—loops whose terminating conditions depend on symbolic values—in the certiﬁcate parsing im- plementation. One way to get around this challenge is to assume the correctness of the parsing code and just focus on the core CCVL logic. Ignoring the parsing logic, however, is not sufﬁcient to capture the majority of the CCVL logic as some of the sanity checks on the certiﬁcate ﬁelds are done during parsing. In addition, capturing only the CCVL logic would require one to manually modify the internal data structure where the certiﬁcate ﬁelds are stored after parsing. This approach requires signiﬁcant manual efforts (i.e., code comprehension) and is also error-prone. E. Our Solution—SymCerts and Problem Decomposition For addressing the scalability challenge we rely on carefully crafting symbolic certiﬁcates and also on our domain speciﬁc observations. Rather than extracting the complete sets A and R, we use domain-speciﬁc observations and specially crafted symbolic certiﬁcate chains to extract an approximation of the sets A and R, i.e., Aapprox and Rapprox. Our approximation has both under- and over-approximation. To overcome path explo- sion, we create a chain of SymCerts where some portions of each certiﬁcate have concrete values whereas the others have symbolic values. SymCerts along with the following observation aid in achieving scalability during the extractions of the sets Aapprox and Rapprox from an X.509 CCVL. One domain speciﬁc observation we use is the logical independence between certiﬁcate ﬁelds. For instance, the logic of checking whether a certiﬁcate is expired according to its notAfter ﬁeld is independent of the logic of checking whether a certiﬁcate’s issuer name matches with the subject name of the predecessor certiﬁcate in the chain. In this case, we can try to capture the logic of checking certiﬁcate expiration independently of the checking of issuer and subject names. Based on the notion of independence, we group the certiﬁcate ﬁelds into equivalence classes where the logic of ﬁelds in the same equivalence class should be extracted at the 7 same time, that is, ﬁelds in the same equivalence class should be marked to have symbolic values at the same time. We leverage this observation by generating a SymCert chain for each equivalence class where each element of the equivalence class has symbolic values whereas the rest of the ﬁelds have concrete values. Note that we certainly do not claim that the checking logic of all certiﬁcate ﬁelds are independent; there are obviously certiﬁcate ﬁelds whose value inﬂuences one another. For instance, the value of the isCA ﬁeld of an X.509 certiﬁcate (i.e., whether the certiﬁcate is a CA certiﬁcate) prescribes certain corresponding key usage purposes (i.e., affecting the KeyUsage extension). In this case, the isCA ﬁeld needs to be in the same equivalence class as KeyUsage. In our analysis, we conservatively partition the certiﬁcate ﬁelds into 2 equivalence classes. We refer to these two equivalence classes as EqC1 and EqC2, respectively. EqC1 has all the relevant certiﬁcate ﬁelds symbolic, except the Validity date time period ﬁelds which are symbolic only in EqC2. V. IMPLEMENTATION In this section, we discuss additional challenges of applying symbolic execution to CCVL code, and our approach to addressing these challenges. We also discuss other aspects of implementing our noncompliance ﬁnding approach. Challenge 1 (Complex Structure of X.509 Certiﬁcates): X.509 certiﬁcates are represented in the Abstract Syntax No- tation One (ASN.1) [91], [92] notation. X.509 certiﬁcates are typically transmitted in byte streams encoded following the DER (Distinguished Encoding Rules), which are binary in nature. Under the DER format, an X.509 certiﬁcate has the form ⟨t, ℓ, v⟩where t denotes a type, ℓdenotes the length of the values in bytes, and ﬁnally v represents the value. t can represent complex types such as a sequence where the value v can be recursively made of other ⟨t, ℓ, v⟩triplets. Such nesting of ⟨t, ℓ, v⟩triplets inside a v ﬁeld can be arbitrarily deep. The problem of marking the whole certiﬁcate byte-stream as symbolic is that, during certiﬁcate parsing, the symbolic ex- ecution engine will try different values for ℓas it is symbolic, and the parsing code will keep reading bytes without knowing when to stop. This will cause memory exhaustion. Approach—SymCerts (Certiﬁcates With Symbolic and Concrete Values): To avoid the scalability problem, instead of using a fully symbolic certiﬁcate chain, we develop a certiﬁcate chain in which each certiﬁcate byte-stream contains some of concrete values and some symbolic values. We call each such certiﬁcate a SymCert. We construct a SymCert in the following way: For each leaf ⟨t, ℓ, v⟩tuple (i.e., v contains a value instead of another ⟨t, ℓ, v⟩tuple) in a certiﬁcate byte-stream, we ensure that the ﬁelds t and ℓhave concrete values whereas only the v ﬁeld is symbolic. Concrete values of t can be obtained from actual certiﬁcates and we use them as the backbone for generating SymCerts. For the l ﬁeld, we consult the RFC document to select appropriate concrete values. For instance, when marking the OIDs used in the ExtKeyUsage extension symbolic, we give it a concrete length of 8, as most of the standard key usage purposes deﬁned in RFC 5280 [2] are 8-byte long. Due to the complexity of DER byte-streams, it is difﬁcult for a user to directly manipulate and construct SymCerts from scratch. In addition, due to nesting, changing the length ﬁeld (i.e., ℓ) of a child ⟨t, ℓ, v⟩triplet may require adjustment on the length ﬁeld (i.e., ℓ) of the parent ⟨t, ℓ, v⟩triplet. For this, we developed a Graphical User Interface (GUI), by extending the ASN.1 JavaScript decoder [93]. Our GUI allows a user to see and click on different certiﬁcate ﬁelds, so that they can be replaced with a desired number of symbolic bytes, and the new length will be correctly adjusted. The GUI will then automatically generate code that can be used for symbolic execution. We use OpenSSL to generate concrete certiﬁcate chains as the input to our GUI, which constitute the basis of our SymCerts. The philosophy here is that all major ﬁelds (e.g. optional extensions, criticality booleans) of a certiﬁcate need to be explicitly available on the base input certiﬁcate, as it is difﬁcult to mark nonexistent ﬁelds symbolic. Challenge 2 (System Time Handling): Given that our symbolic execution of the implementations would happen at different times, if we simply allow the implementations to use the local system time, then the constraints we have extracted would not be comparable, as the system time elapses. Approach—Constant Static Time: We consider a ﬁxed concrete time value for the system time. We use the same concrete value for these inputs during the analysis of all implementations. Using a symbolic variable is also possible, but using concrete values has the advantage of reducing the complexity of the path constraints which consequently improves scalability. Challenge 3 (Cryptographic Functions): The cryptographic functions (e.g., for verifying digital signatures) called by the CCVL contain loops dependent on symbolic data, which severely impact the scalability of symbolic execution. Approach—Cryptographic Stub Functions: We abstract away the cryptographic functions with stub functions. For instance, the function that matches the digital signature of a certiﬁcate is abstracted away by a stub function that returns True indicating the match was successful. In this work, we consider cryptographic correctness beyond our scope. Instead, we are interested in ﬁnding out what ﬁelds are checked and what restrictions are imposed on these ﬁelds. Challenge 4 (Complex String Operations): As part of the CCVL, implementations are sometimes required to perform complex string operations (e.g., wild card matching, null checking) on certiﬁcate ﬁelds such as subject name and issuer name. Faithfully capturing the string operations with QF BVA logic (i.e., QFFOL formulas with equality, bit vector, and array theories)—which is the underlying logic of the symbolic execution engine we use—does not scale well. Approach—Single Byte Strings: We consider names and other string-based certiﬁcate ﬁelds to have a single byte symbolic value, which signiﬁcantly improves the scalability. However, because of this, our analysis misses out on ﬁnding noncompliance due to erroneous string operations. 8 Challenge 5 (Hashing for Checking Multi-Field Equal- ity): When checking the equality of two name ﬁelds of certiﬁcates—name ﬁelds are compound ﬁelds containing the following sub-ﬁelds such as street address, city, state/province, locality, organizational name & unit, country, common name— some implementations take a hash of the concatenation of all the sub-ﬁelds and match the hash values, instead of checking the equality of each sub-ﬁeld. Trying to solve the constraints from such a match would be similar to attacking the hash collision problem, which is not scalable to analyze with symbolic execution due to symbolic data-dependent loops. Approach—Hash Stub: The hash function in question (i.e., SHA-1) returns a 20-byte hash value. We replace it with a SHA-1 stub which returns a 20-byte value where the (symbolic) name sub-ﬁelds are packed together. Because of the single byte approach we introduced to simplify string operations described in the previous challenge, 20-byte is more than enough to pack all name sub-ﬁelds of interests. Challenge 6 (Certiﬁcate Chain Length): While symboli- cally executing the CCVL of a given implementation, one natural question that arises is: “How many certiﬁcates in the symbolic certiﬁcate chain should we consider?” An X.509 CCVL implementation often parses the input X.509 certiﬁcate chain ﬁrst and then checks the validity of different ﬁelds in the certiﬁcates of the parsed chain. During symbolic execution, if the execution detects a loop whose terminating condition relies on a symbolic value, it faces the dilemma of how many times to unroll the loop. Such loops in the implementation often cause path explosion in symbolic execution, resulting in incompleteness and scalability challenges. If we consider the certiﬁcate chain length to be symbolic, then the symbolic execution, especially during parsing, would try all possible values for the chain length, causing memory exhaustion. Approach—Concrete Chain Length: For majority of our analysis, we consider a certiﬁcate chain of length 3 such that one of the certiﬁcates is the root CA certiﬁcate, the other is an intermediate CA certiﬁcate, and ﬁnally the remaining certiﬁcate is the certiﬁcate of the server currently being authenticated. While analyzing the logic of checking the path length constraint of the basic constraint extension, we also consider certiﬁcates with chain length 4 where we have two intermediate CA certiﬁcates. Challenge 7 (Other aspects of Path Explosion): After the simpliﬁcations described above, the symbolic execution engine still generates a large number of paths. We especially observed that making all the v values of ⟨t, ℓ, v⟩-tuples that represent certiﬁcate ﬁelds and extensions symbolic yields a lot of paths. Approach—Early Rejection and Grouping Fields: We observed that implementations sometimes do not return early even in the case one of the certiﬁcates cannot be parsed or one of the ﬁelds validity checks failed. This contributes to a multi- plicative factor to the number of paths. We judiciously applied early rejection when parsing or validation check fail. Finally, we applied the logical independence between certiﬁcate ﬁelds based on their semantics to decompose the noncompliance ﬁnding ﬁelds. We generated two equivalence classes, one consists of time ﬁelds related to the certiﬁcate Validity period checking, whereas the other contains all the remaining ﬁelds. One could possibly employ a more aggressive grouping of ﬁelds that need to be check together. We, however, make a conservative choice because if developer incorrectly introduces artiﬁcial dependencies in the implementation, we would like to capture them as well. Challenge 8 (Time Field Comparison): An X.509 certiﬁcate contains two time ﬁelds (i.e., notBefore and notAfter) which are compared to the current system time. A time ﬁeld can be represented in two formats (i.e., GeneralizedTime and UTCTime). In GeneralizedTime, the time ﬁeld contains a 15- byte ASCII string where day, month, hour, minute, second contribute 2 bytes each; year contributes 4 bytes, and 1 byte is used to represent the time zone. For UTCTime, the only difference is that year contributes 2 bytes instead of 4. Sanity checks are often performed to ensure the ﬁelds are well- formed (e.g., for minute, the most signiﬁcant digit cannot be larger than 6). Marking the format symbolic and let the symbolic execution engine choose the length of the ASCII string contributes to poor scalability. Approach—Decomposing Time Fields: In addition to checking noncompliance in time ﬁelds handling independently from other ﬁelds, we further decompose the analysis by ana- lyzing the two time formats separately. We use two different SymCerts during symbolic execution, one with UTCTime and the other with GeneralizedTime, using the concrete length of the date time ASCII string according to the format. Challenge 9 (Redundant Pair of Paths in Cross-Validation): When cross-validating two implementations Ip and Iq, the upper bound of discrepancies is |Ap| × |Rq| + |Aq| × |Rp|. Based on the number of paths in accepting (e.g., Ap and Aq) and rejecting (e.g., Rp and Rq) universes, the maximum number of noncompliance instances can be fairly large which creates a challenge for manual inspection to identify the root cause of the noncompliance. Approach—Iterative Pruning: We observe that many pairwise discrepancies are due to the same root cause. Suppose implementation Ip does not check a particular ﬁeld that Iq checks. In this case, the missing check in Ip’s accepting path will likely be enumerated through many rejecting paths of Iq, resulting in a large number of redundant noncompliance instances. To make it easier to analyze the results of cross- validation, once we have identiﬁed such a case, we can concretize the value of that speciﬁc ﬁeld, repeat the extraction step and continue cross-validation with a pruned search space. Challenge 10 (False Positives): Due to different domain speciﬁc simpliﬁcations and the fact that we are abstracting away cryptographic functions, our approach can yield false positives, predominantly due to the path constraint extrac- tion might not be capturing the real execution faithfully. In addition, the speciﬁcation (i.e., RFC document) states some ﬁelds should be checked by a certiﬁcate using system, without imposing whether the library or application (the two of them constitute the system) should perform each check. Conse- quently, SSL/TLS libraries have different API designs due to 9 such unclear separation of responsibility. Some libraries might enforce all the checks during certiﬁcate chain validation, while some might not and instead provide optional function calls for application developers desiring such checks, and the other libraries might completely delegate the task of implementing such checks to the application developers. As a clear boundary cannot be drawn easily, false positives can arise if some optional but provided checks are missed out during extraction. Approach—Concrete Replay: To avoid false positives, we use a real client-server setup to help us verify our ﬁndings. We capitalize on the fact that a minimalistic sample client code is often made available in the source tree by library developers to demonstrate how the library should be used in application development and use such clients to draw the baseline. To gain conﬁdence that our extracted path constraints adequately capture the real execution, for each accepted (resp., rejected) path constraint, we consult the SMT solver to obtain a concrete certiﬁcate chain and feed it to a real client-server setup to see whether the client would actually accept (resp., reject) the chain. This helps us to see whether the real execution concurs with our extraction. Similarly, during cross validation between implementations Ip and Iq, for the discrepancies we found (in the form of a model provided by the SMT solver), we construct a concrete certiﬁcate chain out of the model and use the client- server setup to verify it is indeed the case that Ip would accept and Iq would reject the chain. VI. EVALUATION AND RESULTS We applied our approach in testing 9 open-source imple- mentations from 4 major families of SSL/TLS library source trees, as shown in Table I. Implementations that have been tested in previous study by Brubaker et al. [43] are preﬁxed with an asterisk. These libraries have seen active deployments in embedded systems and IoT products to satisfy the security needs for connecting to the Internet (e.g. axTLS in Arduino [51] and MicroPython [52] for ESP8266, mbedTLS, tropicSSL and MatrixSSL on Particle hardware [49], [50], etc.), and are sometimes used even in building applications and libraries on conventional desktop platforms [57]–[61], due to their performance and small footprint advantage. We test multiple versions of a library from the same family in order to compare with previous work, and to see if the more recent versions implement a more complete and robust validation logic. In this section we ﬁrst show statistics that justify the practicality of our approach, and then present noncompliance ﬁndings grouped by how we uncovered them along the 3 approaches described in Section IV-C, together with other discrepancies and observations that we made while work- ing with the libraries. Findings on recent versions of the implementations, whenever applicable, are reported to the corresponding developers. Many of our reports had led to ﬁxes being implemented in newer versions. A. Implementation Efforts and Practicality For our analysis, we used the KLEE symbolic execution engine [45] and the STP SMT solver [65]. We added around 2000 lines of C++ code for implementing the path constraint extraction and cross validation engines, around 500 lines of Python for parsing path constraints and automating concrete test case generation, and around 400 lines of HTML plus less than 300 lines of JavaScript for the GUI that enables the easy construction of SymCerts. In order to implement the various optimizations described before, a limited amount of new code need to be added to the libraries that we tested. As shown in Table I, no more than 75 lines of code were added to each of the library. Most of the new code is used to implement a static system time (see Section V-Challenge 2) and a stub cryptographic signature check (Section V-Challenge 3). Additionally, for CyaSSL 2.7.0, wolfSSL 3.6.6, and MatrixSSL 3.7.2, some code was added to implement the hash stub (see Section V-Challenge 5). PolarSSL 1.2.8 and tropicSSL needed a simpliﬁed version of sscanf(), and axTLS (both 1.4.3 and 1.5.3) needed a simpliﬁed version of mktime(), to avoid symbolic-data dependent loops, both of which are used for reading in and converting the format of date-time inputs. Also shown in Table I are the performance statistics regard- ing path extraction. We ran our experiments on a commodity laptop equipped with an Intel i5-2520M CPU and 16GB RAM. Path extraction using EqC1 for most implementations ﬁnished in minutes, while for some heavier ones it completed in hours. The total number of paths ranges from hundreds to the level of ten thousands. For EqC2, we report the upper bound of the total number of paths, referred to in the table as “Total Paths”, because the actual number could vary within each library due to different treatments (and possibly missing checks) for UTCTime and GeneralizedTime (see Section VI-C and VI-D for examples). For each library, extraction using EqC2 yielded paths at the scale of tens, and ﬁnished within a minute. B. Errors Discovered By Symbolic Execution The ﬁrst opportunity our approach provides is that, during symbolic execution, certain low-level coding issues (e.g. mem- ory access errors, division by zeros, etc.) could be found. Finding 1 (Incorrect extension parsing in CyaSSL 2.7.0 1): As shown in Listing 1, due to a missing break statement after DecodeAltNames(), the execution falls through to the next case and also invokes DecodeAuthKeyId(). Consequently, some bytes of the subject alternative name extension, which we made symbolic, will overwrite the authority key identiﬁer (a pre-computed hash value) at the time of parsing. The error manifests later during certiﬁcate chain validation, when the authority key identiﬁer undergoes some bit shifting operations and modulo arithmetic, effectively turning it into an array accessing index, which is then used to fetch a CA certiﬁcate from a table of trusted CA certiﬁcates. Since some bytes of the authority key identiﬁer were incorrectly made symbolic during parsing, the execution engine caught potential memory access errors in fetching from the table. This was not reported in [43], which applied fuzzing to test CyaSSL 2.7.0. Our conjecture is 1This bug has been ﬁxed in newer versions of CyaSSL and wolfSSL. 10 TABLE I PRACTICALITY AND EFFICACY OF APPLYING THE SYMCERT APPROACH IN TESTING VARIOUS SMALL FOOTPRINT SSL/TLS LIBRARIES Library - version Released Lines of C code in library Lines Added Paths  EqC1  Extraction Time  EqC1  Total Paths  EqC2  Extraction Time  EqC2  Found Instances of Noncompliance axTLS - 1.4.3 Jul 2011 16,283 72 276 (419) ∼1 Minute ≤52 ≤1 minute 7 axTLS - 1.5.3 Apr 2015 16,832 69 276 (419) ∼1 Minute ≤52 ≤1 minute 6 * CyaSSL - 2.7.0 Jun 2013 51,786 33 32 (504) ∼2 Minutes ≤26 ≤1 minute 7 wolfSSL - 3.6.6 Aug 2015 103,690 40 256 (31409) ∼1 Hour ≤26 ≤1 minute 2 tropicSSL - (Github) Mar 2013 13,610 66 16 (67) ∼1 Minute ≤30 ≤1 minute 10 * PolarSSL - 1.2.8 Jun 2013 29,470 66 56 (90) ∼1 Minute ≤81 ≤1 minute 4 mbedTLS - 2.1.4 Jan 2016 53,433 15 13 (536) ∼1 Minute ≤41 ≤1 minute 1 * MatrixSSL - 3.4.2 Feb 2013 18,360 9 8 (160) ∼1 Minute 1 ≤1 minute 6 MatrixSSL - 3.7.2 Apr 2015 37,879 30 3240 (8786) ∼1 Hour ≤25 ≤1 minute 5 § The fourth column of the table refers to the lines of code we added to the libraries to make them amenable to our analysis. The ﬁfth and sixth columns display the number of accepting (rejecting) paths we obtained when we made the ﬁelds in equivalence class EqC1 symbolic, and the time it took to complete the extraction process, respectively. The seventh and eighth columns show the upper bound of total paths (including both accepting and rejecting) we observed when the ﬁelds in EqC2 are made symbolic, and the time it took for the path extraction process to complete, respectively. that it would be difﬁcult for concrete test cases to hit this bug, as the execution is likely to fall through without triggering any noticeable crashes. Listing 1. Extension Processing In CyaSSL 2.7.0 switch (oid) { ... case AUTH_INFO_OID: DecodeAuthInfo(&input[idx], length, cert); break; case ALT_NAMES_OID: DecodeAltNames(&input[idx], length, cert); case AUTH_KEY_OID: DecodeAuthKeyId(&input[idx], length, cert); break; ... } C. Findings From Simple Search of Path Constraints Fields of certiﬁcates, represented by symbolic variables in our approach, will appear on path constraints if they are in- volved in branching decisions either directly or indirectly (e.g. some other decision variables were calculated based on their values). Consequently, the second opportunity our approach offers is that immediately after extracting path constraints using symbolic execution, missing checks of ﬁelds can be discovered by performing “grep” on the path constraints. Finding 2 (pathLenConstraint ignored in CyaSSL 2.7.0, wolfSSL 3.6.6 2): We noticed that both of the aforementioned libraries fail to take pathLenConstraint into consideration, which means any such restrictions imposed by upper level issuing CAs would be ignored by the libraries. This was not reported in [43], where fuzzing was ap- plied to CyaSSL 2.7.0. Interestingly, [43] instead reported that CyaSSL 2.7.0 incorrectly rejects leaf CA certiﬁcates given the intermediate CA certiﬁcate has a pathLenConstraint of 0, and is noncompliant because such certiﬁcates should be accepted according to the RFC. Our ﬁndings, however, demonstrate that CyaSSL 2.7.0 could not possibly be re- jecting certiﬁcates for such a reason because it completely ignores pathLenConstraint. Testing CyaSSL 2.7.0 with con- crete certiﬁcates conﬁrmed our ﬁnding. Thus, the conclu- sion in [43] that CyaSSL 2.7.0 misinterprets RFC regarding 2wolfSSL 3.9.10 has implemented support for pathLenConstraint [94]. pathLenConstraint and leaf CA certiﬁcate is incorrect. We conjecture that this is because the frankencerts used as evi- dence for such conclusion also happen to contain other errors, and were thus rejected by CyaSSL 2.7.0. This demonstrates the difﬁculty of interpreting results obtained from fuzzing. Finding 3 (pathLenConstraint of intermediate CA certiﬁ- cates ignored in tropicSSL, PolarSSL 1.2.8 3): Our path con- straints show that even though both tropicSSL and PolarSSL 1.2.8 recognize the pathLenConstraint variable during parsing time, they check only the one that is on the trusted root certiﬁcate during chain validation, and ignores those that are on intermediate CA certiﬁcates of a given chain. In addition to the fact that PolarSSL 1.2.8 does not check pathLenConstraint on intermediate CA certiﬁcates, another simple search found that PolarSSL 1.2.8 does not check whether the leaf certiﬁcate is CA or not (which is not a noncompliant behavior). It was however reported in [43] that PolarSSL 1.2.8 violates the RFC by always rejecting leaf CA certiﬁcates if the intermediate CA certiﬁcate has a pathLenConstraint of 0. This is incorrect because PolarSSL 1.2.8 checks neither pathLenConstraint on intermediate CA certiﬁcates, nor whether the leaf certiﬁcate is CA or not. Finding 4 (Certain attribute types of distinguished names ignored in axTLS 1.4.3 and 1.5.3): Both axTLS 1.4.3 and 1.5.3 ignore the country, state/province and locality attribute types of the issuer and subject names. In other words, organizations from different countries and states having the same name would be considered equivalent during matching. This is a clear deviation from RFC 5280 (Section 4.1.2.4) [2]. We have this ﬁnding reported to the developer of axTLS, who acknowledged the existence of the problem and imple- mented a ﬁx in the new 2.1.1 release. Finding 5 (Inability to process GeneralizedTime in axTLS 1.4.3, tropicSSL): RFC 5280 (Section 4.1.2.5) [2] states “Con- forming applications MUST be able to process validity dates that are encoded in either UTCTime or GeneralizedTime.” However, given our SymCerts with GeneralizedTime, both 3The enforcement of pathLenConstraint from intermediate CA certiﬁcates has been introduced since PolarSSL 1.2.18 [95]. 11 tropicSSL and axTLS 1.4.3 returned only 1 concrete rejecting path with an empty path constraint, hence we conclude that the aforementioned libraries cannot handle GeneralizedTime, which is a non-conformance to the RFC. However, the same SymCerts managed to yield meaningful path constraints in axTLS 1.5.3, showing that support for GeneralizedTime has been added in the newer version of axTLS. Finding 6 (KeyUsage and ExtKeyUsage being ignored in MatrixSSL 3.4.2, CyaSSL 2.7.0, tropicSSL): The three aforementioned implementations do not check KeyUsage and ExtKeyUsage extensions. This noncompliance implies that certiﬁcates issued speciﬁcally for certain intended purposes (e.g. only for software code signing) can be used to authenti- cate a server in SSL/TLS handshakes. Honoring such restric- tions imposed by issuing CAs allows the PKI to implement different levels of trust, and help avoid certiﬁcate (and CA) misuse in general. Finding 7 (notBefore ignored in tropicSSL, PolarSSL 1.2.8; validity not checked in MatrixSSL 3.4.2): Our SymCerts revealed that PolarSSL 1.2.8 does not check the notBefore ﬁeld, and MatrixSSL 3.4.2 does not have an inbuilt validity check, as there is only 1 path, which is an accepting path with empty constraints, for each of the aforementioned libraries in their respective cases. This is coherent with the ﬁndings in [43]. MatrixSSL 3.4.2 delegates the task of checking certiﬁcate validity to application developers. tropicSSL has the same problem as PolarSSL 1.2.8, which is not a surprise considering the fact that tropicSSL is a fork of PolarSSL. Finding 8 (hhmmss of UTCTime ignored in tropic- SSL, axTLS 1.4.3 and 1.5.3; hhmmss of both UTCTime and GeneralizedTime ignored in MatrixSSL 3.7.2): Given UTCTime on certiﬁcates, even though axTLS 1.4.3 and 1.5.3 check for both notBefore and notAfter, they do not take the hour, minute and second into consideration, which means that there could be a shift for as long as a day in terms of rejecting future and expired certiﬁcates. This ﬁnding is particularly interesting for axTLS 1.5.3, as its implementation of GeneralizedTime support can actually handle hour, minute and second, but for some reason UTCTime is processed in a laxer manner. Following our report, the developer of axTLS has acknowledged the problem and is currently considering a ﬁx. Our extracted path constraints show and tropicSSL also suffer from the same problem. Unlike its older counterpart, MatrixSSL 3.7.2 has im- plemented validity checks that handle both UTCTime and GeneralizedTime. However, our extracted path constraints re- vealed that MatrixSSL 3.7.2 does not attempt to check the time portion of the validity ﬁelds, regardless of whether the date- time information is in UTCTime or GeneralizedTime. The developers of MatrixSSL had explained to us the decision to ignore the time portion was made due to its embedded origin, where a local timer might not always be available, and in their own words “having date set correctly is difﬁcult enough”. They have also admitted that as the result of such decision, a 24-hour shift in rejecting future and expired certiﬁcates is inevitable. Finding 9 (notAfter check applies only to leaf certiﬁcate in tropicSSL): Not just that future certiﬁcates are not rejected (e.g. missing check for notBefore as described above) in tropicSSL, our path constraints show that, given a chain of certiﬁcates, the check on notAfter only applies to the leaf one. This could lead to severe problems, for instance, if a retired private key of an intermediate issuing CA corresponding to an expired certiﬁcate got leaked, attackers would be able to issue new certiﬁcates and construct a new chain of certiﬁcate that will be accepted by tropicSSL. Finding 10 (Incorrect CA certiﬁcate and version number as- sumptions in axTLS 1.4.3 and 1.5.3, CyaSSL 2.7.0, MatrixSSL 3.4.2): The aforementioned implementations deviate from the RFC in how they establish whether certiﬁcates of various versions are CA certiﬁcates or not. As explained previously in Section III-A2, in case the certiﬁcate has a version older than 3, some out-of-band mechanisms would be necessary to verify whether it is a CA certiﬁcate or not. axTLS 1.4.3 and 1.5.3 assume certiﬁcates to be CA certiﬁcates regardless of the version number. CyaSSL 2.7.0 also does not check the version number, though whenever the basicConstraints extension is present, it will be used to determine whether the certiﬁcate is a CA certiﬁcate or not. MatrixSSL 3.4.2 does check the version number, and would check the basicConstraints exten- sion for version 3 certiﬁcates. However, it would just assume certiﬁcates older than version 3 to be CA certiﬁcates. The ﬁndings on CyaSSL 2.7.0 and MatrixSSL 3.4.2 are coherent with the relevant results reported in [43]. Finding 11 (Unrecognized critical extensions in MatrixSSL 3.4.2, CyaSSL 2.7.0, axTLS 1.4.3 and 1.5.3): Section 4.2 of RFC 5280 states “A certiﬁcate-using system MUST reject the certiﬁcate if it encounters a critical extension it does not recognize or a critical extension that contains information that it cannot process.” [2]. Not rejecting unknown critical exten- sions could lead to interoperability issues. For example, certain entities might deﬁne and issue certiﬁcates with additional non- standard custom extensions, and rely on the default rejection behavior as described in RFC 5280 to make sure that only a speciﬁc group of implementations can handle and process their certiﬁcates. However, we found that MatrixSSL v3.4.2 and CyaSSL 2.7.0 would accept certiﬁcates with unrecognized critical extensions, which is consistent to the ﬁndings in [43]. In addition, we found that axTLS 1.4.3 and 1.5.3 would also accept certiﬁcates with unrecognized critical extensions. In fact, based on the path constraints we have extracted, they do not recognize any of the standard extensions that we wanted to test at all, which deviates from RFC 5280, as Section 4.2 says the minimum requirement for applications conforming to the document MUST recognize extensions like key usage, basic constraints, name constraints, and extended key usage, etc. Similarly for mbedTLS 2.1.4, as we have noticed for not implementing support for the name constraints extension, is also noncompliant in that sense. The implication of this is that restrictions imposed by issuing CAs in the form of name constraints will not be honored by mbedTLS 2.1.4, resulting in potential erroneous acceptance of certiﬁcates. At 12 the time of writing, developers of mbedTLS have indicated that they currently have no plans on implementing support for this extension, and suggested that application developers can implement their own if desired. D. Findings From Cross-Validating Libraries The ﬁnal opportunity would be to cross-validate libraries, speciﬁcally, for each accepting path of library A and each rejecting path of library B, we perform a conjunction and see if the resulting constraints would be solvable or not. If yes, it signiﬁes a discrepancy exists between the two libraries. Finding 12 (ExtKeyUsage OID handling in wolfSSL 3.6.6, MatrixSSL 3.7.4): Our path constraints also unveiled that despite being two of the few libraries that support the extended key usage extension, both wolfSSL 3.6.6 and MatrixSSL 3.7.2 opted for a somewhat lax shortcut in handling the extension: given the object identiﬁer (OID) of a key usage purpose, they do a simple summation (referred colloquially as a non- cryptographic digest function by the developers of MatrixSSL) over all nodes of the OID, and then try to match only that sum. For example, under such scheme, the standard usage purpose “server authentication” (OID 1.3.6.1.5.5.7.3.1, DER-encoded byte values are 0x2B 0x06 0x01 0x05 0x05 0x07 0x03 0x01) would be treated as decimal 71. Notice that the extension itself is not restricted to only hold standard usage purposes that are deﬁned in the RFC, and custom key usage purposes are common 4. Since OIDs are only meant to be unique in a hierarchical manner, the sums over nodes of OIDs are not necessarily unique. Hypothetically some enterprises under the private enterprise arc (1.3.6.1.4.1) could deﬁne OIDs to describe their own key usage purposes, and if added to the extension, those OIDs might be incorrectly treated as some of the standard key usage purposes by the two libraries. This could be problematic for both interoperability and security, as custom key usage purposes would be misin- terpreted, and the standard ones could be spoofed. This ﬁnding is a good example of how our approach can be used to discover the exact treatments that variables undergo inside the libraries during execution. It might also be difﬁcult for unguided fuzzing to hit this particular problem. We contacted the corresponding developers of the 2 libraries regarding this, and both acknowledged the problem exists. wolfSSL has introduced a more rigorous OID bytes checking since version 3.7.3 5, and MatrixSSL is planning to incorporate additional checks of the OID bytes in a new release. Finding 13 (Incorrect interpretation of UTCTime year in MatrixSSL 3.7.2, axTLS 1.4.3 and 1.5.3, tropicSSL): Since UTCTime reserves only two bytes for representing the year, one needs to be cautious when interpreting it. RFC 5280 Section 4.1.2.5.1 [2] says that when the YY of a UTCTime is 4For example, Microsoft deﬁnes its own key usage purposes and the corresponding OIDs that are deemed meaningful to the Windows ecosystem [96] (the extension is referred to as “Application Policy” in Microsoft terminology, and is not to be confused with “Certiﬁcate Policy”). 5https://github.com/wolfSSL/wolfssl/commit/ d248a7660cc441b68dc48728b10256e852928ea3 larger than or equal to 50 then it should be treated as 19YY, otherwise it should be treated as 20YY. This essentially means that the represented range of year is 1950 to 2049 inclusively. During cross-validation, we noticed that in certain libraries, some legitimate years are being incorrectly rejected (and accepted). A quick inspection of the path constraints, concrete- value counterexamples, and ﬁnally the source code, found the following instances of noncompliance. Listing 2. UTCTime year adjustment in MatrixSSL 3.7.2 y = 2000 + 10 * (c[0] - ’0’) + (c[1] - ’0’); c += 2; /* Years from ’96 through ’99 are in the 1900’s */ if (y >= 2096) { y -= 100; } As shown in Listing 2, MatrixSSL 3.7.2 interprets any YY less than 96 to be in the twenty ﬁrst century. This means certiﬁcates that had expired back in 1995 would be considered valid, as the expiration date is incorrectly interpreted to be in 2095. On the other hand, long-living certiﬁcates that have a validity period began in 1995 would be treated as not valid yet. The developers acknowledged our report on this and have since implemented a ﬁx in a new release. Listing 3. UTCTime year adjustment in tropicSSL to->year += 100 * (to->year < 90); to->year += 1900; A similar instance of noncompliance was found in tropicSSL, as shown in Listing 3. tropicSSL interprets any YY less than 90 to be in the twenty ﬁrst century. Listing 4. UTCTime year adjustment in axTLS 1.4.3 and 1.5.3 if (tm.tm_year <= 50) { /* 1951-2050 thing */ tm.tm_year += 100; } A similar issue exists in both axTLS 1.4.3 and 1.5.3. As shown in Listing 4, there is an off-by-one error in the condition used to decide whether to adjust the year or not. In this case, the year 1950 would be incorrectly considered to mean 2050. Based on the inline comment, it seems to be a case where the developer misinterpreted the RFC. A ﬁx has been implemented in a new version of axTLS following our report. Finding 14 (Incorrect timezone adjustment in MatrixSSL 3.7.2): During cross-validation with other libraries, we noticed that the boundary of date checking in the path constraints of MatrixSSL 3.7.2 was shifted by one day. A quick inspection of the date time checking code found that MatrixSSL 3.7.2 uses the localtime_r() instead of gmtime_r() to convert the current integer epoch time into a time structure. The shift was due to the fact that in conventional libc implementa- tions, localtime_r() would adjust for the local time zone, which might not necessarily be Zulu, hence deviating from the RFC requirements. Assuming the date time on certiﬁcates are in the Zulu timezone, the implication of this subtle issue is that for systems in GMT-minus time-zones, expired certiﬁcates could be considered still valid because of the shift, and certiﬁcates that just became valid could be considered not yet valid. Similarly, for systems in GMT-plus time-zones, certiﬁcates that are still valid might be considered expired, and future certiﬁcates that are not yet valid would be considered valid. 13 We discussed this with the developers of MatrixSSL. They conjectured the reason for using localtime_r() instead of gmtime_r() was due to the latter being unavailable on certain embedded platforms. They have agreed, however, as MatrixSSL is gaining popularity on non-embedded platforms, in a new release, they will start using gmtime_r() on platforms that support it. Finding 15 (Overly restrictive notBefore check in CyaSSL 2.7.0 6): RFC 5280 Section 4.1.2.5 says “The validity period for a certiﬁcate is the period of time from notBefore through notAfter, inclusive.” However, when cross-validating CyaSSL 2.7.0 with other libraries, from the concrete counterexamples we noticed that discrepancy exists in how the same notBefore values would be accepted by other libraries but rejected by CyaSSL 2.7.0, while such discrepancy was not observed with notAfter. An inspection of the notBefore checking code yielded the following instance of noncompliance: Listing 5. Erroneous “less than” check in CyaSSL 2.7.0 static INLINE int DateLessThan(const struct tm* a, const struct tm* b) { return !DateGreaterThan(a,b); } Notice that the negation of > is ≤, not <, which explains why if the current date time happen to be the same as the one described in notBefore, the certiﬁcate would be considered future (not valid yet) and rejected. Hence the notBefore checking in CyaSSL 2.7.0 turns out to be overly restrictive than what the RFC mandates. This is again a new result, comparing to the previous work [43] that also studied CyaSSL 2.7.0. Our conjecture is that given a large number of possible values, it might be difﬁcult for unguided fuzzing to hit boundary cases, hence such a subtle logical error eluded their analysis. Finding 16 (KeyUsage and ExtKeyUsage being ignored in PolarSSL 1.2.8): The fact that PolarSSL 1.2.8 does not check KeyUsage and ExtKeyUsage, evaded our simple search approach but was caught during cross-validation, as the im- plementation actually parses the two extensions, hence some constraints were added as the result of several basic san- ity checks happened during parsing. However, during cross- validation, it became clear that apart from the parsing sanity checks, PolarSSL 1.2.8 does not do any meaningful checks on KeyUsage and ExtKeyUsage. In fact, this resulted in another instance of noncompliance, as PolarSSL 1.2.8 would not reject certiﬁcates with KeyUsage or ExtKeyUsage, even if those two extensions were made critical, and it does not perform any meaningful checks apart from merely parsing them. This is an example where a library is intended to handle an extension but was not able to, because of incomplete implementation. This is consistent with similar results reported in [43], although the ﬁnding that PolarSSL 1.2.8 does not check the KeyUsage extension on intermediate CA certiﬁcates was not reported in that paper. 6This has been ﬁxed in newer versions of CyaSSL and WolfSSL. Finding 17 (pathLenConstraint of trusted root misinter- preted in tropicSSL): During cross validation, it became clear to us that, in tropicSSL: (1) on one hand, some accepting paths would allow the pathLenConstraint variable to be 0; (2) on the other hand, some rejecting paths reject because the pathLenConstraint was deemed to be smaller than an unex- pectedly large boundary. In both cases, the pathLenConstraint variable appears to have been misinterpreted by tropicSSL. We suspect that this might be due to the value 0 in the internal parsed certiﬁcate data structure is used to capture the case where the pathLenConstraint variable is absent (i.e. no limit is imposed). A quick inspection of the parsing code revealed that our suspicion is indeed correct. In fact, the parsing code is supposed to always add 1 to the variable if it is present on the certiﬁcate, but a coding error 7 of missing a dereferencing operator (*) in front of an integer pointer means that the increment was applied to the pointer itself but not the value, hence the observed behavior described above. This subtle bug has a severe implication: it completely de- feats the purpose of imposing such restriction on a certiﬁcate, as a pathLenConstraint of 0 would be incorrectly treated to mean that the chain length could be unlimited. Finding 18 (Not critical means not a CA in tropicSSL): During cross validation, we also noticed that when the interme- diate CA certiﬁcate’s basicConstraints extension is set to non- critical, and the isCA boolean is set to True, tropicSSL would consider the intermediate CA certiﬁcate not a CA certiﬁcate. Additionally, in the path constraints, the symbolic variable representing the criticality of basicConstraints and the one that represents the isCA boolean are always in conjunction through a logical AND. A quick inspection found the following problem in the parsing code that handles the basicConstraints extension: Listing 6. Incorrect adjustment to the isCA boolean in tropicSSL *ca_istrue = is_critical & is_cacert; This interpretation of the basicConstraints extension de- viates from the speciﬁcation, as RFC 5280 says that clients should process extensions that they can recognize, regardless of whether the extension is critical or not. The criticality of basicConstraints should not affect the semantic meaning of attributes in the extension itself. This is an example of a CCVL being overly restrictive. E. Other ﬁndings Here we present other interesting ﬁndings that are not explicitly noncompliant behaviors deviating from RFC 5280. Extra 1 (Ineffective date string sanity check in MatrixSSL 3.7.2): During cross-validation, we noticed that date time byte values in MatrixSSL 3.7.2 are not bounded for exceedingly large or unexpectedly small values. However, in the con- straints, we see combinations of whether each byte is too small or not (though not affecting the acceptance decision), which looked suspiciously like a failed lower boundary check. A quick inspection of the certiﬁcate parsing code unveiled the 7This has been ﬁxed in later versions of PolarSSL and mbedTLS. 14 snippet shown in Listing 7 that is meant to vet a given date string from a certiﬁcate, and reject it with a parser error if the values are outside of an expected range. Unfortunately, due to incorrectly using the && operator instead of ||, the if conditions are never satisﬁable. This is also proven by the fact that if we symbolically execute the code snippet in Listing 7, all possible execution paths returns 1. Consequently that code snippet would actually never reject any given strings, hence completely defeating the purpose of having a sanity check. Listing 7. date string sanity check in MatrixSSL 3.7.2 if (utctime != 1) { /* 4 character year */ if (*c < ’1’ && *c > ’2’) return 0; c++; /* Year */ if (*c < ’0’ && *c > ’9’) return 0; c++; } if (*c < ’0’ && *c > ’9’) return 0; c++; if (*c < ’0’ && *c > ’9’) return 0; c++; if (*c < ’0’ && *c > ’1’) return 0; c++; /* Month */ if (*c < ’0’ && *c > ’9’) return 0; c++; if (*c < ’0’ && *c > ’3’) return 0; c++; /* Day */ if (*c < ’0’ && *c > ’9’) return 0; return 1; Following our report, the developers of MatrixSSL have ac- knowledged this is indeed a faulty implementation. Along with other ﬁxes being implemented to make date-time processing more robust, they have decided that this sanity check will no longer be used in newer versions of MatrixSSL. Extra 2 (notBefore and notAfter bytes taken “as is” in CyaSSL 2.7.0, WolfSSL 3.6.6, axTLS 1.4.3 and 1.5.3): For the four aforementioned implementations, we noticed during cross-validation that they do not perform any explicit boundary checks on the value of the date time value bytes of notBefore and notAfter, and just assumed that those bytes are going to be valid ASCII digits (i.e. 0–9). It is hence possible to put other ASCII characters in the date time bytes and obtain an exceptionally large (small) values for notAfter (notBefore), though this does not seem to be an imminent threat, nor does it violate the RFC, as the RFC did not stipulate what implementations should do. Extra 3 (Timezone Handling): Another discrepancy that we have observed during cross-validating path constraints of different libraries was how they impose/assume the time zone of notBefore and notAfter on certiﬁcates. Speciﬁcally, we notice that mbedTLS 2.1.4 and wolfSSL v2.3.3 would reject certiﬁcates that do not have the timezone ending with a ‘Z’. This is possibly due to the fact that RFC 5280 [2] mandates conforming CAs to express validity in Zulu time (a.k.a GMT or Zero Meridian Time) when issuing certiﬁcates, regardless of the type being UTCTime or GeneralizedTime. Other imple- mentations like MatrixSSL 3.7.2, axTLS 1.5.3 and PolarSSL 1.2.8 ignore the timezone character and simply assume the Zulu timezone is always being used. This is arguably an example of under-speciﬁcation, as it is not clear whether implementations should try to handle (with proper time zone adjustment) or reject certiﬁcates with a non- Zulu timezone, since RFC 5280 [2] did not explicitly mandate an expected behavior. VII. DISCUSSION A. Takeaway for Application Developers As a takeaway for application developers that need to use SSL/TLS libraries for processing X.509 certiﬁcates, a general rule of thumb is to upgrade to newer versions of the libraries if possible. As demonstrated by our ﬁndings, newer versions of implementations, even when originated from the same source tree as their legacy counterparts, are better equipped in terms of features and extension handling, as well as in general having more rigorous checks. Holding on to legacy code could potentially hurt both security and interoperability. Unfortunately, regular software patching, particularly for IoT devices, does not seem to happen widespread enough [97]. We understand that due to the needs to optimize for different application scenarios (e.g. small footprint for resource con- strained platforms), certain features might not be implemented in their entirety as described in the standard speciﬁcations. In order to help application developers to better understand the trade-offs and make a more well-informed decision in choosing which SSL/TLS library to use, we believe that one possibility would be to have a certiﬁcation program that tests for implementation conformance and interoperability, similar to that of the IPv6 Ready Logo Program [98], and the High Deﬁnition Logos [99]. For example, an “X.509 Gold” for libraries that implement most required features correctly, and an “X.509 Ready” for libraries that can only handle the bare minimum but are missing out on certain features. B. Limitations Since our noncompliance detection approach critically relies on symbolic execution which is known to suffer from path ex- plosion, especially in the presence of symbolic data-dependent loops, it is deliberately made to trade away completeness for practicality (i.e., our approach is not guaranteed to reveal all possible noncompliances in an implementation and can have false negatives). Our current scope of analysis does not include the logic for checking certiﬁcation revocation status and hostname match- ing. As noted in [43], for both revocation status checking and hostname matching, while some libraries provide relevant facilities, some delegate the task to application developers. In addition, a typical implementation of a hostname matching logic uses complex string operations and analyzing these require a dedicated SMT solver with support for the theory of strings [100]. We leave that for future work. Moreover, as we use concrete values in SymCerts, symbolic execution sway away from rigorously exercising the parsing logic. Though we have uncovered parsing bugs as reported in Section VI, our scrutiny on the parsing code is not meant to be comprehensive. Noticeably, low-level memory errors due to incorrect buffer management in the parsing code, as reported in a recent Vulnerability Note [101], can elude our analysis. C. Threat to Validity In some cases during certiﬁcate validation, it is not clear who is required to perform the validity check on a ﬁeld, i.e., 15 the underlying library or the application using the library. The RFC states that some speciﬁc validity check must be performed without clearly identifying the responsible party. This unclear separation of responsibilities have resulted in libraries opting for signiﬁcantly different API designs. We rely on example usage—often come with the source code in the form of a sample client—to draw a boundary for extracting the approximated certiﬁcate accepting (and rejecting) universes. Optional function calls to extra checking logics, if not demon- strated in the sample client programs, will be missed by our analysis. Additionally, if some of the checks performed on certiﬁcates are being pushed down to a different phase during SSL/TLS handshake instead of the server certiﬁcate validation phase, these checks might be missing from our extraction. We rely on the concrete client-server replay setup to catch them and iteratively include them in the extraction. Our optimization often rely on the expectation that the value of some ﬁelds are handled in the implementation in an uniform way. For checking validity of ﬁelds that can have variable lengths, we assume the implementation treats each regular length (not corner cases) uniformly. In addition, we also assume that the semantic independence of certain certiﬁcate ﬁelds are maintained in the implementation. For instance, we assume that the certiﬁcate validity ﬁelds are not dependent on any other ﬁelds. Although we have observed that this seems to be the case and the RFC supports it, hypothetically a developer can mistakenly create an artiﬁcial dependency. VIII. CONCLUSION AND FUTURE DIRECTION In this paper, we present a novel approach that leverages symbolic execution to ﬁnd noncompliance in X.509 implemen- tations. In alignment with the general consensus, we observe that due to the recursive nature of certiﬁcate representation, an off-the-shelf symbolic execution engine suffers from path explosion problem. We overcome this inherent challenge in two ways: (1) Focusing on real implementations with a small resource footprint; (2) Leveraging domain-speciﬁc insights, abstractions, and compartmentalization. We use SymCerts— certiﬁcate chains in which each certiﬁcate has a mix of symbolic and concrete values—such that symbolic execution can be made scalable on many X.509 implementations while meaningful analysis can be conducted. We applied our noncompliance approach to analyze 9 real implementations selected from 4 major families of SSL/TLS source base. Our analysis exposed 48 instances of noncom- pliance, some of which has severe security implications. We have responsibly shared our new ﬁndings with the respective library developers. Most of our reports have generated positive"
Systems and Networking,Guevara Noubir,https://www.khoury.northeastern.edu/people/guevara-noubir/,"Guevara Noubir is a professor and the executive director of cybersecurity programs in the Khoury College of Computer Sciences at Northeastern University, based in Boston. He is also the principal investigator of Northeastern University’s NSA/DHS-designated Center of Academic Excellence in Cybersecurity.",Theory and practice of privacy and security; Robustness of networked systems,"PhD in computer science, Swiss Federal Institute of Technology, Lausanne — Switzerland","May 4th, 2019",Mitigating Location Privacy Attacks on Mobile Devices using Dynamic App Sandboxing,https://content.sciendo.com/downloadpdf/journals/popets/2019/2/article-p66.pdf," S. Narain, G. Noubir, ""Mitigating Location Privacy Attacks on Mobile Devices using Dynamic App Sandboxing"", in Privacy Enhancing Technologies Symposium (PoPETS), 2019, [Full Paper PDF].",Failed to download
Systems and Networking,Ji-Yong Shin,https://www.khoury.northeastern.edu/people/ji-yong-shin/,"Ji-Yong Shin is an assistant professor in the Khoury College of Computer Sciences, based in Boston.",Distributed systems; Formal verification; Cloud storage systems; Operating systems,"PhD in Computer Science, Cornell University; MS in Computer Science, Korea Advanced Institute of Science and Technology — South Korea; BS in Computer Science and Industrial Engineering, Yonsei University — South Korea","March 5th, 2024",FusionFlow: Accelerating Data Preparation for Machine Learning with Hybrid CPU-GPU Processing,https://www.vldb.org/pvldb/vol17/p863-kim.pdf," Taeyoon Kim, Chanho Park, Mansur Mukimbekov, Heelim Hong, Minseok Kim, Ze Jin, Changdae Kim, Ji-Yong Shin, Myeongjae Jeon. (2023). FusionFlow: Accelerating Data Preparation for Machine Learning with Hybrid CPU-GPU Processing Proc. VLDB Endow., 17, 863-876. https://www.vldb.org/pvldb/vol17/p863-kim.pdf","Data augmentation enhances the accuracy of DL models by diversi- fying training samples through a sequence of data transformations. While recent advancements in data augmentation have demonstrated remarkable efficacy, they often rely on computationally expensive and dynamic algorithms. Unfortunately, current system optimiza- tions, primarily designed to leverage CPUs, cannot effectively sup- port these methods due to costs and limited resource availability. To address these issues, we introduce FusionFlow, a system that cooperatively utilizes both CPUs and GPUs to accelerate the data preprocessing stage of DL training that runs the data augmentation algorithm. FusionFlow orchestrates data preprocessing tasks across CPUs and GPUs while minimizing interference with GPU-based model training. In doing so, it effectively mitigates the risk of GPU memory overflow by managing memory allocations of the tasks within the GPU-wide free space. Furthermore, FusionFlow provides a dynamic scheduling strategy for tasks with varying computational demands and reallocates compute resources on the fly to enhance training throughput for both single and multi-GPU DL jobs. Our evaluations show that FusionFlow outperforms existing CPU-based methods by 16–285% in single-machine scenarios and, to achieve similar training speeds, requires 50–60% fewer CPUs compared to utilizing scalable compute resources from external servers. PVLDB Reference Format: Taeyoon Kim, ChanHo Park, Mansur Mukimbekov, Heelim Hong, Minseok Kim, Ze Jin, Changdae Kim, Ji-Yong Shin, and Myeongjae Jeon. FusionFlow: Accelerating Data Preprocessing for Machine Learning with CPU-GPU Cooperation. PVLDB, 17(4): 863 - 876, 2023. doi:10.14778/3636218.3636238 PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://github.com/omnia-unist/FusionFlow. This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 17, No. 4 ISSN 2150-8097. doi:10.14778/3636218.3636238 1We propose FusionFlow that speeds up the dynamic data augmen- tation algorithms on CPUs and GPUs. The key idea is exploiting intra-batch parallelism, which splits an input mini-batch into mul- tiple tiny-batches and augments the mini-batch in parallel on those compute resources. FusionFlow applies several optimizations to make GPU-offloading of tiny-batch tasks highly effective and per- forms CPU worker scaling to make CPU resource usage in data- parallel training more balanced. Experimental results confirm the effectiveness of FusionFlow."
